[["index.html", "Introducción a Ciencia de Datos y Machine Learning BIENVENIDA Objetivo Instructores Alcances del curso Duración y evaluación del curso Recursos y dinámica de clase", " Introducción a Ciencia de Datos y Machine Learning BIENVENIDA Objetivo Brindar al participante los elementos teóricos y prácticos básicos alrededor de la programación para el análisis de datos. Aprenderá a distinguir las diferentes soluciones a problemas que pueden resolverse con algoritmos de machine learning y aprenderá a usar el conjunto de librerías en R más novedosas, estructuradas y ampliamente usadas para la manipulación, transformación y visualización de datos: “TIDYVERSE”. Instructores ACT. ARTURO BRINGAS LinkedIn: arturo-bringas Email: act.arturo.b@ciencias.unam.mx Actuario egresado de la Facultad de Ciencias con maestría en Ciencia de Datos por el ITAM. Se especializa en modelos predictivos y de clasificación de machine learning aplicado a seguros, banca, marketing, deportes, e-commerce y movilidad internacional. Ha sido consultor Senior Data Scientist para empresas y organizaciones como GNP, El Universal, UNAM, la Organización de las Naciones Unidas Contra la Droga y el Delito (UNODC), entre otros. Actualmente es profesor de Ciencia de datos y Machine Learning en AMAT y Data Scientist Expert en BBVA, en donde implementa soluciones de analítica avanzada con impacto global. ACT. KARINA LIZETTE GAMBOA LinkedIn: KaLizzyGam Email: lizzygamboa@ciencias.unam.mx Actuaria egresada de la Facultad de Ciencias y candidata a Maestra en Ciencia de Datos por el ITAM. Experiencia en áreas de analítica predictiva e inteligencia del negocio. Lead y Senior Data Scientist en consultoría en diferentes sectores como tecnología, asegurador, financiero y bancario. Es experta en entendimiento de negocio para la correcta implementación de algoritmos de inteligencia y explotación de datos. Actualmente se desarrolla como lead data specialist en Merama, startup mexicana clasificada como uno de los nuevos unicornios de Latinoamérica. Senior Data Scientist en CLOSTER y como profesora del diplomado de Metodología de la Investigación Social por la UNAM así como instructora de cursos de Ciencia de Datos en AMAT. Empresas anteriores: GNP, Actinver Banco y Casa de Bolsa, PlayCity Casinos, RakenDataGroup Consulting, entre otros. Alcances del curso Al finalizar este curso, el participante será capaz de consumir, manipular y visualizar información para resolver problemas de propósito general asociados a los datos. Apenderá a implementar diferentes algoritmos de machine learning y mejorar su desempeño predictivo en problemas de clasificación, regresión y segmentación. Requisitos: Computadora con al menos 8Gb Ram Instalar la versión más reciente de R Instalar la versión más reciente de RStudio Temario: 1. Introducción a Ciencia de Datos Machine Learning, Bigdata, BI, AI y CD Objetivo de ciencia de datos Requisitos y aplicaciones Tipos de algoritmos Ciclo de vida de un proyecto 2. Manipulación de datos con Tidyverse Importación de tablas (readr) Consultas (dplyr) Transformación de estructuras (tidyr) 3. Concepto de Machine Learning Machine learning Análisis supervisado Análisis no supervisado Sesgo y varianza Partición de datos Preprocesamiento e ingeniería de datos 4. Algoritmos de Machine Learning Clustering: Kmeans, kmedoids, agnes Regresión Lineal Métricas de error Regresión logística Métricas de error KNN Árbol de decisión Random Forest Comparación de modelos Duración y evaluación del curso El programa tiene una duración de 40 hrs. Las clases serán impartidas los días sábado, de 9:00 am a 1:00 pm Serán asignados ejercicios que el participante deberá resolver entre una semana y otra. Al final del curso se solicitará un proyecto final, el cual deberá ser entregado para ser acreedor a la constancia de participación. Recursos y dinámica de clase En esta clase estaremos usando: R da click aquí si aún no lo descargas RStudio da click aquí también Zoom Clases Pulgar arriba: Voy bien, estoy entendiendo! Pulgar abajo: Eso no quedó muy claro Mano arriba: Quiero participar/preguntar ó Ya estoy listo para iniciar Google Drive Notas de clase Revisame si quieres aprender "],["conceptos-de-ciencia-de-datos.html", "Capítulo 1 Conceptos de Ciencia de Datos 1.1 ¿Qué es Ciencia de Datos? 1.2 Objetivos 1.3 Requisitos 1.4 Aplicaciones 1.5 Tipos de algoritmos 1.6 Ciclo de un proyecto", " Capítulo 1 Conceptos de Ciencia de Datos 1.1 ¿Qué es Ciencia de Datos? Definiendo conceptos: Estadística Disciplina que recolecta, organiza, analiza e interpreta datos. Lo hace a través de una población muestral generando estadística descriptiva y estadística inferencial. La estadística descriptiva, como su nombre lo indica, se encarga de describir datos y obtener conclusiones. Se utilizan números (media, mediana, moda, mínimo, máximo, etc) para analizar datos y llegar a conclusiones de acuerdo a ellos. La estadística inferencial argumenta o infiere sus resultados a partir de las muestras de una población. Se intenta conseguir información al utilizar un procedimiento ordenado en el manejo de los datos de la muestra. La estadística predictiva busca estimar valores y escenarios futuros más probables de ocurrir a partir de referencias históricas previas. Se suelen ocupar como apoyo características y factores áltamente asociados al fenómeno que se desea predecir. Business Intelligence: BI aprovecha el software y los servicios para transformar los datos en conocimientos prácticos que informan las decisiones empresariales estratégicas y tácticas de una organización. Las herramientas de BI acceden y analizan conjuntos de datos y presentan hallazgos analíticos en informes, resúmenes, tableros, gráficos, cuadros, -indicadores- o KPI’s y mapas para proporcionar a los usuarios inteligencia detallada sobre el estado del negocio. BI esta enfocado en analizar la historia pasada para tomar decisiones hacia el futuro. ¿Qué características tiene un KPI? Específicos Continuos y periódicos Objetivos Cuantificables Medibles Realistas Concisos Coherentes Relevantes Machine Learning: Machine learning –aprendizaje de máquina– es una rama de la inteligencia artificial que permite que las máquinas aprendan de los patrones existentes en los datos. Se usan métodos computacionales para aprender de datos con el fin de producir reglas para mejorar el desempeño en alguna tarea o toma de decisión. (Está enfocado en la programación de máquinas para aprender de los patrones existentes en datos principalmente estructurados y anticiparse al futuro) Deep Learning: El aprendizaje profundo es un subcampo del aprendizaje automático que se ocupa de los algoritmos inspirados en la estructura y función del cerebro llamados redes neuronales artificiales. En Deep Learning, un modelo de computadora aprende a realizar tareas de clasificación directamente a partir de imágenes, texto o sonido. Los modelos de aprendizaje profundo pueden lograr una precisión de vanguardia, a veces superando el rendimiento a nivel humano. Los modelos se entrenan mediante el uso de un gran conjunto de datos etiquetados y arquitecturas de redes neuronales que contienen muchas capas. (Está enfocado en la programación de máquinas para el reconocimiento de imágenes y audio (datos no estructurados)) Big data se refiere a los grandes y diversos conjuntos de información que crecen a un ritmo cada vez mayor. Abarca el volumen de información, la velocidad a la que se crea y recopila, y la variedad o alcance de los puntos de datos que se cubren. Los macrodatos a menudo provienen de la minería de datos y llegan en múltiples formatos. Es común que se confunda los conceptos de Big Data y Big Compute, como se mencionó, Big Data se refiere al procesamiento de conjuntos de datos que son más voluminosos y complejos que los tradicionales y Big Compute a herramientas y enfoques que utilizan una gran cantidad de recursos de CPU y memoria de forma coordinada para resolver problemas que usan algoritmos muy complejos. Curiosidad: Servidores en líquido para ser enfriados Curiosidad 2: Centro de datos en el océano Entonces, ¿qué NO es ciencia de datos? No es una tecnología No es una herramienta No es desarrollo de software No es Business Intelligence* No es Big Data* No es Inteligencia Artificial* No es (solo) machine learning No es (solo) deep learning No es (solo) visualización No es (solo) hacer modelos 1.2 Objetivos Los científicos de datos analizan qué preguntas necesitan respuesta y dónde encontrar los datos relacionados. Tienen conocimiento de negocio y habilidades analíticas, así como la capacidad de extraer, limpiar y presentar datos. Las empresas utilizan científicos de datos para obtener, administrar y analizar grandes cantidades de datos no estructurados. Luego, los resultados se sintetizan y comunican a las partes interesadas clave para impulsar la toma de decisiones estratégicas en la organización. Fuente: Blog post de Drew Conway Más sobre Conway: Forbes 2016 1.3 Requisitos Background científico: Conocimientos generales de probabilidad, estadística, álgebra lineal, cálculo, geometría analítica, programación, conocimientos computacionales… etc Datos relevantes y suficientes: Es indispensable saber si los datos con los que se trabajará son relevantes y suficientes, debemos evaluar qué preguntas podemos responder con los datos con los que contamos. Suficiencia: Los datos con los que trabajamos tienen que ser representativos de la población en general, necesitamos que las características representadas en la información sean suficientes para aproximar a la población objetivo. Relevancia: De igual manera los datos tienen que tener relevancia para la tarea que queremos resolver, por ejemplo, es probable que información sobre gusto en alimentos sea irrelevante para predecir número de hijos. Etiquetas: Se necesita la intervención humana para etiquetar, clasificar e introducir los datos en el algoritmo. Software: Existen distintos lenguajes de programación para realizar ciencia de datos 1.4 Aplicaciones Dependiendo de la industria en la que se quiera aplicar Machine Learning, podemos pensar en distintos enfoques, en la siguiente imagen se muestran algunos ejemplos: Podemos pensar en una infinidad de aplicaciones comerciales basadas en el análisis de datos. Con la intención de estructurar las posibles aplicaciones, se ofrece a continuación una categorización que, aunque no es suficiente para englobar todos los posibles casos de uso, sí es sorprendente la cantidad de aplicaciones que abarca. 1. Aplicaciones centradas en los clientes Incrementar beneficio al mejorar recomendaciones de productos Up-selling Cross-selling Reducir tasas de cancelación y mejorar tasas de retención Personalizar experiencia de usuario Mejorar el marketing dirigido Análisis de sentimientos Personalización de productos o servicios 2. Optimización de problemas Optimización de precios Ubicación de nuevas sucursales Maximización de ganancias mediante producción de materias primas Construcción de portafolios de inversión 3. Predicción de demanda Número futuro de clientes Número esperado de viajes en avión / camión / bicis Número de contagios por un virus (demanda médica / medicamentos / etc) Predicción de uso de recursos (luz / agua / gas) 4. Análisis de detección de fraudes Detección de robo de identidad Detección de transacciones ilícitas Detección de servicios fraudulentos Detección de zonas geográficas con actividades ilícitas 1.5 Tipos de algoritmos Los algoritmos de Machine Learning se dividen en tres categorías, siendo las dos primeras las más comunes: La diferencia entre el análisis supervisado y el no supervisado es la etiqueta, es decir, en el análisis supervisado tenemos una etiqueta “correcta” y el objetivo de los algoritmos es predecir esta etiqueta. 1.5.1 Aprendizaje supervisado En el aprendizaje supervisado, la idea principal es aprender bajo supervisión, donde la señal de supervisión se nombra como valor objetivo o etiqueta. Estos algoritmos cuentan con un aprendizaje previo basado en un sistema de etiquetas asociadas a unos datos que les permiten tomar decisiones o hacer predicciones. Conocemos la respuesta correcta de antemano. Esta respuesta correcta fue “etiquetada” por un humano (la mayoría de las veces, en algunas circunstancias puede ser generada por otro algoritmo). Debido a que conocemos la respuesta correcta, existen muchas métricas de desempeño del modelo para verificar que nuestro algoritmo está haciendo las cosas “bien”. Algunos ejemplos son: - Un detector de spam que etiqueta un e-mail como spam o no. - Predecir precios de casas - Clasificación de imagenes - Predecir el clima - ¿Quiénes son los clientes descontentos? Tipos de aprendizaje supervisado (Regresión vs clasificación) Existen dos tipos principales de aprendizaje supervisado, esto depende del tipo de la variable respuesta: Clasificación En el aprendizaje supervisado, los algoritmos de clasificación se usan cuando el resultado es una etiqueta discreta. Esto quiere decir que se utilizan cuando la respuesta se fundamenta en conjunto finito de resultados. Regresión El análisis de regresión es un subcampo del aprendizaje automático supervisado cuyo objetivo es establecer un método para la relación entre un cierto número de características y una variable objetivo continua. 1.5.2 Aprendizaje no supervisado En el aprendizaje no supervisado, carecemos de etiquetas. Por lo tanto, necesitamos encontrar nuestro camino sin ninguna supervisión ni guía. Esto simplemente significa que necesitamos descubrir ¿qué es qué? por nosotros mismos. Aquí no tenemos la respuesta correcta de antemano ¿cómo podemos saber que el algoritmo está bien o mal? Estadísticamente podemos verificar que el algoritmo está bien Siempre tenemos que verificar con el cliente si los resultados que estamos obteniendo tienen sentido de negocio. Por ejemplo, número de grupos y características Algunos ejemplos son: - Encontrar segmentos de clientes. - Reducir la complejidad de un problema - Selección de variables - Encontrar grupos - Reducción de dimensionalidad 1.5.3 Aprendizaje por refuerzo Su objetivo es que un algoritmo aprenda a partir de la propia experiencia. Esto es, que sea capaz de tomar la mejor decisión ante diferentes situaciones de acuerdo a un proceso de prueba y error en el que se recompensan las decisiones correctas. Algunos ejemplos son: - Optimización de campañas de marketing - Reconocimiento facial - Diagnósticos médicos - Clasificar secuencias de ADN Ejemplo: Mario Bros 1.6 Ciclo de un proyecto Identificación del problema Debemos conocer si el problema es significativo, si el problema se puede resolver con ciencia de datos, y si habrá un compromiso real del lado de cliente/usuario/partner para implementar la solución con todas sus implicaciones: recursos físicos y humanos. Scoping El objetivo es definir el alcance del proyecto y por lo tanto definir claramente los objetivos. Conocer las acciones que se llevarán a cabo para cada objetivo. Estas definirán las soluciones analíticas a hacer. Queremos saber si los datos con los que contamos son relevantes y suficientes. Hacer visible los posibles conflictos éticos que se pueden tener en esta fase. Debemos definir el cómo evaluaremos que el análisis de esos datos será balanceada entre eficiencia, efectividad y equidad. Adquisición de datos Adquisición, almacenamiento, entendimiento y preparación de los datos para después poder hacer analítica sober ellos. Asegurar que en la transferencia estamos cumpliendo con el manejo adecuado de datos sensibles y privados. EDA El objetivo en esta fase es conocer los datos con los que contamos y contexto de negocio explicado a través de los mismos. Identificamos datos faltantes, sugerimos cómo imputarlos. Altamente apoyado de visualización y procesos de adquisición y limpieza de datos. Formulación analítica Esta fase incluye empezar a formular nuestro problema como uno de ciencia de datos, el conocimiento adquirido en la fase de exploración nos permite conocer a mayor detalle del problema y por lo tanto de la solución adecuada. Modelado Proceso iterativo para desarrollar diferentes “experimentos”. Mismo algoritmo/método diferentes hiperparámetros (grid search). Diferentes algortimos. Selección de un muy pequeño conjunto de modelos tomando en cuenta un balance entre interpretabilidad, complejidad, desempeño, fairness. Correcta interpretación de los resultados de desempeño de cada modelo. Validación Es muy importante poner a prueba el/los modelo/modelos seleccionados en la fase anterior. Esta prueba es en campo con datos reales, le llamamos prueba piloto. Debemos medir el impacto causal que nuestro modelo tuvo en un ambiente real. Acciones a realizar Finalmente esta etapa corresponde a compartir con los tomadores de decisiones/stakeholders/creadores de política pública los resultados obtenidos y la recomendación de acciones a llevar a cabo -menú de opciones-. Las implicaciones éticas de esta fase consisten en hacer conciente el impacto social de nuestro trabajo. "],["introducción-a-r.html", "Capítulo 2 Introducción a R 2.1 ¿Cómo obtener R? 2.2 ¿Qué es RStudio? 2.3 Lectura de datos 2.4 Consultas de datos 2.5 Orden y estructura", " Capítulo 2 Introducción a R R (R Core Team) es un entorno y lenguaje de programación que permite el análisis estadístico de información y reportes gráficos. Es ampliamente usado en investigación por la comunidad estadística en campos como la biomedicina, minería de datos, matemáticas financieras, entre otros. Ha ganado mucha popularidad en los últimos años al ser un software libre que está en constante crecimiento por las aportaciones de otros usuarios y que permite la interacción con software estadísticos como STATA, SAS, SPSS, etc.. R permite la incorporación de librerías y paqueterías con funcionalidades específicas, por lo que es un lenguaje de programación muy completo y fácil de usar. 2.1 ¿Cómo obtener R? R puede ser fácilmente descargado de forma gratuita desde el sitio oficial http://www.r-project.org/. R está disponible para las plataformas Windows, Mac y Linux. 2.2 ¿Qué es RStudio? RStudio es un Entorno de Desarrollo Integrado (IDE, por sus siglas en inglés) para R. Este permite y facilita el desarrollo y ejecución de sintaxis para código en R, incluye una consola y proporciona herramientas para la gestión del espacio de trabajo. RStudio está disponible para Windows, Mac y Linux o para navegadores conectados a RStudio Server o RStudio Server Pro. Algunas de las principales características de Rstudio que lo hacen una gran herramienta para trabajar en R, son: Auto completado de código Sangría inteligente Resaltado de sintaxis Facilidad para definir funciones Soporte integrado Documentación integrada Administración de directorios y proyectos Visor de datos Depurador interactivo para corregir errores Conección con Rmarkwon y Sweave La siguiente imagen muestra la forma en la que está estructurado RStudio. El orden de los páneles puede ser elegido por el usuario, así como las características de tipo de letra, tamaño y color de fondo, entre otras características. Figure 2.1: Páneles de trabajo de Rstudio 2.3 Lectura de datos El primer paso para analizar datos es incorporarlos a la sesión de R para que puedan ser manipulados y observados. Existen múltiples librerías y funciones en R que permiten leer la información proveniente de un archivo externo, el cual puede tener una de muchas posibles extensiones. Usualmente, no creamos los datos desde la sesión de R, sino que a través de un archivo externo se realiza la lectura de datos escritos en un archivo. Los más comúnes son: La paquetería readr fue desarrollada recientemente para lidiar con la lectura de archivos grandes rápidamente. Esta paquetería proporciona funciones que suelen ser mucho más rápidas que las funciones base que proporciona R. Ventajas de readr: Por lo general, son mucho más rápidos (~ 10x) que sus funciones equivalentes. Producen tibbles: No convierten vectores de caracteres en factores. No usan nombres de filas ni modifican los nombres de columnas. Reproducibilidad 2.3.1 Archivos csv A la hora de importar conjuntos de datos en R, uno de los formatos más habituales en los que hallamos información es en archivos separados por comas (comma separated values), cuya extensión suele ser .csv. En ellos encontramos múltiples líneas que recogen la tabla de interés, y en las cuales los valores aparecen, de manera consecutiva, separados por el carácter ,. Para importar este tipo de archivos en nuestra sesión de R, se utiliza la función read_csv(). Para acceder a su documentación utilizamos el comando ?read_csv. El único argumento que debemos de pasar a esta función de manera obligatoria, es file, el nombre o la ruta completa del archivo que pretendemos importar. library(readr) read_csv( file, col_names = TRUE, col_types = NULL, locale = default_locale(), na = c(&quot;&quot;, &quot;NA&quot;), quoted_na = TRUE, quote = &quot;\\&quot;&quot;, comment = &quot;&quot;) La paquetería readr fue desarrollada recientemente para lidiar con la lectura de archivos grandes rápidamente. El paquete proporciona reemplazos para funciones como read.table(), read.csv() entre otras. Esta paquetería proporciona funciones que suelen ser mucho más rápidas que las funciones base que proporciona R. Ventajas de readr: Por lo general, son mucho más rápidos (~ 10x) que sus funciones equivalentes. Producen tibbles: No convierten vectores de caracteres en factores. No usan nombres de filas ni modifican los nombres de columnas. Reproducibilidad No convierte, automáticamente, las columnas con cadenas de caracteres a factores, como sí hacen por defecto las otras funciones base de R. Reconoce ocho clases diferentes de datos (enteros, lógicos, etc.), dejando el resto como cadenas de caracteres. Veamos un ejemplo: La base de datos llamada AmesHousing contiene un conjunto de datos con información de la Oficina del Tasador de Ames utilizada para calcular los valores tasados para las propiedades residenciales individuales vendidas en Ames, Iowa, de 2006 a 2010. FUENTES: Ames, Oficina del Tasador de Iowa. Pueden descargar los datos para la clase aquí base &lt;- read.csv(&quot;data/ames.csv&quot;) head(base, 2) ## MS_SubClass MS_Zoning Lot_Frontage ## 1 One_Story_1946_and_Newer_All_Styles Residential_Low_Density 141 ## 2 One_Story_1946_and_Newer_All_Styles Residential_High_Density 80 ## Lot_Area Street Alley Lot_Shape Land_Contour Utilities ## 1 31770 Pave No_Alley_Access Slightly_Irregular Lvl AllPub ## 2 11622 Pave No_Alley_Access Regular Lvl AllPub ## Lot_Config Land_Slope Neighborhood Condition_1 Condition_2 Bldg_Type ## 1 Corner Gtl North_Ames Norm Norm OneFam ## 2 Inside Gtl North_Ames Feedr Norm OneFam ## House_Style Overall_Cond Year_Built Year_Remod_Add Roof_Style Roof_Matl ## 1 One_Story Average 1960 1960 Hip CompShg ## 2 One_Story Above_Average 1961 1961 Gable CompShg ## Exterior_1st Exterior_2nd Mas_Vnr_Type Mas_Vnr_Area Exter_Cond Foundation ## 1 BrkFace Plywood Stone 112 Typical CBlock ## 2 VinylSd VinylSd None 0 Typical CBlock ## Bsmt_Cond Bsmt_Exposure BsmtFin_Type_1 BsmtFin_SF_1 BsmtFin_Type_2 ## 1 Good Gd BLQ 2 Unf ## 2 Typical No Rec 6 LwQ ## BsmtFin_SF_2 Bsmt_Unf_SF Total_Bsmt_SF Heating Heating_QC Central_Air ## 1 0 441 1080 GasA Fair Y ## 2 144 270 882 GasA Typical Y ## Electrical First_Flr_SF Second_Flr_SF Gr_Liv_Area Bsmt_Full_Bath ## 1 SBrkr 1656 0 1656 1 ## 2 SBrkr 896 0 896 0 ## Bsmt_Half_Bath Full_Bath Half_Bath Bedroom_AbvGr Kitchen_AbvGr TotRms_AbvGrd ## 1 0 1 0 3 1 7 ## 2 0 1 0 2 1 5 ## Functional Fireplaces Garage_Type Garage_Finish Garage_Cars Garage_Area ## 1 Typ 2 Attchd Fin 2 528 ## 2 Typ 0 Attchd Unf 1 730 ## Garage_Cond Paved_Drive Wood_Deck_SF Open_Porch_SF Enclosed_Porch ## 1 Typical Partial_Pavement 210 62 0 ## 2 Typical Paved 140 0 0 ## Three_season_porch Screen_Porch Pool_Area Pool_QC Fence ## 1 0 0 0 No_Pool No_Fence ## 2 0 120 0 No_Pool Minimum_Privacy ## Misc_Feature Misc_Val Mo_Sold Year_Sold Sale_Type Sale_Condition Sale_Price ## 1 None 0 5 2010 WD Normal 215000 ## 2 None 0 6 2010 WD Normal 105000 ## Longitude Latitude ## 1 -93.61975 42.05403 ## 2 -93.61976 42.05301 tidy &lt;- read_csv(&quot;data/ames.csv&quot;) head(tidy, 2) ## # A tibble: 2 × 74 ## MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 One_Story_1946_and_New… Resident… 141 31770 Pave No_A… Slightly… ## 2 One_Story_1946_and_New… Resident… 80 11622 Pave No_A… Regular ## # ℹ 67 more variables: Land_Contour &lt;chr&gt;, Utilities &lt;chr&gt;, Lot_Config &lt;chr&gt;, ## # Land_Slope &lt;chr&gt;, Neighborhood &lt;chr&gt;, Condition_1 &lt;chr&gt;, Condition_2 &lt;chr&gt;, ## # Bldg_Type &lt;chr&gt;, House_Style &lt;chr&gt;, Overall_Cond &lt;chr&gt;, Year_Built &lt;dbl&gt;, ## # Year_Remod_Add &lt;dbl&gt;, Roof_Style &lt;chr&gt;, Roof_Matl &lt;chr&gt;, ## # Exterior_1st &lt;chr&gt;, Exterior_2nd &lt;chr&gt;, Mas_Vnr_Type &lt;chr&gt;, ## # Mas_Vnr_Area &lt;dbl&gt;, Exter_Cond &lt;chr&gt;, Foundation &lt;chr&gt;, Bsmt_Cond &lt;chr&gt;, ## # Bsmt_Exposure &lt;chr&gt;, BsmtFin_Type_1 &lt;chr&gt;, BsmtFin_SF_1 &lt;dbl&gt;, … ¿Y si el archivo que necesitamos leer esta en excel? 2.3.2 Archivos txt Uno de los archivos más comunes es el .txt. La librería readr también cuenta con funciones que permiten leer fácilmente los datos contenidos en formato tabular. ames_txt &lt;- read_delim(&quot;data/ames.txt&quot;, delim = &quot;;&quot;, col_names = TRUE) head(ames_txt, 2) ## # A tibble: 2 × 74 ## MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 One_Story_1946_and_New… Resident… 141 31770 Pave No_A… Slightly… ## 2 One_Story_1946_and_New… Resident… 80 11622 Pave No_A… Regular ## # ℹ 67 more variables: Land_Contour &lt;chr&gt;, Utilities &lt;chr&gt;, Lot_Config &lt;chr&gt;, ## # Land_Slope &lt;chr&gt;, Neighborhood &lt;chr&gt;, Condition_1 &lt;chr&gt;, Condition_2 &lt;chr&gt;, ## # Bldg_Type &lt;chr&gt;, House_Style &lt;chr&gt;, Overall_Cond &lt;chr&gt;, Year_Built &lt;dbl&gt;, ## # Year_Remod_Add &lt;dbl&gt;, Roof_Style &lt;chr&gt;, Roof_Matl &lt;chr&gt;, ## # Exterior_1st &lt;chr&gt;, Exterior_2nd &lt;chr&gt;, Mas_Vnr_Type &lt;chr&gt;, ## # Mas_Vnr_Area &lt;dbl&gt;, Exter_Cond &lt;chr&gt;, Foundation &lt;chr&gt;, Bsmt_Cond &lt;chr&gt;, ## # Bsmt_Exposure &lt;chr&gt;, BsmtFin_Type_1 &lt;chr&gt;, BsmtFin_SF_1 &lt;dbl&gt;, … La función read_delim() funciona para leer archivos con diferentes delimitadores posibles, es decir, es posible especificar si las columnas están separadas por espacios, comas, punto y coma, tabulador o algún otro delimitador (““,”,“,”;“,”, “@”). Adicionalmente, se puede especificar si el archivo contiene encabezado, si existen renglones a saltar, codificación, tipo de variable y muchas más opciones. Todos estos detalles pueden consultarse en la documentación de ayuda. 2.3.3 Archivos xls y xlsx La paquetería readxl facilita la obtención de datos tabulares de archivos de Excel. Admite tanto el formato .xls heredado como el formato .xlsx moderno basado en XML. Esta paquetería pone a disposición las siguientes funciones: read_xlsx() lee un archivo con extensión xlsx. read_xlsx( path, sheet = NULL, range = NULL, col_names = TRUE, col_types = NULL, na = &quot;&quot;, trim_ws = TRUE, skip = 0, n_max = Inf, guess_max = min(1000, n_max), progress = readxl_progress(), .name_repair = &quot;unique&quot; ) read_xls() lee un archivo con extensión xls. read_xls( path, sheet = NULL, range = NULL, col_names = TRUE, col_types = NULL, na = &quot;&quot;, trim_ws = TRUE, skip = 0, n_max = Inf, guess_max = min(1000, n_max), progress = readxl_progress(), .name_repair = &quot;unique&quot; ) read_excel() determina si el archivo es de tipo xls o xlsx para después llamar a una de las funciones mencionadas anteriormente. read_excel( path, sheet = NULL, range = NULL, col_names = TRUE, col_types = NULL, na = &quot;&quot;, trim_ws = TRUE, skip = 0, n_max = Inf, guess_max = min(1000, n_max), progress = readxl_progress(), .name_repair = &quot;unique&quot; ) EJERCICIO: Leer archivo excel de la carpeta del curso 2.3.4 Archivos json Se utiliza la función fromJSON de la paquetería jsonlite library(jsonlite) base_json &lt;- jsonlite::fromJSON(&quot;data/ames.json&quot;) head(base_json, 2) ## MS_SubClass MS_Zoning Lot_Frontage ## 1 One_Story_1946_and_Newer_All_Styles Residential_Low_Density 141 ## 2 One_Story_1946_and_Newer_All_Styles Residential_High_Density 80 ## Lot_Area Street Alley Lot_Shape Land_Contour Utilities ## 1 31770 Pave No_Alley_Access Slightly_Irregular Lvl AllPub ## 2 11622 Pave No_Alley_Access Regular Lvl AllPub ## Lot_Config Land_Slope Neighborhood Condition_1 Condition_2 Bldg_Type ## 1 Corner Gtl North_Ames Norm Norm OneFam ## 2 Inside Gtl North_Ames Feedr Norm OneFam ## House_Style Overall_Cond Year_Built Year_Remod_Add Roof_Style Roof_Matl ## 1 One_Story Average 1960 1960 Hip CompShg ## 2 One_Story Above_Average 1961 1961 Gable CompShg ## Exterior_1st Exterior_2nd Mas_Vnr_Type Mas_Vnr_Area Exter_Cond Foundation ## 1 BrkFace Plywood Stone 112 Typical CBlock ## 2 VinylSd VinylSd None 0 Typical CBlock ## Bsmt_Cond Bsmt_Exposure BsmtFin_Type_1 BsmtFin_SF_1 BsmtFin_Type_2 ## 1 Good Gd BLQ 2 Unf ## 2 Typical No Rec 6 LwQ ## BsmtFin_SF_2 Bsmt_Unf_SF Total_Bsmt_SF Heating Heating_QC Central_Air ## 1 0 441 1080 GasA Fair Y ## 2 144 270 882 GasA Typical Y ## Electrical First_Flr_SF Second_Flr_SF Gr_Liv_Area Bsmt_Full_Bath ## 1 SBrkr 1656 0 1656 1 ## 2 SBrkr 896 0 896 0 ## Bsmt_Half_Bath Full_Bath Half_Bath Bedroom_AbvGr Kitchen_AbvGr TotRms_AbvGrd ## 1 0 1 0 3 1 7 ## 2 0 1 0 2 1 5 ## Functional Fireplaces Garage_Type Garage_Finish Garage_Cars Garage_Area ## 1 Typ 2 Attchd Fin 2 528 ## 2 Typ 0 Attchd Unf 1 730 ## Garage_Cond Paved_Drive Wood_Deck_SF Open_Porch_SF Enclosed_Porch ## 1 Typical Partial_Pavement 210 62 0 ## 2 Typical Paved 140 0 0 ## Three_season_porch Screen_Porch Pool_Area Pool_QC Fence ## 1 0 0 0 No_Pool No_Fence ## 2 0 120 0 No_Pool Minimum_Privacy ## Misc_Feature Misc_Val Mo_Sold Year_Sold Sale_Type Sale_Condition Sale_Price ## 1 None 0 5 2010 WD Normal 215000 ## 2 None 0 6 2010 WD Normal 105000 ## Longitude Latitude ## 1 -93.6198 42.054 ## 2 -93.6198 42.053 2.3.5 Archivos rds Un tipo de archivo que resulta de particular interés, es el .RDS. Este archivo comprime cualquier objeto o resultado que sea usado o producido en R. Uno puede almacenar el objeto de interés de la siguiente manera: saveRDS(base_json, &quot;data/ames.rds&quot;) Puede observarse que en el explorador de archivos se encuentra ahora el nuevo archivo con extensión .rds, el cual puede ser posteriormente incorporado a una sesión de R para seguir trabajando con él. base_rds &lt;- readRDS(&quot;data/ames.rds&quot;) Algunas de las grandes ventajas que tiene almacenar los archivos en formato rds, son las siguientes: No es necesario volver a ejecutar procesos largos cuando ya se ha logrado realizar una vez. El tiempo de lectura de la información es considerablemente más rápido. 2.4 Consultas de datos Ahora que ya se ha estudiado la manera de cargar datos, aprenderemos como manipularlos con dplyr. El paquete dplyr proporciona un conjunto de funciones muy útiles para manipular data-frames y así reducir el número de repeticiones, la probabilidad de cometer errores y el número de caracteres que hay que escribir. Como valor extra, podemos encontrar que la gramática de dplyr es más fácil de entender. Revisaremos algunas de sus funciones más usadas (verbos), así como el uso de pipes (%&gt;%) para combinarlas. select() filter() arrange() mutate() summarise() join() group_by() Primero tenemos que instalar y cargar la paquetería (parte de tidyverse): # install.packages(&quot;dplyr&quot;) library(dplyr) library(readr) Usaremos el dataset AmesHousing que se proporcionó en el capítulo anterior (el alumno puede hacer el ejercicio con datos propios) ames_housing &lt;- read_csv(&quot;data/ames.csv&quot;) glimpse(ames_housing) ## Rows: 2,930 ## Columns: 74 ## $ MS_SubClass &lt;chr&gt; &quot;One_Story_1946_and_Newer_All_Styles&quot;, &quot;One_Story_1… ## $ MS_Zoning &lt;chr&gt; &quot;Residential_Low_Density&quot;, &quot;Residential_High_Densit… ## $ Lot_Frontage &lt;dbl&gt; 141, 80, 81, 93, 74, 78, 41, 43, 39, 60, 75, 0, 63,… ## $ Lot_Area &lt;dbl&gt; 31770, 11622, 14267, 11160, 13830, 9978, 4920, 5005… ## $ Street &lt;chr&gt; &quot;Pave&quot;, &quot;Pave&quot;, &quot;Pave&quot;, &quot;Pave&quot;, &quot;Pave&quot;, &quot;Pave&quot;, &quot;Pa… ## $ Alley &lt;chr&gt; &quot;No_Alley_Access&quot;, &quot;No_Alley_Access&quot;, &quot;No_Alley_Acc… ## $ Lot_Shape &lt;chr&gt; &quot;Slightly_Irregular&quot;, &quot;Regular&quot;, &quot;Slightly_Irregula… ## $ Land_Contour &lt;chr&gt; &quot;Lvl&quot;, &quot;Lvl&quot;, &quot;Lvl&quot;, &quot;Lvl&quot;, &quot;Lvl&quot;, &quot;Lvl&quot;, &quot;Lvl&quot;, &quot;H… ## $ Utilities &lt;chr&gt; &quot;AllPub&quot;, &quot;AllPub&quot;, &quot;AllPub&quot;, &quot;AllPub&quot;, &quot;AllPub&quot;, &quot;… ## $ Lot_Config &lt;chr&gt; &quot;Corner&quot;, &quot;Inside&quot;, &quot;Corner&quot;, &quot;Corner&quot;, &quot;Inside&quot;, &quot;… ## $ Land_Slope &lt;chr&gt; &quot;Gtl&quot;, &quot;Gtl&quot;, &quot;Gtl&quot;, &quot;Gtl&quot;, &quot;Gtl&quot;, &quot;Gtl&quot;, &quot;Gtl&quot;, &quot;G… ## $ Neighborhood &lt;chr&gt; &quot;North_Ames&quot;, &quot;North_Ames&quot;, &quot;North_Ames&quot;, &quot;North_Am… ## $ Condition_1 &lt;chr&gt; &quot;Norm&quot;, &quot;Feedr&quot;, &quot;Norm&quot;, &quot;Norm&quot;, &quot;Norm&quot;, &quot;Norm&quot;, &quot;N… ## $ Condition_2 &lt;chr&gt; &quot;Norm&quot;, &quot;Norm&quot;, &quot;Norm&quot;, &quot;Norm&quot;, &quot;Norm&quot;, &quot;Norm&quot;, &quot;No… ## $ Bldg_Type &lt;chr&gt; &quot;OneFam&quot;, &quot;OneFam&quot;, &quot;OneFam&quot;, &quot;OneFam&quot;, &quot;OneFam&quot;, &quot;… ## $ House_Style &lt;chr&gt; &quot;One_Story&quot;, &quot;One_Story&quot;, &quot;One_Story&quot;, &quot;One_Story&quot;,… ## $ Overall_Cond &lt;chr&gt; &quot;Average&quot;, &quot;Above_Average&quot;, &quot;Above_Average&quot;, &quot;Avera… ## $ Year_Built &lt;dbl&gt; 1960, 1961, 1958, 1968, 1997, 1998, 2001, 1992, 199… ## $ Year_Remod_Add &lt;dbl&gt; 1960, 1961, 1958, 1968, 1998, 1998, 2001, 1992, 199… ## $ Roof_Style &lt;chr&gt; &quot;Hip&quot;, &quot;Gable&quot;, &quot;Hip&quot;, &quot;Hip&quot;, &quot;Gable&quot;, &quot;Gable&quot;, &quot;Ga… ## $ Roof_Matl &lt;chr&gt; &quot;CompShg&quot;, &quot;CompShg&quot;, &quot;CompShg&quot;, &quot;CompShg&quot;, &quot;CompSh… ## $ Exterior_1st &lt;chr&gt; &quot;BrkFace&quot;, &quot;VinylSd&quot;, &quot;Wd Sdng&quot;, &quot;BrkFace&quot;, &quot;VinylS… ## $ Exterior_2nd &lt;chr&gt; &quot;Plywood&quot;, &quot;VinylSd&quot;, &quot;Wd Sdng&quot;, &quot;BrkFace&quot;, &quot;VinylS… ## $ Mas_Vnr_Type &lt;chr&gt; &quot;Stone&quot;, &quot;None&quot;, &quot;BrkFace&quot;, &quot;None&quot;, &quot;None&quot;, &quot;BrkFac… ## $ Mas_Vnr_Area &lt;dbl&gt; 112, 0, 108, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6… ## $ Exter_Cond &lt;chr&gt; &quot;Typical&quot;, &quot;Typical&quot;, &quot;Typical&quot;, &quot;Typical&quot;, &quot;Typica… ## $ Foundation &lt;chr&gt; &quot;CBlock&quot;, &quot;CBlock&quot;, &quot;CBlock&quot;, &quot;CBlock&quot;, &quot;PConc&quot;, &quot;P… ## $ Bsmt_Cond &lt;chr&gt; &quot;Good&quot;, &quot;Typical&quot;, &quot;Typical&quot;, &quot;Typical&quot;, &quot;Typical&quot;,… ## $ Bsmt_Exposure &lt;chr&gt; &quot;Gd&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Mn&quot;, &quot;No&quot;, &quot;No… ## $ BsmtFin_Type_1 &lt;chr&gt; &quot;BLQ&quot;, &quot;Rec&quot;, &quot;ALQ&quot;, &quot;ALQ&quot;, &quot;GLQ&quot;, &quot;GLQ&quot;, &quot;GLQ&quot;, &quot;A… ## $ BsmtFin_SF_1 &lt;dbl&gt; 2, 6, 1, 1, 3, 3, 3, 1, 3, 7, 7, 1, 7, 3, 3, 1, 3, … ## $ BsmtFin_Type_2 &lt;chr&gt; &quot;Unf&quot;, &quot;LwQ&quot;, &quot;Unf&quot;, &quot;Unf&quot;, &quot;Unf&quot;, &quot;Unf&quot;, &quot;Unf&quot;, &quot;U… ## $ BsmtFin_SF_2 &lt;dbl&gt; 0, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1120, 0… ## $ Bsmt_Unf_SF &lt;dbl&gt; 441, 270, 406, 1045, 137, 324, 722, 1017, 415, 994,… ## $ Total_Bsmt_SF &lt;dbl&gt; 1080, 882, 1329, 2110, 928, 926, 1338, 1280, 1595, … ## $ Heating &lt;chr&gt; &quot;GasA&quot;, &quot;GasA&quot;, &quot;GasA&quot;, &quot;GasA&quot;, &quot;GasA&quot;, &quot;GasA&quot;, &quot;Ga… ## $ Heating_QC &lt;chr&gt; &quot;Fair&quot;, &quot;Typical&quot;, &quot;Typical&quot;, &quot;Excellent&quot;, &quot;Good&quot;, … ## $ Central_Air &lt;chr&gt; &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;… ## $ Electrical &lt;chr&gt; &quot;SBrkr&quot;, &quot;SBrkr&quot;, &quot;SBrkr&quot;, &quot;SBrkr&quot;, &quot;SBrkr&quot;, &quot;SBrkr… ## $ First_Flr_SF &lt;dbl&gt; 1656, 896, 1329, 2110, 928, 926, 1338, 1280, 1616, … ## $ Second_Flr_SF &lt;dbl&gt; 0, 0, 0, 0, 701, 678, 0, 0, 0, 776, 892, 0, 676, 0,… ## $ Gr_Liv_Area &lt;dbl&gt; 1656, 896, 1329, 2110, 1629, 1604, 1338, 1280, 1616… ## $ Bsmt_Full_Bath &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, … ## $ Bsmt_Half_Bath &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Full_Bath &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 2, … ## $ Half_Bath &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, … ## $ Bedroom_AbvGr &lt;dbl&gt; 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 1, 4, 4, … ## $ Kitchen_AbvGr &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ TotRms_AbvGrd &lt;dbl&gt; 7, 5, 6, 8, 6, 7, 6, 5, 5, 7, 7, 6, 7, 5, 4, 12, 8,… ## $ Functional &lt;chr&gt; &quot;Typ&quot;, &quot;Typ&quot;, &quot;Typ&quot;, &quot;Typ&quot;, &quot;Typ&quot;, &quot;Typ&quot;, &quot;Typ&quot;, &quot;T… ## $ Fireplaces &lt;dbl&gt; 2, 0, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, … ## $ Garage_Type &lt;chr&gt; &quot;Attchd&quot;, &quot;Attchd&quot;, &quot;Attchd&quot;, &quot;Attchd&quot;, &quot;Attchd&quot;, &quot;… ## $ Garage_Finish &lt;chr&gt; &quot;Fin&quot;, &quot;Unf&quot;, &quot;Unf&quot;, &quot;Fin&quot;, &quot;Fin&quot;, &quot;Fin&quot;, &quot;Fin&quot;, &quot;R… ## $ Garage_Cars &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, … ## $ Garage_Area &lt;dbl&gt; 528, 730, 312, 522, 482, 470, 582, 506, 608, 442, 4… ## $ Garage_Cond &lt;chr&gt; &quot;Typical&quot;, &quot;Typical&quot;, &quot;Typical&quot;, &quot;Typical&quot;, &quot;Typica… ## $ Paved_Drive &lt;chr&gt; &quot;Partial_Pavement&quot;, &quot;Paved&quot;, &quot;Paved&quot;, &quot;Paved&quot;, &quot;Pav… ## $ Wood_Deck_SF &lt;dbl&gt; 210, 140, 393, 0, 212, 360, 0, 0, 237, 140, 157, 48… ## $ Open_Porch_SF &lt;dbl&gt; 62, 0, 36, 0, 34, 36, 0, 82, 152, 60, 84, 21, 75, 0… ## $ Enclosed_Porch &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 170, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Three_season_porch &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Screen_Porch &lt;dbl&gt; 0, 120, 0, 0, 0, 0, 0, 144, 0, 0, 0, 0, 0, 0, 140, … ## $ Pool_Area &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Pool_QC &lt;chr&gt; &quot;No_Pool&quot;, &quot;No_Pool&quot;, &quot;No_Pool&quot;, &quot;No_Pool&quot;, &quot;No_Poo… ## $ Fence &lt;chr&gt; &quot;No_Fence&quot;, &quot;Minimum_Privacy&quot;, &quot;No_Fence&quot;, &quot;No_Fenc… ## $ Misc_Feature &lt;chr&gt; &quot;None&quot;, &quot;None&quot;, &quot;Gar2&quot;, &quot;None&quot;, &quot;None&quot;, &quot;None&quot;, &quot;No… ## $ Misc_Val &lt;dbl&gt; 0, 0, 12500, 0, 0, 0, 0, 0, 0, 0, 0, 500, 0, 0, 0, … ## $ Mo_Sold &lt;dbl&gt; 5, 6, 6, 4, 3, 6, 4, 1, 3, 6, 4, 3, 5, 2, 6, 6, 6, … ## $ Year_Sold &lt;dbl&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 201… ## $ Sale_Type &lt;chr&gt; &quot;WD&quot;, &quot;WD&quot;, &quot;WD&quot;, &quot;WD&quot;, &quot;WD&quot;, &quot;WD&quot;, &quot;WD&quot;, &quot;WD&quot;, &quot;WD… ## $ Sale_Condition &lt;chr&gt; &quot;Normal&quot;, &quot;Normal&quot;, &quot;Normal&quot;, &quot;Normal&quot;, &quot;Normal&quot;, &quot;… ## $ Sale_Price &lt;dbl&gt; 215000, 105000, 172000, 244000, 189900, 195500, 213… ## $ Longitude &lt;dbl&gt; -93.61975, -93.61976, -93.61939, -93.61732, -93.638… ## $ Latitude &lt;dbl&gt; 42.05403, 42.05301, 42.05266, 42.05125, 42.06090, 4… 2.4.1 Seleccionar columnas Observamos que nuestros datos tienen 2,930 observaciones y 74 variables, con select() podemos seleccionar las variables que se indiquen. ames_housing %&gt;% select(Lot_Area, Neighborhood, Year_Sold, Sale_Price) ## # A tibble: 2,930 × 4 ## Lot_Area Neighborhood Year_Sold Sale_Price ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 31770 North_Ames 2010 215000 ## 2 11622 North_Ames 2010 105000 ## 3 14267 North_Ames 2010 172000 ## 4 11160 North_Ames 2010 244000 ## 5 13830 Gilbert 2010 189900 ## 6 9978 Gilbert 2010 195500 ## 7 4920 Stone_Brook 2010 213500 ## 8 5005 Stone_Brook 2010 191500 ## 9 5389 Stone_Brook 2010 236500 ## 10 7500 Gilbert 2010 189000 ## # ℹ 2,920 more rows ¡¡ RECORDAR !! El operador pipe (%&gt;%) se usa para conectar un elemento con una función o acción a realizar. En este caso solo se indica que en los datos de ames se seleccionan 4 variables. Con select() y contains() podemos seleccionar variables con alguna cadena de texto. ames_housing %&gt;% select(contains(&quot;Area&quot;)) ## # A tibble: 2,930 × 5 ## Lot_Area Mas_Vnr_Area Gr_Liv_Area Garage_Area Pool_Area ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 31770 112 1656 528 0 ## 2 11622 0 896 730 0 ## 3 14267 108 1329 312 0 ## 4 11160 0 2110 522 0 ## 5 13830 0 1629 482 0 ## 6 9978 20 1604 470 0 ## 7 4920 0 1338 582 0 ## 8 5005 0 1280 506 0 ## 9 5389 0 1616 608 0 ## 10 7500 0 1804 442 0 ## # ℹ 2,920 more rows De igual manera, con select(), ends_with y start_with() podemos seleccionar que inicien o terminen con alguna cadena de texto. ames_housing %&gt;% select(starts_with(&quot;Garage&quot;)) ## # A tibble: 2,930 × 5 ## Garage_Type Garage_Finish Garage_Cars Garage_Area Garage_Cond ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Attchd Fin 2 528 Typical ## 2 Attchd Unf 1 730 Typical ## 3 Attchd Unf 1 312 Typical ## 4 Attchd Fin 2 522 Typical ## 5 Attchd Fin 2 482 Typical ## 6 Attchd Fin 2 470 Typical ## 7 Attchd Fin 2 582 Typical ## 8 Attchd RFn 2 506 Typical ## 9 Attchd RFn 2 608 Typical ## 10 Attchd Fin 2 442 Typical ## # ℹ 2,920 more rows Funciones útiles para select(): contains(): Selecciona variables cuyo nombre contiene la cadena de texto. ends_with(): Selecciona variables cuyo nombre termina con la cadena de caracteres. everything(): Selecciona todas las columnas. matches(): Selecciona las variables cuyos nombres coinciden con una expresión regular. num_range(): Selecciona las variables por posición. start_with(): Selecciona variables cuyos nombres empiezan con la cadena de caracteres. any_of: Selecciona cualquiera de estas variables, en caso de existir EJERCICIO: Crear con datos propios una consulta de columnas usando como variable auxiliar cada una de las listadas anteriormente. Será suficiente con realizar un ejemplo de cada una. 2.4.2 Filtrar observaciones La función filter() nos permite filtrar filas según una condición, primero notemos que la variable Sale_Condition tiene distintas categorías. table(ames_housing$Sale_Condition) ## ## Abnorml AdjLand Alloca Family Normal Partial ## 190 12 24 46 2413 245 ¡¡ SPOILER !! En un modelo predictivo de Machine Learning, no es correcto agregar columnas cuyo valor es conocido hasta el momento de la observación. Es decir, no deben agregarse variables que no se conozca su valor al momento de la predicción, como es el caso de condición de venta. Ahora usaremos la función filter para quedarnos solo con las observaciones con condición de venta “normal”. ames_housing %&gt;% filter(Sale_Condition == &quot;Normal&quot;) ## # A tibble: 2,413 × 74 ## MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 One_Story_1946_and_Ne… Resident… 141 31770 Pave No_A… Slightly… ## 2 One_Story_1946_and_Ne… Resident… 80 11622 Pave No_A… Regular ## 3 One_Story_1946_and_Ne… Resident… 81 14267 Pave No_A… Slightly… ## 4 One_Story_1946_and_Ne… Resident… 93 11160 Pave No_A… Regular ## 5 Two_Story_1946_and_Ne… Resident… 74 13830 Pave No_A… Slightly… ## 6 Two_Story_1946_and_Ne… Resident… 78 9978 Pave No_A… Slightly… ## 7 One_Story_PUD_1946_an… Resident… 41 4920 Pave No_A… Regular ## 8 One_Story_PUD_1946_an… Resident… 43 5005 Pave No_A… Slightly… ## 9 One_Story_PUD_1946_an… Resident… 39 5389 Pave No_A… Slightly… ## 10 Two_Story_1946_and_Ne… Resident… 60 7500 Pave No_A… Regular ## # ℹ 2,403 more rows ## # ℹ 67 more variables: Land_Contour &lt;chr&gt;, Utilities &lt;chr&gt;, Lot_Config &lt;chr&gt;, ## # Land_Slope &lt;chr&gt;, Neighborhood &lt;chr&gt;, Condition_1 &lt;chr&gt;, Condition_2 &lt;chr&gt;, ## # Bldg_Type &lt;chr&gt;, House_Style &lt;chr&gt;, Overall_Cond &lt;chr&gt;, Year_Built &lt;dbl&gt;, ## # Year_Remod_Add &lt;dbl&gt;, Roof_Style &lt;chr&gt;, Roof_Matl &lt;chr&gt;, ## # Exterior_1st &lt;chr&gt;, Exterior_2nd &lt;chr&gt;, Mas_Vnr_Type &lt;chr&gt;, ## # Mas_Vnr_Area &lt;dbl&gt;, Exter_Cond &lt;chr&gt;, Foundation &lt;chr&gt;, Bsmt_Cond &lt;chr&gt;, … También se puede usar para filtrar variables numéricas: ames_housing %&gt;% filter(Lot_Area &gt; 1000 &amp; Sale_Price &gt;= 150000) ## # A tibble: 1,677 × 74 ## MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 One_Story_1946_and_Ne… Resident… 141 31770 Pave No_A… Slightly… ## 2 One_Story_1946_and_Ne… Resident… 81 14267 Pave No_A… Slightly… ## 3 One_Story_1946_and_Ne… Resident… 93 11160 Pave No_A… Regular ## 4 Two_Story_1946_and_Ne… Resident… 74 13830 Pave No_A… Slightly… ## 5 Two_Story_1946_and_Ne… Resident… 78 9978 Pave No_A… Slightly… ## 6 One_Story_PUD_1946_an… Resident… 41 4920 Pave No_A… Regular ## 7 One_Story_PUD_1946_an… Resident… 43 5005 Pave No_A… Slightly… ## 8 One_Story_PUD_1946_an… Resident… 39 5389 Pave No_A… Slightly… ## 9 Two_Story_1946_and_Ne… Resident… 60 7500 Pave No_A… Regular ## 10 Two_Story_1946_and_Ne… Resident… 75 10000 Pave No_A… Slightly… ## # ℹ 1,667 more rows ## # ℹ 67 more variables: Land_Contour &lt;chr&gt;, Utilities &lt;chr&gt;, Lot_Config &lt;chr&gt;, ## # Land_Slope &lt;chr&gt;, Neighborhood &lt;chr&gt;, Condition_1 &lt;chr&gt;, Condition_2 &lt;chr&gt;, ## # Bldg_Type &lt;chr&gt;, House_Style &lt;chr&gt;, Overall_Cond &lt;chr&gt;, Year_Built &lt;dbl&gt;, ## # Year_Remod_Add &lt;dbl&gt;, Roof_Style &lt;chr&gt;, Roof_Matl &lt;chr&gt;, ## # Exterior_1st &lt;chr&gt;, Exterior_2nd &lt;chr&gt;, Mas_Vnr_Type &lt;chr&gt;, ## # Mas_Vnr_Area &lt;dbl&gt;, Exter_Cond &lt;chr&gt;, Foundation &lt;chr&gt;, Bsmt_Cond &lt;chr&gt;, … Notemos que en el ejemplo anterior se usa &amp;, que ayuda a filtrar por dos condiciones. También puede usarse | para filtrar por alguna de las dos condiciones. ames_housing %&gt;% filter(Lot_Area &lt; 1000 | Sale_Price &lt;= 150000) ## # A tibble: 1,271 × 74 ## MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 One_Story_1946_and_Ne… Resident… 80 11622 Pave No_A… Regular ## 2 One_Story_1946_and_Ne… Resident… 140 19138 Pave No_A… Regular ## 3 One_Story_1946_and_Ne… Resident… 0 11241 Pave No_A… Slightly… ## 4 One_Story_1946_and_Ne… Resident… 0 12537 Pave No_A… Slightly… ## 5 One_Story_1946_and_Ne… Resident… 65 8450 Pave No_A… Regular ## 6 One_Story_1946_and_Ne… Resident… 70 8400 Pave No_A… Regular ## 7 One_Story_1946_and_Ne… Resident… 70 10500 Pave No_A… Regular ## 8 Two_Story_PUD_1946_an… Resident… 21 1680 Pave No_A… Regular ## 9 Two_Story_PUD_1946_an… Resident… 21 1680 Pave No_A… Regular ## 10 Two_Story_PUD_1946_an… Resident… 21 1680 Pave No_A… Regular ## # ℹ 1,261 more rows ## # ℹ 67 more variables: Land_Contour &lt;chr&gt;, Utilities &lt;chr&gt;, Lot_Config &lt;chr&gt;, ## # Land_Slope &lt;chr&gt;, Neighborhood &lt;chr&gt;, Condition_1 &lt;chr&gt;, Condition_2 &lt;chr&gt;, ## # Bldg_Type &lt;chr&gt;, House_Style &lt;chr&gt;, Overall_Cond &lt;chr&gt;, Year_Built &lt;dbl&gt;, ## # Year_Remod_Add &lt;dbl&gt;, Roof_Style &lt;chr&gt;, Roof_Matl &lt;chr&gt;, ## # Exterior_1st &lt;chr&gt;, Exterior_2nd &lt;chr&gt;, Mas_Vnr_Type &lt;chr&gt;, ## # Mas_Vnr_Area &lt;dbl&gt;, Exter_Cond &lt;chr&gt;, Foundation &lt;chr&gt;, Bsmt_Cond &lt;chr&gt;, … Las condiciones pueden ser expresiones lógicas construidas mediante los operadores relacionales y lógicos: &lt; : Menor que &gt; : Mayor que == : Igual que &lt;= : Menor o igual que &gt;= : Mayor o igual que != : Diferente que %in% : Pertenece al conjunto is.na : Es NA !is.na : No es NA EJERCICIO: Practicar la función de filtro de observaciones usando los operadores auxiliares. Concatenar el resultado de seleccionar columnas y posteriormente filtrar columnas. 2.4.3 Ordenar registros La función arrange() se utiliza para ordenar las filas de un data frame de acuerdo a una o varias variables. Este ordenamiento puede ser ascendente o descendente. Por defecto arrange() ordena las filas por orden ascendente: ames_housing %&gt;% arrange(Sale_Price) ## # A tibble: 2,930 × 74 ## MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 One_Story_1945_and_Ol… Resident… 68 9656 Pave No_A… Regular ## 2 One_Story_1946_and_Ne… A_agr 80 14584 Pave No_A… Regular ## 3 One_Story_1945_and_Ol… C_all 60 7879 Pave No_A… Regular ## 4 One_Story_1945_and_Ol… Resident… 60 8088 Pave Grav… Regular ## 5 One_Story_1946_and_Ne… C_all 50 9000 Pave No_A… Regular ## 6 One_and_Half_Story_Fi… Resident… 50 5925 Pave No_A… Regular ## 7 One_Story_1946_and_Ne… Resident… 50 5000 Pave No_A… Regular ## 8 Two_Story_1945_and_Ol… C_all 50 8500 Pave Paved Regular ## 9 One_Story_1945_and_Ol… C_all 72 9392 Pave No_A… Regular ## 10 One_Story_1945_and_Ol… Resident… 50 5925 Pave No_A… Regular ## # ℹ 2,920 more rows ## # ℹ 67 more variables: Land_Contour &lt;chr&gt;, Utilities &lt;chr&gt;, Lot_Config &lt;chr&gt;, ## # Land_Slope &lt;chr&gt;, Neighborhood &lt;chr&gt;, Condition_1 &lt;chr&gt;, Condition_2 &lt;chr&gt;, ## # Bldg_Type &lt;chr&gt;, House_Style &lt;chr&gt;, Overall_Cond &lt;chr&gt;, Year_Built &lt;dbl&gt;, ## # Year_Remod_Add &lt;dbl&gt;, Roof_Style &lt;chr&gt;, Roof_Matl &lt;chr&gt;, ## # Exterior_1st &lt;chr&gt;, Exterior_2nd &lt;chr&gt;, Mas_Vnr_Type &lt;chr&gt;, ## # Mas_Vnr_Area &lt;dbl&gt;, Exter_Cond &lt;chr&gt;, Foundation &lt;chr&gt;, Bsmt_Cond &lt;chr&gt;, … Si las queremos ordenar de forma ascendente, lo haremos del siguiente modo: ames_housing %&gt;% arrange(desc(Sale_Price)) ## # A tibble: 2,930 × 74 ## MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Two_Story_1946_and_Ne… Resident… 104 21535 Pave No_A… Slightly… ## 2 Two_Story_1946_and_Ne… Resident… 160 15623 Pave No_A… Slightly… ## 3 Two_Story_1946_and_Ne… Resident… 118 35760 Pave No_A… Slightly… ## 4 One_Story_1946_and_Ne… Resident… 106 12720 Pave No_A… Regular ## 5 One_Story_1946_and_Ne… Resident… 100 12919 Pave No_A… Slightly… ## 6 One_Story_1946_and_Ne… Resident… 105 13693 Pave No_A… Regular ## 7 One_Story_1946_and_Ne… Resident… 52 51974 Pave No_A… Slightly… ## 8 Two_Story_1946_and_Ne… Resident… 114 17242 Pave No_A… Slightly… ## 9 Two_Story_1946_and_Ne… Resident… 107 13891 Pave No_A… Regular ## 10 Two_Story_1946_and_Ne… Resident… 85 16056 Pave No_A… Slightly… ## # ℹ 2,920 more rows ## # ℹ 67 more variables: Land_Contour &lt;chr&gt;, Utilities &lt;chr&gt;, Lot_Config &lt;chr&gt;, ## # Land_Slope &lt;chr&gt;, Neighborhood &lt;chr&gt;, Condition_1 &lt;chr&gt;, Condition_2 &lt;chr&gt;, ## # Bldg_Type &lt;chr&gt;, House_Style &lt;chr&gt;, Overall_Cond &lt;chr&gt;, Year_Built &lt;dbl&gt;, ## # Year_Remod_Add &lt;dbl&gt;, Roof_Style &lt;chr&gt;, Roof_Matl &lt;chr&gt;, ## # Exterior_1st &lt;chr&gt;, Exterior_2nd &lt;chr&gt;, Mas_Vnr_Type &lt;chr&gt;, ## # Mas_Vnr_Area &lt;dbl&gt;, Exter_Cond &lt;chr&gt;, Foundation &lt;chr&gt;, Bsmt_Cond &lt;chr&gt;, … Si se desea usar dos o más columnas para realizar el ordenamiento, deben separarse por comas cada una de las características ames_housing %&gt;% arrange(Sale_Condition, desc(Sale_Price), Lot_Area) %&gt;% select(Sale_Condition, Sale_Price, Lot_Area) ## # A tibble: 2,930 × 3 ## Sale_Condition Sale_Price Lot_Area ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Abnorml 745000 15623 ## 2 Abnorml 552000 14836 ## 3 Abnorml 475000 11778 ## 4 Abnorml 390000 13418 ## 5 Abnorml 328900 5119 ## 6 Abnorml 310000 14541 ## 7 Abnorml 290000 9950 ## 8 Abnorml 287000 15498 ## 9 Abnorml 258000 12090 ## 10 Abnorml 257000 10994 ## # ℹ 2,920 more rows Notemos que en el ejemplo anterior usamos dos pipes (%&gt;%), como habíamos mencionado se pueden usar los necesarios para combinar funciones. 2.4.4 Agregar / Modificar Con la función mutate() podemos computar transformaciones de variables en un data frame. A menudo, tendremos la necesidad de crear nuevas variables que se calculan a partir de variables existentes. La función mutate() proporciona una interfaz clara para realizar este tipo de operaciones. Por ejemplo, haremos el cálculo de la antigüedad del inmueble a partir de las variables Year_Sold y Year_Remod_Add: ejemplo_mutate &lt;- ames_housing %&gt;% select(Year_Sold, Year_Remod_Add) %&gt;% mutate(Antique = Year_Sold - Year_Remod_Add) ejemplo_mutate ## # A tibble: 2,930 × 3 ## Year_Sold Year_Remod_Add Antique ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2010 1960 50 ## 2 2010 1961 49 ## 3 2010 1958 52 ## 4 2010 1968 42 ## 5 2010 1998 12 ## 6 2010 1998 12 ## 7 2010 2001 9 ## 8 2010 1992 18 ## 9 2010 1996 14 ## 10 2010 1999 11 ## # ℹ 2,920 more rows El ejemplo anterior crea una nueva variable. Ahora se presenta otro ejemplo en donde se modifica una variable ya creada. ejemplo_mutate %&gt;% mutate(Antique = Antique * 12) ## # A tibble: 2,930 × 3 ## Year_Sold Year_Remod_Add Antique ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2010 1960 600 ## 2 2010 1961 588 ## 3 2010 1958 624 ## 4 2010 1968 504 ## 5 2010 1998 144 ## 6 2010 1998 144 ## 7 2010 2001 108 ## 8 2010 1992 216 ## 9 2010 1996 168 ## 10 2010 1999 132 ## # ℹ 2,920 more rows En este segundo ejemplo, se modifica el número de años de antigüedad y se multiplica por un factor de 12 para modificar el tiempo en una escala de meses. 2.4.5 Resumen estadístico La función summarise() se comporta de forma análoga a la función mutate(), excepto que en lugar de añadir nuevas columnas crea un nuevo data frame. Podemos usar el ejemplo anterior y calcular la media de la variable creada Antique: ames_housing %&gt;% select(Year_Sold, Year_Remod_Add) %&gt;% mutate(Antique = Year_Sold - Year_Remod_Add) %&gt;% summarise(Mean_Antique = mean(Antique)) ## # A tibble: 1 × 1 ## Mean_Antique ## &lt;dbl&gt; ## 1 23.5 Solo fue necesario agregar un pipe, especificar el nombre de la variable creada y la operación a realizar. A continuación se muestran funciones que trabajando conjuntamente con la función summarise() facilitarán nuestro trabajo diario. Las primeras pertenecen al paquete base y las otras son del paquete dplyr. Todas ellas toman como argumento un vector y devuelven un único resultado: min(), max() : Valores max y min. mean() : Media. median() : Mediana. sum() : Suma de los valores. var(), sd() : Varianza y desviación estándar. first() : Primer valor en un vector. last() : El último valor en un vector n() : El número de valores en un vector. n_distinct() : El número de valores distintos en un vector. nth() : Extrae el valor que ocupa la posición n en un vector. Mas adelante veremos como combinar esta función con la función group_by() para calcular estadísticos agrupados por alguna característica de interés. EJERCICIO: Realizar una consulta usando summarise() y cada una de las funciones estadísticas listadas anteriormente. 2.4.6 Agrupamiento La función group_by() agrupa un conjunto de filas de acuerdo con los valores de una o más columnas o expresiones. Usaremos el ejemplo anterior. Primero creamos nuestra nueva variable Antique, después agrupamos por vecindario y al final calculamos la media de la variable Antique. Gracias al agrupamiento, nos regresara una media por cada grupo creado, es decir, nos regresara el promedio de la antigüedad por vecindario. ames_housing %&gt;% mutate(Antique = Year_Sold - Year_Remod_Add) %&gt;% group_by(Neighborhood) %&gt;% summarise(Mean_Antique = round(mean(Antique), 0)) ## # A tibble: 28 × 2 ## Neighborhood Mean_Antique ## &lt;chr&gt; &lt;dbl&gt; ## 1 Bloomington_Heights 2 ## 2 Blueste 25 ## 3 Briardale 35 ## 4 Brookside 39 ## 5 Clear_Creek 28 ## 6 College_Creek 8 ## 7 Crawford 29 ## 8 Edwards 33 ## 9 Gilbert 9 ## 10 Green_Hills 14 ## # ℹ 18 more rows ¡¡ RECORDAR !! En este link se encuentra un buen resumen de las funciones básicas de dplyr 2.5 Orden y estructura Un conjunto de datos puede ser representado de muchas maneras distintas y contener en todos los casos la misma información. Sin embargo, no todos los modos en que se presenta la información resulta óptimo para su procesamiento y análisis. Los conjuntos de datos ordenados serán más fáciles de trabajar y analizar. Algunas de las características principales que presentan los conjuntos de datos ordenados son las siguientes: Cada variable debe tener su propia columna. Cada observación debe tener su propio renglón. Cada valor debe tener su propia celda. La figura anterior muestra la estructura de orden que debe tener un conjunto de datos. A pesar de que pueda parecer intuitivo y sencillo, en la práctica es considerable el número de conjuntos de datos desordenados. La limpieza y ordenamiento debe ser trabajado de forma impecable a fin de que puedan realizarse buenas prácticas. El tiempo de limpieza y ordenamiento varía mucho dependiendo de la dimensión del conjunto de datos. Algunos de los principales problemas que pueden tener los conjuntos de datos no ordenados son: Una variable puede estar dispersa en múltiples columnas Una observación puede estar esparcida en múltiples renglones La paquetería tidyr cuenta con funciones para resolver dichos problemas. Entre las principales funciones que tiene la paquetería, se encuentran pivot_longer(), pivot_wider(), separate() y unite(), mismas que se analizarán a continuación. 2.5.1 Pivote horizontal La función pivot_wider() resulta muy útil a la hora de organizar los datos. Su función consiste en dispersar una variable clave en múltiples columnas. Lo primero que se debe hacer para poder hacer uso de dicha función es instalar y cargar la librería. El siguiente conjunto de datos contiene el número de localidades rurales y urbanas por municipio de la Ciudad de México. Como es posible observar, algunos municipios aparecen más de una vez en el marco de datos, esto se debe a que cada municipio puede tener ambos ámbitos, rural y urbano. Para hacer que el conjunto de datos sea ordenado, es necesario que cada observación aparezca una sola vez por renglón y cada una de las categorías (rural y urbano) de la variable “Ámbito” deberá ocupar el lugar de una columna. El siguiente código muestra cómo convertir los datos no ordenados en un conjunto ordenado. library(tidyr) Resumen &lt;- readRDS(&quot;data/loc_mun_cdmx.rds&quot;) Resumen %&gt;% pivot_wider( names_from = Ambito, values_from = Total_localidades ) ## # A tibble: 16 × 3 ## NOM_MUN Rural Urbano ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Álvaro Obregón 3 1 ## 2 La Magdalena Contreras 8 1 ## 3 Cuajimalpa de Morelos 14 2 ## 4 Tláhuac 31 5 ## 5 Xochimilco 78 1 ## 6 Tlalpan 95 4 ## 7 Milpa Alta 187 10 ## 8 Azcapotzalco NA 1 ## 9 Benito Juárez NA 1 ## 10 Coyoacán NA 1 ## 11 Cuauhtémoc NA 1 ## 12 Gustavo A. Madero NA 1 ## 13 Iztacalco NA 1 ## 14 Iztapalapa NA 1 ## 15 Miguel Hidalgo NA 1 ## 16 Venustiano Carranza NA 1 En la tabla actual existe ahora un y solo un registro por cada observación (nombre de municipio en este caso). El valor que le corresponde a cada una de las columnas creadas es la frecuencia absoluta de localidades que tienen la característica “Rural” y “Urbano” respectivamente. Pero… ¿qué pasa cuando no existen todos los valores en ambas columnas? Si no se especifica la manera de llenar los datos faltantes, estos contendrán NAs. Siempre se puede elegir el caracter o número con el cual se imputan los datos faltantes. fish_encounters %&gt;% pivot_wider(names_from = station, values_from = seen) ## # A tibble: 19 × 12 ## fish Release I80_1 Lisbon Rstr Base_TD BCE BCW BCE2 BCW2 MAE MAW ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 4842 1 1 1 1 1 1 1 1 1 1 1 ## 2 4843 1 1 1 1 1 1 1 1 1 1 1 ## 3 4844 1 1 1 1 1 1 1 1 1 1 1 ## 4 4845 1 1 1 1 1 NA NA NA NA NA NA ## 5 4847 1 1 1 NA NA NA NA NA NA NA NA ## 6 4848 1 1 1 1 NA NA NA NA NA NA NA ## 7 4849 1 1 NA NA NA NA NA NA NA NA NA ## 8 4850 1 1 NA 1 1 1 1 NA NA NA NA ## 9 4851 1 1 NA NA NA NA NA NA NA NA NA ## 10 4854 1 1 NA NA NA NA NA NA NA NA NA ## 11 4855 1 1 1 1 1 NA NA NA NA NA NA ## 12 4857 1 1 1 1 1 1 1 1 1 NA NA ## 13 4858 1 1 1 1 1 1 1 1 1 1 1 ## 14 4859 1 1 1 1 1 NA NA NA NA NA NA ## 15 4861 1 1 1 1 1 1 1 1 1 1 1 ## 16 4862 1 1 1 1 1 1 1 1 1 NA NA ## 17 4863 1 1 NA NA NA NA NA NA NA NA NA ## 18 4864 1 1 NA NA NA NA NA NA NA NA NA ## 19 4865 1 1 1 NA NA NA NA NA NA NA NA fish_encounters %&gt;% pivot_wider(names_from = station, values_from = seen, values_fill = 0) ## # A tibble: 19 × 12 ## fish Release I80_1 Lisbon Rstr Base_TD BCE BCW BCE2 BCW2 MAE MAW ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 4842 1 1 1 1 1 1 1 1 1 1 1 ## 2 4843 1 1 1 1 1 1 1 1 1 1 1 ## 3 4844 1 1 1 1 1 1 1 1 1 1 1 ## 4 4845 1 1 1 1 1 0 0 0 0 0 0 ## 5 4847 1 1 1 0 0 0 0 0 0 0 0 ## 6 4848 1 1 1 1 0 0 0 0 0 0 0 ## 7 4849 1 1 0 0 0 0 0 0 0 0 0 ## 8 4850 1 1 0 1 1 1 1 0 0 0 0 ## 9 4851 1 1 0 0 0 0 0 0 0 0 0 ## 10 4854 1 1 0 0 0 0 0 0 0 0 0 ## 11 4855 1 1 1 1 1 0 0 0 0 0 0 ## 12 4857 1 1 1 1 1 1 1 1 1 0 0 ## 13 4858 1 1 1 1 1 1 1 1 1 1 1 ## 14 4859 1 1 1 1 1 0 0 0 0 0 0 ## 15 4861 1 1 1 1 1 1 1 1 1 1 1 ## 16 4862 1 1 1 1 1 1 1 1 1 0 0 ## 17 4863 1 1 0 0 0 0 0 0 0 0 0 ## 18 4864 1 1 0 0 0 0 0 0 0 0 0 ## 19 4865 1 1 1 0 0 0 0 0 0 0 0 Ejercicio: Realiza un pivote horizontal a través del ámbito y el total de localidades. Rellena los datos faltantes con ceros. En caso de que existan múltiples columnas que se desean dispersar mediante el pivote de una columna con múltiples categorías, es posible especificar tal re estructuración a través del siguiente código. us_rent_income %&gt;% arrange(NAME) ## # A tibble: 104 × 5 ## GEOID NAME variable estimate moe ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 Alabama income 24476 136 ## 2 01 Alabama rent 747 3 ## 3 02 Alaska income 32940 508 ## 4 02 Alaska rent 1200 13 ## 5 04 Arizona income 27517 148 ## 6 04 Arizona rent 972 4 ## 7 05 Arkansas income 23789 165 ## 8 05 Arkansas rent 709 5 ## 9 06 California income 29454 109 ## 10 06 California rent 1358 3 ## # ℹ 94 more rows us_rent_income %&gt;% pivot_wider(names_from = variable, values_from = c(estimate, moe)) ## # A tibble: 52 × 6 ## GEOID NAME estimate_income estimate_rent moe_income moe_rent ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 Alabama 24476 747 136 3 ## 2 02 Alaska 32940 1200 508 13 ## 3 04 Arizona 27517 972 148 4 ## 4 05 Arkansas 23789 709 165 5 ## 5 06 California 29454 1358 109 3 ## 6 08 Colorado 32401 1125 109 5 ## 7 09 Connecticut 35326 1123 195 5 ## 8 10 Delaware 31560 1076 247 10 ## 9 11 District of Columbia 43198 1424 681 17 ## 10 12 Florida 25952 1077 70 3 ## # ℹ 42 more rows Ejercicio: Agrupa los datos de localidades por ámbito Agrega una columna con el porcentaje de localidades por alcaldía Realiza un pivote horizontal sobre el ámbito y las variables numéricas de total de localidades y su respectivo porcentaje creado en el paso anterior Ordena los registros de forma descendente de acuerdo con el total de localidades rural y urbano. Adicionalmente, se puede especificar una función de agregación que operara antes de acomodar los datos en las respectivas columnas indicadas. Un ejemplo de funciones agregadas en la re estructuración de tabla se muestra a continuación, donde se muestra la media de los valores en las categorías tension y breaks. warpbreaks &lt;- warpbreaks[c(&quot;wool&quot;, &quot;tension&quot;, &quot;breaks&quot;)] %&gt;% as_tibble() warpbreaks ## # A tibble: 54 × 3 ## wool tension breaks ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 A L 26 ## 2 A L 30 ## 3 A L 54 ## 4 A L 25 ## 5 A L 70 ## 6 A L 52 ## 7 A L 51 ## 8 A L 26 ## 9 A L 67 ## 10 A M 18 ## # ℹ 44 more rows warpbreaks %&gt;% pivot_wider( names_from = wool, values_from = breaks, values_fn = ~mean(.x, na.rm = T) ) ## # A tibble: 3 × 3 ## tension A B ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 L 44.6 28.2 ## 2 M 24 28.8 ## 3 H 24.6 18.8 Ejercicio: Sobre el conjunto de localidades crea una variable con 5 categorías numéricas creadas aleatoriamente. Elimina la columna con el nombre del municipio. Crea un pivote horizontal con el ámbito, sumando el total de localidades y rellenando con ceros los datos faltantes. Ordena las categorías numéricas de forma ascendente. 2.5.2 Pivote vertical pivot_longer() es podría ser la función inversa de la anterior, se necesita comúnmente para ordenar los conjuntos de datos capturados en crudo, ya que a menudo no son capturados acorde a las mejores estructuras para facilitar el análisis. El conjunto de datos relig_income almacena recuentos basados en una encuesta que (entre otras cosas) preguntó a las personas sobre su religión e ingresos anuales: relig_income ## # A tibble: 18 × 11 ## religion `&lt;$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k` `$75-100k` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Agnostic 27 34 60 81 76 137 122 ## 2 Atheist 12 27 37 52 35 70 73 ## 3 Buddhist 27 21 30 34 33 58 62 ## 4 Catholic 418 617 732 670 638 1116 949 ## 5 Don’t k… 15 14 15 11 10 35 21 ## 6 Evangel… 575 869 1064 982 881 1486 949 ## 7 Hindu 1 9 7 9 11 34 47 ## 8 Histori… 228 244 236 238 197 223 131 ## 9 Jehovah… 20 27 24 24 21 30 15 ## 10 Jewish 19 19 25 25 30 95 69 ## 11 Mainlin… 289 495 619 655 651 1107 939 ## 12 Mormon 29 40 48 51 56 112 85 ## 13 Muslim 6 7 9 10 9 23 16 ## 14 Orthodox 13 17 23 32 32 47 38 ## 15 Other C… 9 7 11 13 13 14 18 ## 16 Other F… 20 33 40 46 49 63 46 ## 17 Other W… 5 2 3 4 2 7 3 ## 18 Unaffil… 217 299 374 365 341 528 407 ## # ℹ 3 more variables: `$100-150k` &lt;dbl&gt;, `&gt;150k` &lt;dbl&gt;, ## # `Don&#39;t know/refused` &lt;dbl&gt; ¿Crees que ésta es la mejor estructura para la tabla? ¿Cómo imaginas que podría modificarse? Este conjunto de datos contiene tres variables: religión, almacenada en las filas income repartidos entre los nombres de columna count almacenado en los valores de las celdas. Para ordenarlo usamos pivot_longer(): relig_income %&gt;% pivot_longer(cols = -religion, names_to = &quot;income&quot;, values_to = &quot;count&quot;) ## # A tibble: 180 × 3 ## religion income count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Agnostic &lt;$10k 27 ## 2 Agnostic $10-20k 34 ## 3 Agnostic $20-30k 60 ## 4 Agnostic $30-40k 81 ## 5 Agnostic $40-50k 76 ## 6 Agnostic $50-75k 137 ## 7 Agnostic $75-100k 122 ## 8 Agnostic $100-150k 109 ## 9 Agnostic &gt;150k 84 ## 10 Agnostic Don&#39;t know/refused 96 ## # ℹ 170 more rows El primer argumento es el conjunto de datos para remodelar, relig_income. El segundo argumento describe qué columnas necesitan ser reformadas. En este caso, es cada columna aparte de religion. El names_to da el nombre de la variable que se creará a partir de los datos almacenados en los nombres de columna, es decir, ingresos. Los values_to dan el nombre de la variable que se creará a partir de los datos almacenados en el valor de la celda, es decir, count. Ni la columna names_to ni la values_to existen en relig_income, por lo que las proporcionamos como cadenas de caracteres entre comillas. 2.5.3 Unión de columnas Es común que en los conjuntos de datos exista información esparcida en distintas columnas que sería deseable (en muy pocas ocasiones) tenerlas en una sola columna. Algunos ejemplos de esta situación deseable son las fechas y claves geoestadísticas. La función unite() sirve para concatenar el contenido de columnas mediante un separador elegible. Se usará la variable de la clave geoestadística de localidades del país como ejemplo. El formato para las claves geoestadísticas para estado, municipio y localidad son claves alfanuméricas que contienen 2, 3 y 4 caracteres respectivamente. Es indispensable que al trabajar con claves geoestadísticas, las claves estén en su formato original. A continuación se hará la homologación de las claves para usar la función unite(). library(magrittr) library(readxl) library(stringr) Datos &lt;- read_excel(&quot;data/Margin CONAPO.xlsx&quot;, sheet = &quot;Margin CONAPO&quot;) Datos ## # A tibble: 107,458 × 21 ## ENT NOM_ENT MUN NOM_MUN LOC NOM_LOC POB_TOT VPH ANAL10 SPRIM10 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Aguascalient… 1 Aguasc… 1 Aguasc… 722250 184123 2.26 10.9 ## 2 1 Aguascalient… 1 Aguasc… 96 Agua A… 37 11 17.9 48.1 ## 3 1 Aguascalient… 1 Aguasc… 104 Ardill… 14 3 0 20 ## 4 1 Aguascalient… 1 Aguasc… 106 Arella… 1382 255 5.60 24.7 ## 5 1 Aguascalient… 1 Aguasc… 112 Bajío … 55 11 14.3 38.1 ## 6 1 Aguascalient… 1 Aguasc… 114 Reside… 757 202 0 1.63 ## 7 1 Aguascalient… 1 Aguasc… 120 Buenav… 935 217 10.7 29.5 ## 8 1 Aguascalient… 1 Aguasc… 121 Cabeci… 184 44 4.55 32.6 ## 9 1 Aguascalient… 1 Aguasc… 125 Cañada… 395 82 8.86 23.9 ## 10 1 Aguascalient… 1 Aguasc… 126 Cañada… 509 123 4.75 19.6 ## # ℹ 107,448 more rows ## # ℹ 11 more variables: SEXC10 &lt;dbl&gt;, SEE10 &lt;dbl&gt;, SAGUAE10 &lt;dbl&gt;, ## # PROM_OCC10 &lt;dbl&gt;, PISOTIE10 &lt;dbl&gt;, SREFRI10 &lt;dbl&gt;, IM_2010 &lt;dbl&gt;, ## # GM_2010 &lt;chr&gt;, IMC0A100 &lt;dbl&gt;, LUG_NAL &lt;dbl&gt;, LUG_EDO &lt;dbl&gt; Como puede apreciarse en la tabla anterior, las claves de los campos Ent, Mun y Loc aparecen como numéricos. La estructura deseada para estos campos es de tipo alfanumérico y de longitud 2, 3 y 4 respectivamente. Para lograr esta estructura de datos, es necesario concatenar tantos ceros como sean necesarios antes del valor actual hasta lograr la longitud deseada. Datos2 &lt;- Datos %&gt;% select(ENT, MUN, LOC) Datos2$ENT %&lt;&gt;% str_pad(width = 2, side = &quot;left&quot;, pad = &quot;0&quot;) Datos2$MUN %&lt;&gt;% str_pad(width = 3, side = &quot;left&quot;, pad = &quot;0&quot;) Datos2$LOC %&lt;&gt;% str_pad(width = 4, side = &quot;left&quot;, pad = &quot;0&quot;) Datos2 %&gt;% head(5) ## # A tibble: 5 × 3 ## ENT MUN LOC ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 01 001 0001 ## 2 01 001 0096 ## 3 01 001 0104 ## 4 01 001 0106 ## 5 01 001 0112 Datos2 %&gt;% unite(&quot;CVE_GEO&quot;, c(&quot;ENT&quot;,&quot;MUN&quot;,&quot;LOC&quot;), sep=&quot;&quot;, remove = F) %&gt;% head(5) ## # A tibble: 5 × 4 ## CVE_GEO ENT MUN LOC ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 010010001 01 001 0001 ## 2 010010096 01 001 0096 ## 3 010010104 01 001 0104 ## 4 010010106 01 001 0106 ## 5 010010112 01 001 0112 Datos2 %&gt;% unite(&quot;CVE_GEO&quot;, c(&quot;ENT&quot;,&quot;MUN&quot;,&quot;LOC&quot;), sep=&quot;/&quot;,remove = T) %&gt;% head(5) ## # A tibble: 5 × 1 ## CVE_GEO ## &lt;chr&gt; ## 1 01/001/0001 ## 2 01/001/0096 ## 3 01/001/0104 ## 4 01/001/0106 ## 5 01/001/0112 En el código anterior se carga la librería magrittr para poder hacer uso del operador pipe doble “%&lt;&gt;%”, que permite al igual que el operador pipe simple “%&gt;%”, usar como argumento al primer elemento y mandarlo hacia la función definida, además de guardar el resultado final de la cadena de pipes en el argumento original que fue usado como insumo para la función. Es importante tener en cuenta que el dato será reescrito y no se podrá tener acceso a su información almacenada antes de ser usado el operador. Es opción del programador poder eliminar las variables originales que crearon la nueva variable o mantenerlas en el conjunto de datos. Esta opción está disponible en el parámetro remove de la función unite(). 2.5.4 Separador de columnas Los procesos que se han visto hasta ahora han tenido cada uno una función inversa, este es también el caso de la función unite que tiene por objetivo unir dos o más columnas en una. La función separate() separará una columna en dos o más dependiendo de la longitud que tenga y de las especificaciones de separación. Datos_unite1 &lt;- Datos2 %&gt;% unite(&quot;CVE_GEO&quot;, c(&quot;ENT&quot;,&quot;MUN&quot;,&quot;LOC&quot;), sep = &quot;&quot;, remove = T) Datos_unite1 %&gt;% head(5) ## # A tibble: 5 × 1 ## CVE_GEO ## &lt;chr&gt; ## 1 010010001 ## 2 010010096 ## 3 010010104 ## 4 010010106 ## 5 010010112 Datos_unite1 %&gt;% separate(&quot;CVE_GEO&quot;, c(&quot;EDO&quot;,&quot;MUNI&quot;,&quot;LOC&quot;), sep = c(2, 5), remove=F) %&gt;% head(5) ## # A tibble: 5 × 4 ## CVE_GEO EDO MUNI LOC ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 010010001 01 001 0001 ## 2 010010096 01 001 0096 ## 3 010010104 01 001 0104 ## 4 010010106 01 001 0106 ## 5 010010112 01 001 0112 Ya sea que se le especifique el número de caracteres que debe de contar para hacer un corte o que se le indique qué caracter debe identificar para hacer la separación, la función separate() puede dividir la columna indicada y crear nuevas a partir de la original. "],["visualización.html", "Capítulo 3 Visualización 3.1 EDA: Análisis Exploratorio de Datos 3.2 GEDA: Análisis Exploratorio de Datos Gráficos 3.3 Ggplot 3.4 Análisis univariado 3.5 Análisis multivariado 3.6 Visualización interactiva 3.7 Reporte interactivos", " Capítulo 3 Visualización “El análisis exploratorio de datos se refiere al proceso de realizar investigaciones iniciales sobre los datos para descubrir patrones, detectar anomalías, probar hipótesis y verificar suposiciones con la ayuda de estadísticas resumidas y representaciones gráficas.” Towards 3.1 EDA: Análisis Exploratorio de Datos Un análisis exploratorio de datos tiene principalmente 5 objetivos: Maximizar el conocimiento de un conjunto de datos Descubrir la estructura subyacente de los datos Extraer variables importantes Detectar valores atípicos y anomalías Probar los supuestos subyacentes EDA no es idéntico a los gráficos estadísticos aunque los dos términos se utilizan casi indistintamente. Los gráficos estadísticos son una colección de técnicas, todas basadas en gráficos y todas centradas en un aspecto de caracterización de datos. EDA abarca un lugar más grande. EDA es una filosofía sobre cómo diseccionar un conjunto de datos; lo que buscamos; cómo nos vemos; y cómo interpretamos. Los científicos de datos pueden utilizar el análisis exploratorio para garantizar que los resultados que producen sean válidos y aplicables a los resultados y objetivos comerciales deseados. EDA se utiliza principalmente para ver qué datos pueden revelar más allá del modelado formal o la tarea de prueba de hipótesis y proporciona una mejor comprensión de las variables del conjunto de datos y las relaciones entre ellas. También puede ayudar a determinar si las técnicas estadísticas que está considerando para el análisis de datos son apropiadas. Dependiendo del tipo de variable queremos obtener la siguiente información: Variables numéricas: Tipo de dato: float, integer Número de observaciones Mean Desviación estándar Cuartiles: 25%, 50%, 75% Valor máximo Valor mínimo Número de observaciones únicos Top 5 observaciones repetidas Número de observaciones con valores faltantes ¿Hay redondeos? Variables categóricas Número de categorías Valor de las categorías Moda Valores faltantes Número de observaciones con valores faltantes Proporción de observaciones por categoría Top 1, top 2, top 3 (moda 1, moda 2, moda 3) Faltas de ortografía ? Fechas Fecha inicio Fecha fin Huecos en las fechas: sólo tenemos datos entre semana, etc. Formatos de fecha (YYYY-MM-DD) Tipo de dato: date, time, timestamp Número de faltantes (NA) Número de observaciones Texto Longitud promedio de cada observación Identificar el lenguaje, si es posible Longitud mínima de cada observación Longitud máxima de cada observación Cuartiles de longitud: 25%, 50%, 75% Coordenadas geoespaciales Primero se pone la latitud y luego la longitud Primer decimal: 111 kms Segundo decimal: 11.1 kms Tercer decimal: 1.1 kms Cuarto decimal: 11 mts Quinto decimal: 1.1 mt Sexto decimal: 0.11 mts Valores que están cercanos al 100 representan la longitud El símbolo en cada coordenada representa si estamos al norte (positivo) o sur (negativo) -en la latitud-, al este (positivo) o al - oeste (negativo) -en la longitud-. 3.2 GEDA: Análisis Exploratorio de Datos Gráficos Como complemento al EDA podemos realizar un GEDA, que es un análisis exploratorio de los datos apoyándonos de visualizaciones, la visualización de datos no trata de hacer gráficas “bonitas” o “divertidas”, ni de simplificar lo complejo. Más bien, trata de aprovechar nuestra gran capacidad de procesamiento visual para exhibir de manera clara aspectos importantes de los datos. 3.2.1 Lo que no se debe hacer… Fuentes: WTF Visualizations Flowingdata 3.2.2 Principios de visualización El objetivo de una visualización es sintetizar información relevante al análisis presentada de manera sencilla y sin ambigüedad. Lo usamos de apoyo para explicar a una audiencia más amplia que puede no ser tan técnica. Una gráfica debe reportar el resultado de un análisis detallado, nunca lo reemplaza. No hacer gráficas porque se vean “cool” Antes de hacer una gráfica, debe pensarse en lo que se quiere expresar o representar Existen “reglas” o mejores gráficas para representar cierto tipo de información de acuerdo a los tipos de datos que se tienen o al objetivo se quiere lograr con la visualización. From Data to Viz No utilizar pie charts 3.3 Ggplot En Ggplot todo funciona a través de capas. Las capas se añaden una sobre otra eligiendo y personalizando las estéticas visuales. En todo momento es posible especificar los colores, grosor, transparencia, formas, etc que cada uno de los gráficos va tomando para formar la imagen general. 3.3.1 Estéticas En ggplot2, aestetics significa “algo que puedes ver”. Algunos ejemplos son: Posición (por ejemplo, los ejes x e y) Color (color “externo”) Fill (color de relleno) Shape (forma de puntos) Linetype (tipo de linea) Size (tamaño) Alpha (para la transparencia: los valores más altos tendrían formas opacas y los más bajos, casi transparentes). Hay que advertir que no todas las estéticas tienen la misma potencia en un gráfico. El ojo humano percibe fácilmente longitudes distintas. Pero tiene problemas para comparar áreas (que es lo que regula la estética size) o intensidades de color. Se recomienda usar las estéticas más potentes para representar las variables más importantes. Cada tipo de objeto geométrico (geom) solo acepta un subconjunto de todos los aestéticos. Puedes consultar la pagina de ayuda de geom() para ver que aestéticos acepta. El mapeo aestético se hace con la función aes(). 3.3.2 Objetos geométricos o capas Los objetos geométricos son las formas que puede tomar un gráfico. Algunos ejemplos son: Barras (geom_bar(), para las variables univariados discretos o nominales) Histogramas (geom_hist() para aquellas variables univariadas continuas) Puntos (geom_point() para scatter plots, gráficos de puntos, etc…) Lineas (geom_line() para series temporales, lineas de tendencia, etc…) Cajas (geom_boxplot() para gráficos de cajas) Un gráfico debe tener al menos un geom, pero no hay limite. Puedes añadir más geom usando el signo +. Una vez añadida una capa al gráfico a este pueden agregarse nuevas capas 3.3.3 Facetas Muchos de los gráficos que pueden generarse con los elementos anteriores pueden reproducirse usando los gráficos tradicionales de R, pero no los que usan facetas, que pueden permitirnos explorar las variables de diferente forma, por ejemplo: crea tres gráficos dispuestos horizontalmente que comparan la relación entre la anchura y la longitud del pétalo de las tres especies de iris. Una característica de estos gráficos, que es crítica para poder hacer comparaciones adecuadas, es que comparten ejes. 3.3.4 Más sobre estéticas Para los ejercicios en clase utilizaremos el set de datos: Diamonds: library(dplyr) library(ggplot2) library(reshape2) data(&quot;diamonds&quot;) Descripción Un conjunto de datos que contiene los precios y otros atributos de casi 54.000 diamantes. Las variables son las siguientes: price: precio en dólares estadounidenses ( $ 326 -  $ 18,823) carat: peso del diamante (0.2–5.01) cut: calidad del corte (Regular, Bueno, Muy Bueno, Premium, Ideal) color: color del diamante, de D (mejor) a J (peor) clarity: una medida de la claridad del diamante (I1 (peor), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (mejor)) x: longitud en mm (0-10,74) y: ancho en mm (0–58,9) width in mm (0–58.9) z: profundidad en mm (0–31,8) depth porcentaje de profundidad total = z / media (x, y) = 2 * z / (x + y) (43–79) table: ancho de la parte superior del diamante en relación con el punto más ancho (43–95) Ejemplo práctico: diamonds %>% ggplot() + aes(x = cut_number(carat, 5), y = price) + geom_boxplot() + aes(color = cut) + labs(title = 'Distribución de precio por categoría de corte') + labs(caption = 'Data source:Diamont set') + labs(x = 'Peso del diamante') + labs(y = 'Precio') + guides(color = guide_legend(title = 'Calidad del corte')) + ylim(0, 20000) + scale_y_continuous( labels = scales::dollar_format(), breaks = seq(0, 20000,2500 ), limits = c(0, 20000) ) 3.3.5 Quick View library(DataExplorer) plot_intro(diamonds) plot_missing(diamonds) 3.4 Análisis univariado El análisis univariado tiene como objetivo conocer la calidad y distribución de los datos. Se busca conocer medidas de tendencia central, variación promedio, cantidad de valores perdidos, etc. Es vital conocer los datos y su calidad antes de usarlos. 3.4.1 Variables numéricas Los histogramas son gráficas de barras que se obtienen a partir de tablas de frecuencias, donde cada barra se escala según la frecuencia relativa entre el ancho del intervalo de clase correspondiente. Un histograma muestra la acumulación ó tendencia, la variabilidad o dispersión y la forma de la distribución. El Diagrama de Caja y bigotes un tipo de gráfico que muestra un resumen de una gran cantidad de datos en cinco medidas descriptivas, además de intuir su morfología y simetría. Este tipo de gráficos nos permite identificar valores atípicos y comparar distribuciones. Además de conocer de una forma cómoda y rápida como el 50% de los valores centrales se distribuyen. Se puede detectar rápidamente los siguientes valores: Primer cuartil: el 25% de los valores son menores o igual a este valor (punto 2 en el gráfico anterior). Mediana o Segundo Cuartil: Divide en dos partes iguales la distribución. De forma que el 50% de los valores son menores o igual a este valor (punto 3 en el gráfico siguiente). Tercer cuartil: el 75% de los valores son menores o igual a este valor (punto 4 en el gráfico siguiente). Rango Intercuartílico (RIC): Diferencia entre el valor del tercer cuartil y el primer cuartil. Tip: El segmento que divide la caja en dos partes es la mediana (punto 3 del gráfico), que facilitará la comprensión de si la distribución es simétrica o asimétrica, si la mediana se sitúa en el centro de la caja entonces la distribución es simétrica y tanto la media, mediana y moda coinciden. Precio diamonds %&gt;% ggplot( aes( x = price)) + geom_histogram(color= &quot;purple&quot;, fill= &quot;pink&quot;, bins = 30) + scale_x_continuous(labels = scales::dollar_format()) + scale_y_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución de precio&quot;) diamonds %&gt;% ggplot( aes( x = price)) + geom_histogram( aes(y = ..density..), color= &quot;Blue&quot;, fill= &quot;White&quot;, bins = 30 ) + stat_density(geom = &quot;line&quot;, colour = &quot;black&quot;, size = 1)+ scale_x_continuous(labels = scales::dollar_format()) + scale_y_continuous(labels = scales::comma_format()) + stat_density(geom = &quot;line&quot;, colour = &quot;black&quot;, size = 1) + ggtitle(&quot;Distribución de precio&quot;) diamonds %&gt;% ggplot( aes( x = price)) + geom_boxplot(binwidth = 1000, color= &quot;Blue&quot;, fill= &quot;lightblue&quot;) + scale_x_continuous(labels = scales::dollar_format()) + scale_y_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución de precio&quot;) diamonds %&gt;% ggplot( aes( x = carat)) + geom_boxplot(color= &quot;purple&quot;, fill= &quot;pink&quot;, alpha= 0.3) + scale_x_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución de peso de los diamantes&quot;) + theme_bw() + coord_flip() Peso del diamante diamonds %&gt;% ggplot( aes( x = carat)) + geom_histogram(binwidth = .03, color= &quot;purple&quot;, fill= &quot;pink&quot;, alpha= 0.3) + scale_y_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución de peso de los diamantes&quot;) + theme_bw() diamonds %&gt;% ggplot( aes( x = carat)) + geom_boxplot(color= &quot;purple&quot;, fill= &quot;pink&quot;, alpha= 0.3) + scale_x_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución de peso de los diamantes&quot;) + theme_bw() 3.4.2 Variables nominales/categóricas Calidad de corte diamonds %&gt;% ggplot( aes( x = cut)) + geom_bar( color= &quot;darkblue&quot;, fill= &quot;cyan&quot;, alpha= 0.7) + scale_y_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución de calidad de corte&quot;) + theme_dark() df_pie &lt;- diamonds %&gt;% group_by(cut) %&gt;% summarise(freq = n(), .groups=&#39;drop&#39;) df_pie %&gt;% ggplot( aes( x = &quot;&quot;, y=freq, fill = factor(cut))) + geom_bar(width = 1, stat = &quot;identity&quot;) + coord_polar(theta = &quot;y&quot;, start=0) ggplot(data = diamonds)+ geom_bar( mapping = aes(x = cut, fill = cut), show.legend = F, width = 1)+ theme(aspect.ratio = 1)+ labs(x= NULL, y = NULL)+ coord_polar() Claridad diamonds %&gt;% ggplot( aes( y = clarity)) + geom_bar( color= &quot;darkblue&quot;, fill= &quot;black&quot;, alpha= 0.7) + geom_text(aes(label = scales::comma(..count..)), stat = &quot;count&quot;, vjust = 1, hjust = 1.1,colour = &quot;white&quot;) + scale_x_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución claridad&quot;) + theme_get() diamonds %&gt;% ggplot( aes( y = clarity)) + geom_bar( color= &quot;darkblue&quot;, fill= &quot;black&quot;, alpha= 0.7) + geom_text(aes(label = scales::percent(..count../sum(..count..) ) ), stat = &quot;count&quot;, vjust = -0.25, colour = &quot;darkblue&quot;) + scale_x_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución claridad&quot;) + coord_flip() 3.5 Análisis multivariado Un buen análisis de datos, requiere del análisis conjunto de variables. Una sola variable es importante de analizar en cuanto a su distribución y calidad, no obstante, no dice mucho al analizarse por sí sola. Es por ello, que es indispensable analizar la covariabilidad y dependencia entre los distintos atributos de la información. En el análisis multivariado, se busca comparar la información haciendo contrastes de colores, formas, tamaños, paneles, etc. Precio vs Calidad del corte diamonds %&gt;% ggplot(aes(y= price,x=cut,color=cut)) + geom_jitter(size=1.2, alpha= 0.5) diamonds %&gt;% ggplot(aes(y= price,x=cut,color=cut)) + geom_boxplot(size=1.2, alpha= 0.5) diamonds %&gt;% ggplot(aes(x= price ,fill=cut)) + geom_histogram(position = &#39;identity&#39;, alpha = 0.5) diamonds %&gt;% ggplot(aes(x= price ,fill=cut)) + geom_histogram(position = &#39;identity&#39;, alpha = 0.5) + facet_wrap(~cut, ncol = 1) diamonds %&gt;% ggplot( aes(x = carat ,y=price)) + geom_point(aes(col = clarity) ) + geom_smooth() diamonds %&gt;% ggplot( aes(x = carat ,y=price)) + geom_point(aes(col = clarity) ) + facet_wrap(~clarity)+ geom_smooth() 3.6 Visualización interactiva A través de la librería plotly es posible crear interactividad entre las gráficas creadas con gglot2. Es posible tanto usar las funciones existentes como crear funciones que enriquezcan las estéticas comunes. A continuación se hace uso de una combinación de nuevas estéticas con la interactividad añadida. library(stringr) fun_mean &lt;- function(x){ mean &lt;- data.frame( y = mean(x), label = mean(x, na.rm = T) ) return(mean) } means &lt;- diamonds %&gt;% group_by(clarity) %&gt;% summarise(price = round(mean(price), 1)) plot &lt;- diamonds %&gt;% ggplot(aes(x = clarity, y = price)) + geom_boxplot(aes(fill = clarity)) + stat_summary( fun = mean, geom = &quot;point&quot;, colour = &quot;darkred&quot;, shape = 18, size = 2 ) + geom_text( data = means, aes(label = str_c(&quot;$&quot;,price), y = price + 600) ) + ggtitle(&quot;Precio vs Claridad de diamantes&quot;) + xlab(&quot;Claridad&quot;) + ylab(&quot;Precio&quot;) plotly::ggplotly(plot) Como puede apreciarse, este nuevo gráfico permite hacer uso de zoom, filtros, etiquetas, snapshots, etc. El contenido es creado como HTML, por lo que puede integrarse a documentos web como bookdown, shiny, xaringan, markdown, etc. ¡ Warning ! Nunca se debe olvidar que debemos de analizar los datos de manera objetiva, nuestro criterio sobre un problema o negocio no debe de tener sesgos sobre lo que “nos gustaría encontrar en los datos” o lo “que creemos que debe pasar”…. 3.7 Reporte interactivos Es posible automatizar reportes de análisis de datos. Los reportes pueden realizarse tanto en formato estático (.docx y .pdf) como en formato interactivo (.html). Existen diversos manuales sumamente amplios que permiten conocer las múltiples funcionalidades de las librerías que hacen posible la creación de documentos. Para el caso de documentos, existe la librería Rmarkdown, la cual crea un documento estático o interactivo, mientras que para reportes en presentaciones existe la librería Xaringan, la cual sustituye a las presentaciones de power point. Es importante tomar en cuenta el balance entre complejidad y funcionalidad. Si bien es cierto que a través de esta herramienta pueden automatizarse reportes que consideren los resultados salientes de R, es importante considerar que las personas que puedan editar tal presentación serán limitadas. A continuación se enlistan los links de tutoriales para crear los documentos mencionados: Rmarkdown Xaringan Otros lenguajes Tablas estáticas Tablas interactivas "],["introducción-a-machine-learning.html", "Capítulo 4 Introducción a Machine Learning 4.1 Análisis Supervisado vs No supervisado 4.2 Sesgo vs varianza 4.3 Orden y estructura de proyecto 4.4 Partición de datos 4.5 Pre-procesamiento de datos 4.6 Ingeniería de datos 4.7 Recetas 4.8 Datos y tipos de modelos", " Capítulo 4 Introducción a Machine Learning Como se había mencionado, el Machine Learning es una disciplina del campo de la Inteligencia Artificial que, a través de algoritmos, dota a los ordenadores de la capacidad de identificar patrones en datos para hacer predicciones. Este aprendizaje permite a los computadores realizar tareas específicas de forma autónoma. El término se utilizó por primera vez en 1959. Sin embargo, ha ganado relevancia en los últimos años debido al aumento de la capacidad de computación y al BOOM de los datos. Un algoritmo para computadoras puede ser pensado como una receta. Describe exactamente qué pasos se realizan uno tras otro. Los ordenadores no entienden las recetas de cocina, sino los lenguajes de programación: En ellos, el algoritmo se descompone en pasos formales (comandos) que el ordenador puede entender. La cuestión no es solo saber para qué sirve el Machine Learning, sino que saber cómo funciona y cómo poder implementarlo en la industria para aprovecharse de sus beneficios. Hay ciertos pasos que usualmente se siguen para crear un modelo de Machine Learning. Estos son típicamente realizados por científicos de los datos que trabajan en estrecha colaboración con los profesionales de los negocios para los que se está desarrollando el modelo. Seleccionar y preparar un conjunto de datos de entrenamiento Los datos de entrenamiento son un conjunto de datos representativos de los datos que el modelo de Machine Learning ingerirá para resolver el problema que está diseñado para resolver. Los datos de entrenamiento deben prepararse adecuadamente: aleatorizados y comprobados en busca de desequilibrios o sesgos que puedan afectar al entrenamiento. También deben dividirse en dos subconjuntos: el subconjunto de entrenamiento, que se utilizará para entrenar el algoritmo, y el subconjunto de validación, que se utilizará para probarlo y perfeccionarlo. Elegir un algoritmo para ejecutarlo en el conjunto de datos de entrenamiento Este es uno de los pasos más importantes, ya que se debe elegir qué algoritmo utilizar, siendo este un conjunto de pasos de procesamiento estadístico. El tipo de algoritmo depende del tipo (supervisado o no supervisado), la cantidad de datos del conjunto de datos de entrenamiento y del tipo de problema que se debe resolver. Entrenamiento del algoritmo para crear el modelo El entrenamiento del algoritmo es un proceso iterativo: implica ejecutar las variables a través del algoritmo, comparar el resultado con los resultados que debería haber producido, ajustar los pesos y los sesgos dentro del algoritmo que podrían dar un resultado más exacto, y ejecutar las variables de nuevo hasta que el algoritmo devuelva el resultado correcto la mayoría de las veces. El algoritmo resultante, entrenado y preciso, es el modelo de Machine Learning. Usar y mejorar el modelo El paso final es utilizar el modelo con nuevos datos y, en el mejor de los casos, para que mejore en precisión y eficacia con el tiempo. De dónde procedan los nuevos datos dependerá del problema que se resuelva. Por ejemplo, un modelo de Machine Learning diseñado para identificar el spam ingerirá mensajes de correo electrónico, mientras que un modelo de Machine Learning que maneja una aspiradora robot ingerirá datos que resulten de la interacción en el mundo real con muebles movidos o nuevos objetos en la habitación. 4.1 Análisis Supervisado vs No supervisado Los algoritmos de Machine Learning se dividen en tres categorías, siendo las dos primeras las más comunes: Aprendizaje supervisado: estos algoritmos cuentan con un aprendizaje previo basado en un sistema de etiquetas asociadas a unos datos que les permiten tomar decisiones o hacer predicciones. Algunos ejemplos son: - Un detector de spam que etiqueta un e-mail como spam o no. - Predecir precios de casas - Clasificación de imagenes - Predecir el clima - ¿Quiénes son los clientes descontentos? Aprendizaje no supervisado: en el aprendizaje supervisado, la idea principal es aprender bajo supervisión, donde la señal de supervisión se nombra como valor objetivo o etiqueta. En el aprendizaje no supervisado, carecemos de este tipo de etiqueta. Por lo tanto, necesitamos encontrar nuestro camino sin ninguna supervisión ni guía. Esto simplemente significa que necesitamos descubrir qué es qué por nosotros mismos. Algunos ejemplos son: - Encontrar segmentos de clientes. - Reducir la complejidad de un problema - Selección de variables - Encontrar grupos - Reducción de dimensionalidad Aprendizaje por refuerzo: su objetivo es que un algoritmo aprenda a partir de la propia experiencia. Esto es, que sea capaz de tomar la mejor decisión ante diferentes situaciones de acuerdo a un proceso de prueba y error en el que se recompensan las decisiones correctas. Algunos ejemplos son: - Reconocimiento facial - Diagnósticos médicos - Clasificar secuencias de ADN 4.1.1 Regresión vs clasificación Existen dos tipos principales de aprendizaje supervisado, esto depende del tipo de la variable respuesta: Clasificación En el aprendizaje supervisado, los algoritmos de clasificación se usan cuando el resultado es una etiqueta discreta. Esto quiere decir que se utilizan cuando la respuesta se fundamenta en conjunto finito de resultados. Regresión El análisis de regresión es un subcampo del aprendizaje automático supervisado cuyo objetivo es establecer un método para la relación entre un cierto número de características y una variable objetivo continua. 4.2 Sesgo vs varianza En el mundo de Machine Learning cuando desarrollamos un modelo nos esforzamos para hacer que sea lo más preciso, ajustando los parámetros, pero la realidad es que no se puede construir un modelo 100% preciso ya que nunca pueden estar libres de errores. Comprender cómo las diferentes fuentes de error generan sesgo y varianza nos ayudará a mejorar el proceso de ajuste de datos, lo que resulta en modelos más precisos, adicionalmente también evitará el error de sobre-ajuste y falta de ajuste. 4.2.1 Balance entre sesgo y varianza o Trade-off El objetivo de cualquier algoritmo supervisado de Machine Learning es lograr un sesgo bajo, una baja varianza y a su vez el algoritmo debe lograr un buen rendimiento de predicción. El sesgo frente a la varianza se refiere a la precisión frente a la consistencia de los modelos entrenados por su algoritmo. Podemos diagnosticarlos de la siguiente manera: Los algoritmos de baja varianza (alto sesgo) tienden a ser menos complejos, con una estructura subyacente simple o rígida. Los algoritmos de bajo sesgo (alta varianza) tienden a ser más complejos, con una estructura subyacente flexible. No hay escapatoria a la relación entre el sesgo y la varianza en Machine Learning, aumentar el sesgo disminuirá la varianza, aumentar la varianza disminuirá el sesgo. 4.2.2 Error total Comprender el sesgo y la varianza es fundamental para comprender el comportamiento de los modelos de predicción, pero en general lo que realmente importa es el error general, no la descomposición específica. El punto ideal para cualquier modelo es el nivel de complejidad en el que el aumento en el sesgo es equivalente a la reducción en la varianza. Para construir un buen modelo, necesitamos encontrar un buen equilibrio entre el sesgo y la varianza de manera que minimice el error total. 4.2.3 Overfitting El modelo es muy particular. Error debido a la varianza Durante el entrenamiento tiene un desempeño muy bueno, pero al pasar nuevos datos su desempeño es malo. 4.2.4 Underfitting El modelo es demasiado general. Error debido al sesgo. Durante el entrenamiento no tiene un buen desempeño. 4.2.5 Error irreducible El error irreducible no se puede reducir, independientemente de qué algoritmo se usa. También se le conoce como ruido y, por lo general, proviene por factores como variables desconocidas que influyen en el mapeo de las variables de entrada a la variable de salida, un conjunto de características incompleto o un problema mal enmarcado. Acá es importante comprender que no importa cuán bueno hagamos nuestro modelo, nuestros datos tendrán cierta cantidad de ruido o un error irreductible que no se puede eliminar. 4.3 Orden y estructura de proyecto Resulta elemental contar con una adecuada estructura de carpetas que permitan al analista mantener orden y control a lo largo de todo el proyecto. Gran parte del caos en los problemas de analítica de datos nace desde el momento en que no se sabe en donde ubicar cada uno de los archivos necesarios para el proyecto. 4.3.1 Plantilla de estructura proyecto En esta sección, se presenta una introducción a la librería ProjectTemplate, la cual facilita una estructura predeterminada que ayudará como punto de partida para mantener orden y control en cada momento del proyecto. library(ProjectTemplate) create.project(project.name = &#39;intro2dsml&#39;, rstudio.project = T) create.project() creará toda la estructura de carpetas para un nuevo proyecto. Configurará todos los directorios relevantes y sus contenidos iniciales. Para aquellos que solo desean la funcionalidad mínima, el argumento de template se puede establecer en minimal para crear un subconjunto de directorios predeterminados de ProjectTemplate. cache: En esta carpeta se almacenarán los datos que desear cargarse automáticamente cuando se cargue la sesión del proyecto. config: Se realiza la configuración de R y su sesión, la cual será establecida cada que se abra el proyecto. data: Se almacenan las fuentes de información crudas necesarias en el proyecto. En caso de encontrarse codificadas en algún formato de archivo soportado por la librería, automáticamente serán cargadas a la sesión con la función load.project() diagnostics: En este folder puedes almacenar cualquier script usado para realizar diagnósticos sobre los datos. Es particularmente útil para al análisis de elementos corruptos o problemáticos dentro del conjunto de datos. doc: En este folder puede almacenarse cualquier documentación que haya escrito sobre el análisis. También se puede usar como directorio raíz para las páginas de GitHub para crear un sitio web de proyecto. graphs: Sirve para almacenar las gráficas producidas por el análisis lib: Aquí se almacenarán todos los archivos que proporcionen una funcionalidad útil para su trabajo, pero que no constituyan un análisis estadístico per se. Específicamente, debe usar el script lib/helpers.R para organizar cualquier función que use en su proyecto que no sea lo suficientemente general como para pertenecer a un paquete. Si tiene una configuración específica del proyecto que le gustaría almacenar en el objeto de configuración, puede especificarla en lib/globals.R. logs: Aquí puede almacenarse un archivo de registro de cualquier trabajo que haya realizado en este proyecto. Si va a registrar su trabajo, se recomienda utilizar el paquete log4r, que ProjectTemplate cargará automáticamente si activa la opción de configuración de registro. El nivel de registro se puede establecer a través de la configuración logging_level en la configuración, el valor predeterminado es “INFO”. munge: En este folder puede almacenarse cualquier código de pre-procesamiento o manipulación de datos para el proyecto. Por ejemplo, si necesita agregar columnas en tiempo de ejecución, fusionar conjuntos de datos normalizados o censurar globalmente cualquier punto de datos, ese código debe almacenarse en el directorio munge. Los scripts de pre-procesamiento almacenados en munge se ejecutarán en orden alfabético cuando se llame a la función load.project(), por lo que debe anteponerse números a los nombres de archivo para indicar su orden secuencial. profiling: Aquí puede almacenar cualquier script que use para comparar y cronometrar su código. reports: Aquí puede almacenar cualquier informe de salida, como versiones de tablas HTML o LaTeX, que produzca. Los documentos de sweave o brew también deben ir en el directorio de informes. src: Aquí se almacenarán los scripts de análisis estadístico finales. Debe agregar el siguiente fragmento de código al comienzo de cada secuencia de comandos de análisis: library('ProjectTemplate); load.project(). También debe hacer todo lo posible para asegurarse de que cualquier código compartido entre los análisis en src se mueva al directorio munge; si lo hace, puede ejecutar todos los análisis en el directorio src en paralelo. Una versión futura de ProjectTemplate proporcionará herramientas para ejecutar automáticamente cada análisis individual de src en paralelo. tests: Aquí puede almacenarse cualquier caso de prueba para las funciones que ha escrito. Los archivos de prueba deben usar pruebas de estilo testthat para que pueda llamar a la función test.project() para ejecutar automáticamente todo su código de prueba. README: En este archivo, debe escribir algunas notas para ayudar a orientar a los recién llegados a su proyecto. TODO: En este archivo, debe escribir una lista de futuras mejoras y correcciones de errores que planea realizar en sus análisis. Si algunas o todas estas carpetas resultan innecesarias, puede comenzarse con una versión simplificada a través del comando: create.project(project.name = &#39;intro2dsml&#39;, template=&#39;minimal&#39;) 4.4 Partición de datos Cuando hay una gran cantidad de datos disponibles, una estrategia inteligente es asignar subconjuntos específicos de datos para diferentes tareas, en lugar de asignar la mayor cantidad posible solo a la estimación de los parámetros del modelo. Si el conjunto inicial de datos no es lo suficientemente grande, habrá cierta superposición de cómo y cuándo se asignan nuestros datos, y es importante contar con una metodología sólida para la partición de datos. 4.4.1 Métodos comunes para particionar datos El enfoque principal para la validación del modelo es dividir el conjunto de datos existente en dos conjuntos distintos: Entrenamiento: Este conjunto suele contener la mayoría de los datos, los cuales sirven para la construcción de modelos donde se pueden ajustar diferentes modelos, se investigan estrategias de ingeniería de características, etc. La mayor parte del proceso de modelado se utiliza este conjunto. Prueba: La otra parte de las observaciones se coloca en este conjunto. Estos datos se mantienen en reserva hasta que se elijan uno o dos modelos como los de mejor rendimiento. El conjunto de prueba se utiliza como árbitro final para determinar la eficiencia del modelo, por lo que es fundamental mirar el conjunto de prueba una sola vez. Supongamos que asignamos el \\(80\\%\\) de los datos al conjunto de entrenamiento y el \\(20\\%\\) restante a las pruebas. El método más común es utilizar un muestreo aleatorio simple. El paquete rsample tiene herramientas para realizar divisiones de datos como esta; la función initial_split() fue creada para este propósito. library(tidymodels) tidymodels_prefer() # Fijar un número aleatorio con para que los resultados puedan ser reproducibles set.seed(123) # Partición 80/20 de los datos ames_split &lt;- initial_split(ames, prop = 0.80) ames_split ## &lt;Training/Testing/Total&gt; ## &lt;2344/586/2930&gt; La información impresa denota la cantidad de datos en el conjunto de entrenamiento \\((n = 2,344)\\), la cantidad en el conjunto de prueba \\((n = 586)\\) y el tamaño del grupo original de muestras \\((n = 2,930)\\). El objeto ames_split es un objeto rsplit y solo contiene la información de partición; para obtener los conjuntos de datos resultantes, aplicamos dos funciones más: ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) dim(ames_train) ## [1] 2344 74 El muestreo aleatorio simple es apropiado en muchos casos, pero hay excepciones. Cuando hay un desbalance de clases en los problemas de clasificación, el uso de una muestra aleatoria simple puede asignar al azar estas muestras poco frecuentes de manera desproporcionada al conjunto de entrenamiento o prueba. Para evitar esto, se puede utilizar un muestreo estratificado. La división de entrenamiento/prueba se lleva a cabo por separado dentro de cada clase y luego estas submuestras se combinan en el conjunto general de entrenamiento y prueba. Para los problemas de regresión, los datos de los resultados se pueden agrupar artificialmente en cuartiles y luego realizar un muestreo estratificado cuatro veces por separado. Este es un método eficaz para mantener similares las distribuciones del resultado entre el conjunto de entrenamiento y prueba. Observamos que la distribución del precio de venta está sesgada a la derecha. Las casas más caras no estarían bien representadas en el conjunto de entrenamiento con una simple partición; esto aumentaría el riesgo de que nuestro modelo sea ineficaz para predecir el precio de dichas propiedades. Las líneas verticales punteadas indican los cuatro cuartiles para estos datos. Una muestra aleatoria estratificada llevaría a cabo la división 80/20 dentro de cada uno de estos subconjuntos de datos y luego combinaría los resultados. En rsample, esto se logra usando el argumento de estratos: set.seed(123) ames_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) Hay muy pocas desventajas en el uso de muestreo estratificado. Un caso es cuando los datos tienen un componente de tiempo, como los datos de series de tiempo. Aquí, es más común utilizar los datos más recientes como conjunto de prueba. El paquete rsample contiene una función llamada initial_time_split() que es muy similar a initial_split(). En lugar de usar un muestreo aleatorio, el argumento prop denota qué proporción de la primera parte de los datos debe usarse como conjunto de entrenamiento; la función asume que los datos se han clasificado previamente en un orden apropiado. ¿Qué proporción debería ser usada? No hay un porcentaje de división óptimo para el conjunto de entrenamiento y prueba. Muy pocos datos en el conjunto de entrenamiento obstaculizan la capacidad del modelo para encontrar estimaciones de parámetros adecuadas y muy pocos datos en el conjunto de prueba reducen la calidad de las estimaciones de rendimiento. Se debe elegir un porcentaje que cumpla con los objetivos de nuestro proyecto con consideraciones que incluyen: Costo computacional en el entrenamiento del modelo. Costo computacional en la evaluación del modelo. Representatividad del conjunto de formación. Representatividad del conjunto de pruebas. Los porcentajes de división más comunes comunes son: Entrenamiento: \\(80\\%\\), Prueba: \\(20\\%\\) Entrenamiento: \\(67\\%\\), Prueba: \\(33\\%\\) Entrenamiento: \\(50\\%\\), Prueba: \\(50\\%\\) 4.4.2 Conjunto de validación El conjunto de validación se definió originalmente cuando los investigadores se dieron cuenta de que medir el rendimiento del conjunto de entrenamiento conducía a resultados que eran demasiado optimistas. Esto llevó a modelos que se sobre-ajustaban, lo que significa que se desempeñaron muy bien en el conjunto de entrenamiento pero mal en el conjunto de prueba. Para combatir este problema, se retuvo un pequeño conjunto de datos de validación y se utilizó para medir el rendimiento del modelo mientras este está siendo entrenado. Una vez que la tasa de error del conjunto de validación comenzara a aumentar, la capacitación se detendría. En otras palabras, el conjunto de validación es un medio para tener una idea aproximada de qué tan bien se desempeñó el modelo antes del conjunto de prueba. Los conjuntos de validación se utilizan a menudo cuando el conjunto de datos original es muy grande. En este caso, una sola partición grande puede ser adecuada para caracterizar el rendimiento del modelo sin tener que realizar múltiples iteraciones de remuestreo. Con rsample, un conjunto de validación es como cualquier otro objeto de remuestreo; este tipo es diferente solo en que tiene una sola iteración set.seed(12) val_set &lt;- validation_split(ames_train, prop = 3/4, strata = NULL) val_set #val_set contiene el conjunto de entrenamiento y validación. ## # Validation Set Split (0.75/0.25) ## # A tibble: 1 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [1756/586]&gt; validation Esta función regresa una columna para los objetos de división de datos y una columna llamada id que tiene una cadena de caracteres con el identificador de remuestreo. El argumento de estratos hace que el muestreo aleatorio se lleve a cabo dentro de la variable de estratificación. Esto puede ayudar a garantizar que el número de datos en los datos del análisis sea equivalente a las proporciones del conjunto de datos original. (Los estratos inferiores al 10% del total se agrupan). Otra opción de muestreo bastante común es la realizada mediante múltiples submuestras de los datos originales. Diversos métodos se revisarán a lo largo del curso. 4.4.3 Leave-one-out cross-validation La validación cruzada es una manera de predecir el ajuste de un modelo a un hipotético conjunto de datos de prueba cuando no disponemos del conjunto explícito de datos de prueba. El método LOOCV en un método iterativo que se inicia empleando como conjunto de entrenamiento todas las observaciones disponibles excepto una, que se excluye para emplearla como validación. Si se emplea una única observación para calcular el error, este varía mucho dependiendo de qué observación se haya seleccionado. Para evitarlo, el proceso se repite tantas veces como observaciones disponibles se tengan, excluyendo en cada iteración una observación distinta, ajustando el modelo con el resto y calculando el error con dicha observación. Finalmente, el error estimado por el es el promedio de todos lo \\(i\\) errores calculados. La principal desventaja de este método es su costo computacional. El proceso requiere que el modelo sea reajustado y validado tantas veces como observaciones disponibles se tengan lo que en algunos casos puede ser muy complicado. rsample contiene la función loo_cv(). set.seed(55) ames_loo &lt;- loo_cv(ames_train) ames_loo ## # Leave-one-out cross-validation ## # A tibble: 2,342 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [2341/1]&gt; Resample1 ## 2 &lt;split [2341/1]&gt; Resample2 ## 3 &lt;split [2341/1]&gt; Resample3 ## 4 &lt;split [2341/1]&gt; Resample4 ## 5 &lt;split [2341/1]&gt; Resample5 ## 6 &lt;split [2341/1]&gt; Resample6 ## 7 &lt;split [2341/1]&gt; Resample7 ## 8 &lt;split [2341/1]&gt; Resample8 ## 9 &lt;split [2341/1]&gt; Resample9 ## 10 &lt;split [2341/1]&gt; Resample10 ## # ℹ 2,332 more rows 4.4.3.1 Cálculo del error En la validación cruzada dejando uno fuera se realizan tantas iteraciones como muestras \\((N)\\) tenga el conjunto de datos. De forma que para cada una de las \\(N\\) iteraciones se realiza un cálculo de error. El resultado final se obtiene realizando la media de los \\(N\\) errores obtenidos, según la fórmula: \\[E = \\frac{1}{N}\\sum_{i = 1}^N E_i\\] 4.4.4 V Fold Cross Validation En la validación cruzada de V iteraciones (V Fold Cross Validation) los datos de muestra se dividen en V subconjuntos. Uno de los subconjuntos se utiliza como datos de prueba y el resto \\((V-1)\\) como datos de entrenamiento. El proceso de validación cruzada es repetido durante \\(v\\) iteraciones, con cada uno de los posibles subconjuntos de datos de prueba. Finalmente se obtiene el promedio de los rendimientos de cada iteración para obtener un único resultado. Lo más común es utilizar la validación cruzada de 10 iteraciones. Este método de validación cruzada se utiliza principalmente para: Estimar el error cuando nuestro conjunto de prueba es muy pequeño. Es decir, se tiene la misma configuración de parámetros y solamente cambia el conjunto de prueba y validación. Encontrar lo mejores hiperparámetros que ajusten mejor el modelo. Es decir, en cada bloque se tiene una configuración de hiperparámetros distinto y se seleccionará aquellos hiperparámetros que hayan producido el error más pequeño. En la función vfold_cv() la entrada principal es el conjunto de entrenamiento, así como el número de bloques: set.seed(55) ames_folds &lt;- vfold_cv(ames_train, v = 10) ames_folds ## # 10-fold cross-validation ## # A tibble: 10 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [2107/235]&gt; Fold01 ## 2 &lt;split [2107/235]&gt; Fold02 ## 3 &lt;split [2108/234]&gt; Fold03 ## 4 &lt;split [2108/234]&gt; Fold04 ## 5 &lt;split [2108/234]&gt; Fold05 ## 6 &lt;split [2108/234]&gt; Fold06 ## 7 &lt;split [2108/234]&gt; Fold07 ## 8 &lt;split [2108/234]&gt; Fold08 ## 9 &lt;split [2108/234]&gt; Fold09 ## 10 &lt;split [2108/234]&gt; Fold10 La columna denominada splits contiene la información sobre cómo dividir los datos (similar al objeto utilizado para crear la partición inicial de entrenamiento / prueba). Si bien cada fila de divisiones tiene una copia incrustada de todo el conjunto de entrenamiento, R es lo suficientemente inteligente como para no hacer copias de los datos en la memoria. El método de impresión dentro del tibble muestra la frecuencia de cada uno: [2K / 230] indica que aproximadamente dos mil muestras están en el conjunto de análisis y 230 están en ese conjunto de evaluación en particular. Estos objetos rsample también contienen siempre una columna de caracteres llamada id que etiqueta la partición. Algunos métodos de remuestreo requieren varios campos de identificación. Para recuperar manualmente los datos particionados, las funciones de analysis() y assessment() devuelven los de datos de análisis y evaluación respectivamente. # Primer bloque ames_folds$splits[[1]] %&gt;% analysis() %&gt;% # O assessment() head(7) ## # A tibble: 7 × 74 ## MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 One_Story_1946_and_New… Resident… 70 8400 Pave No_A… Regular ## 2 Two_Story_PUD_1946_and… Resident… 21 1680 Pave No_A… Regular ## 3 Two_Story_PUD_1946_and… Resident… 21 1680 Pave No_A… Regular ## 4 Two_Story_PUD_1946_and… Resident… 21 1680 Pave No_A… Regular ## 5 One_Story_PUD_1946_and… Resident… 53 4043 Pave No_A… Regular ## 6 One_Story_PUD_1946_and… Resident… 24 2280 Pave No_A… Regular ## 7 One_Story_PUD_1946_and… Resident… 50 7175 Pave No_A… Regular ## # ℹ 67 more variables: Land_Contour &lt;fct&gt;, Utilities &lt;fct&gt;, Lot_Config &lt;fct&gt;, ## # Land_Slope &lt;fct&gt;, Neighborhood &lt;fct&gt;, Condition_1 &lt;fct&gt;, Condition_2 &lt;fct&gt;, ## # Bldg_Type &lt;fct&gt;, House_Style &lt;fct&gt;, Overall_Cond &lt;fct&gt;, Year_Built &lt;int&gt;, ## # Year_Remod_Add &lt;int&gt;, Roof_Style &lt;fct&gt;, Roof_Matl &lt;fct&gt;, ## # Exterior_1st &lt;fct&gt;, Exterior_2nd &lt;fct&gt;, Mas_Vnr_Type &lt;fct&gt;, ## # Mas_Vnr_Area &lt;dbl&gt;, Exter_Cond &lt;fct&gt;, Foundation &lt;fct&gt;, Bsmt_Cond &lt;fct&gt;, ## # Bsmt_Exposure &lt;fct&gt;, BsmtFin_Type_1 &lt;fct&gt;, BsmtFin_SF_1 &lt;dbl&gt;, … 4.4.5 Medidas de ajuste Las medidas de ajuste obtenidas pueden ser utilizadas para estimar cualquier medida cuantitativa de ajuste apropiada para los datos y el modelo. En un modelo basado en clasificación binaria, para resumir el ajuste del modelo se pueden usar las medidas: Tasa de error de clasificación (Accuracy) Precisión Sensibilidad o cobertura (Recall) Especificidad Cuando el valor a predecir se distribuye de forma continua se puede calcular el error utilizando medidas como: Error porcentual absoluto medio (MAPE) Error absoluto medio (MAE) Error cuadrático medio (MSE) Raíz del error cuadrático medio (RMSE) Raíz del error logarítmico cuadrático medio (RMLSE) \\(R^2\\) (Coeficiente de determinación) \\(R^2_a\\) (Coeficiente de determinación ajustado) 4.4.5.1 Cálculo del error En cada una de las \\(v\\) iteraciones de este tipo de validación se realiza un cálculo de error. El resultado final lo obtenemos a partir de realizar la media de los \\(V\\) valores de errores obtenidos, según la fórmula: \\[E = \\frac{1}{V}\\sum_{i = 1}^vE_i\\] 4.4.6 Validación cruzada para series de tiempo En este procedimiento, hay una serie de conjuntos de prueba, cada uno de los cuales consta de una única observación. El conjunto de entrenamiento correspondiente consta solo de observaciones que ocurrieron antes de la observación que forma el conjunto de prueba. Por lo tanto, no se pueden utilizar observaciones futuras para construir el pronóstico. El siguiente diagrama ilustra la serie de conjuntos de entrenamiento y prueba, donde las observaciones azules forman los conjuntos de entrenamiento y las observaciones rojas forman los conjuntos de prueba. La precisión del pronóstico se calcula promediando los conjuntos de prueba. Este procedimiento a veces se conoce como “evaluación en un origen de pronóstico continuo” porque el “origen” en el que se basa el pronóstico avanza en el tiempo. Con los pronósticos de series de tiempo, los pronósticos de un paso pueden no ser tan relevantes como los pronósticos de varios pasos. En este caso, el procedimiento de validación cruzada basado en un origen de pronóstico continuo se puede modificar para permitir el uso de errores de varios pasos. Suponga que estamos interesados en modelos que producen buenos pronósticos de 4 pasos por delante. Entonces el diagrama correspondiente se muestra a continuación. La validación cruzada de series de tiempo se implementa con la función tsCV() del paquete forecast. 4.5 Pre-procesamiento de datos Hay varios pasos que se deben de seguir para crear un modelo útil: Recopilación de datos. Limpieza de datos. Creación de nuevas variables. Estimación de parámetros. Selección y ajuste del modelo. Evaluación del rendimiento. Al comienzo de un proyecto, generalmente hay un conjunto finito de datos disponibles para todas estas tareas. OJO: A medida que los datos se reutilizan para múltiples tareas, aumentan los riesgos de agregar sesgos o grandes efectos de errores metodológicos. Como punto de partida para nuestro flujo de trabajo de aprendizaje automático, necesitaremos datos de entrada. En la mayoría de los casos, estos datos se cargarán y almacenarán en forma de data frames o tibbles en R. Incluirán una o varias variables predictivas y, en caso de aprendizaje supervisado, también incluirán un resultado conocido. Sin embargo, no todos los modelos pueden lidiar con diferentes problemas de datos y, a menudo, necesitamos transformar los datos para obtener el mejor rendimiento posible del modelo. Este proceso se denomina pre-procesamiento y puede incluir una amplia gama de pasos, como: Dicotomización de variables: Variables cualitativas que solo pueden tomar el valor \\(0\\) o \\(1\\) para indicar la ausencia o presencia de una condición específica. Estas variables se utilizan para clasificar los datos en categorías mutuamente excluyentes o para activar comandos de encendido / apagado Near Zero Value (nzv) o Varianza Cero: En algunas situaciones, el mecanismo de generación de datos puede crear predictores que solo tienen un valor único (es decir, un “predictor de varianza cercando a cero”). Para muchos modelos (excluidos los modelos basados en árboles), esto puede hacer que el modelo se bloquee o que el ajuste sea inestable. De manera similar, los predictores pueden tener solo una pequeña cantidad de valores únicos que ocurren con frecuencias muy bajas. Imputaciones: Si faltan algunos predictores, ¿deberían estimarse mediante imputación? Des-correlacionar: Si hay predictores correlacionados, ¿debería mitigarse esta correlación? Esto podría significar filtrar predictores, usar análisis de componentes principales o una técnica basada en modelos (por ejemplo, regularización). Normalizar: ¿Deben centrarse y escalar los predictores? Transformar: ¿Es útil transformar los predictores para que sean más simétricos? (por ejemplo, escala logarítmica). Dependiendo del caso de uso, algunos pasos de pre-procesamiento pueden ser indispensables para pasos posteriores, mientras que otros solo son opcionales. Sin embargo, dependiendo de los pasos de pre-procesamiento elegidos, el rendimiento del modelo puede cambiar significativamente en pasos posteriores. Por lo tanto, es muy común probar varias configuraciones. 4.6 Ingeniería de datos La ingeniería de datos abarca actividades que dan formato a los valores de los predictores para que se puedan utilizar de manera eficaz para nuestro modelo. Esto incluye transformaciones y codificaciones de los datos para representar mejor sus características importantes. Por ejemplo: 1.- Supongamos que un conjunto de datos tiene dos predictores que se pueden representar de manera más eficaz en nuestro modelo como una proporción, así, tendríamos un nuevo predictor a partir de la proporción de los dos predictores originales. X Proporción (X) 691 0.1836789 639 0.1698565 969 0.2575758 955 0.2538543 508 0.1350346 2.- Al elegir cómo codificar nuestros datos en el modelado, podríamos elegir una opción que creemos que está más asociada con el resultado. El formato original de los datos, por ejemplo numérico (edad) versus categórico (grupo). Edad Grupo 7 Niños 78 Adultos mayores 17 Adolescentes 25 Adultos 90 Adultos mayores La ingeniería y el pre-procesamiento de datos también pueden implicar el cambio de formato requerido por el modelo. Algunos modelos utilizan métricas de distancia geométrica y, en consecuencia, los predictores numéricos deben centrarse y escalar para que estén todos en las mismas unidades. De lo contrario, los valores de distancia estarían sesgados por la escala de cada columna. 4.7 Recetas Una receta es una serie de pasos o instrucciones para el procesamiento de datos. A diferencia del método de fórmula dentro de una función de modelado, la receta define los pasos sin ejecutarlos inmediatamente; es sólo una especificación de lo que se debe hacer. La estructura de una receta sigue los siguientes pasos: Inicialización Transformación Preparación Aplicación La siguiente sección explica la estructura y flujo de transformaciones: receta &lt;- recipe(response ~ X1 + X2 + X3 + ... + Xn, data = dataset ) %&gt;% transformation_1(...) %&gt;% transformation_2(...) %&gt;% transformation_3(...) %&gt;% ... final_transformation(...) %&gt;% prep() bake(receta, new_data = new_dataset) A continuación se muestran distintos ejemplos de transformaciones realizadas comúnmente en el pre-procesamiento de modelos predictivos. Como ejemplo, utilizaremos el subconjunto de predictores disponibles en los datos de vivienda: Ames Vecindario (29 vecindarios) Superficie habitable bruta sobre el nivel del suelo Año de constricción Tipo de edificio ANTERIORMENTE… Un modelo de regresión lineal ordinario se ajustaba a los datos con la función estándar lm() de la siguiente manera: lm(Sale_Price ~ Neighborhood + log10(Gr_Liv_Area) + Year_Built + Bldg_Type, data = ames) Cuando se ejecuta esta función, los datos se convierten en a una matriz de diseño numérico (también llamada matriz de modelo) y luego se utiliza el método de mínimos cuadrados para estimar los parámetros. Lo que hace la fórmula anterior se puede descomponer en una serie de pasos: 1.- El precio de venta se define como el resultado, mientras que las variables de vecindario, superficie habitable bruta, año de construcción y tipo de edificio se definen como predictores. 2.- Se aplica una transformación logarítmica al predictor de superficie habitable bruta. 3.- Las columnas de vecindad y tipo de edificio se convierten de un formato no numérico a un formato numérico (dado que los mínimos cuadrados requieren predictores numéricos). La siguiente receta es equivalente a la fórmula anterior: simple_ames &lt;- recipe( Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type, data = ames) %&gt;% step_log(Gr_Liv_Area, base = 10) %&gt;% step_dummy(all_nominal_predictors()) simple_ames ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 4 ## ## Operations: ## ## Log transformation on Gr_Liv_Area ## Dummy variables from all_nominal_predictors() Ventajas de usar una receta: Los cálculos se pueden reciclar entre modelos ya que no están estrechamente acoplados a la función de modelado. Una receta permite un conjunto más amplio de opciones de procesamiento de datos que las que pueden ofrecer las fórmulas. La sintaxis puede ser muy compacta. Por ejemplo, all_nominal_predictors() se puede usar para capturar muchas variables para tipos específicos de procesamiento, mientras que una fórmula requeriría que cada una se enumere explícitamente. Todo el procesamiento de datos se puede capturar en un solo objeto en lugar de tener scripts que se repiten o incluso se distribuyen en diferentes archivos. 4.7.1 Pasos y estructura de recetas Como se mostró anteriormente, existen 4 pasos fundamentales para el procesamiento y transformación de conjuntos de datos. Estos pasos se describen de la siguiente manera: Receta: Inicializa una receta y define los roles de las variables Transformaciones: Mutaciones a los renglones y columnas hasta desear el resultado Preparación: Se realizan las estimaciones estadísticas con los datos La función prep() estima las cantidades requeridas y las estadísticas necesarias para cualquier paso declarado en la receta. prep &lt;- prep(simple_ames) prep ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 4 ## ## Training data contained 2930 data points and no missing data. ## ## Operations: ## ## Log transformation on Gr_Liv_Area [trained] ## Dummy variables from Neighborhood, Bldg_Type [trained] Aplicación Se llevan a cabo las transformaciones especificadas en la receta preparada a un conjunto de datos. Finalmente, la función bake() lleva a cabo la transformación de un conjunto de datos a través de las estimaciones indicadas en una receta y aplica las operaciones a un conjunto de datos para crear una matriz de diseño. La función bake(object, new_data = NULL) devolverá los datos con los que se entrenó la receta. Nota: La función juice() devolverá los resultados de una receta en la que se hayan aplicado todos los pasos a los datos. Similar a la función bake() con el comando new_data = NULL. simple_ames %&gt;% prep() %&gt;% bake(new_data = NULL) %&gt;% glimpse() ## Rows: 2,930 ## Columns: 35 ## $ Gr_Liv_Area &lt;dbl&gt; 3.219060, 2.95230… ## $ Year_Built &lt;int&gt; 1960, 1961, 1958,… ## $ Sale_Price &lt;int&gt; 215000, 105000, 1… ## $ Neighborhood_College_Creek &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Old_Town &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Edwards &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Somerset &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Northridge_Heights &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Gilbert &lt;dbl&gt; 0, 0, 0, 0, 1, 1,… ## $ Neighborhood_Sawyer &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Northwest_Ames &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Sawyer_West &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Mitchell &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Brookside &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Crawford &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Iowa_DOT_and_Rail_Road &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Timberland &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Northridge &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Stone_Brook &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_South_and_West_of_Iowa_State_University &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Clear_Creek &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Meadow_Village &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Briardale &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Bloomington_Heights &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Veenker &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Northpark_Villa &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Blueste &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Greens &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Green_Hills &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Landmark &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Hayden_Lake &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Bldg_Type_TwoFmCon &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Bldg_Type_Duplex &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Bldg_Type_Twnhs &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Bldg_Type_TwnhsE &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… En cuanto a las transformaciones posibles, existe una gran cantidad de funciones que soportan este proceso. En esta sección se muestran algunas de las transformación más comunes, entre ellas: Normalización Dicotomización Creación de nuevas columnas Datos faltantes Imputaciones Interacciones Etc. 4.7.1.1 Normalizar columnas numéricas Quizá la transformación numérica más usada en todos los modelos es la estandarización o normalización de variables numéricas. Este proceso se realiza para homologar la escala de las variables numéricas, de modo que no predomine una sobre otra debido a la diferencia de magnitudes o escalas. Este proceso se tiene de fondo el siguiente proceso estadístico: \\[Z=\\frac{X-\\hat{\\mu}_x}{\\hat{\\sigma}_x}\\] Donde: X = Es una variable o columna numérica \\(\\hat{\\mu}_x\\) = Es la estimación de la media de la variable X \\(\\hat{\\sigma}_x\\) = Es la estimación de la desviación estándar de la variable X La librería recipes nos permite realizar este proceso ágilmente mediante la función: step_normalize(). ames %&gt;% select(Sale_Price, Neighborhood, Gr_Liv_Area, Year_Built, Bldg_Type) %&gt;% head(5) ## # A tibble: 5 × 5 ## Sale_Price Neighborhood Gr_Liv_Area Year_Built Bldg_Type ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; ## 1 215000 North_Ames 1656 1960 OneFam ## 2 105000 North_Ames 896 1961 OneFam ## 3 172000 North_Ames 1329 1958 OneFam ## 4 244000 North_Ames 2110 1968 OneFam ## 5 189900 Gilbert 1629 1997 OneFam simple_ames &lt;- recipe(Sale_Price ~ ., data = ames) %&gt;% step_normalize(all_numeric_predictors()) simple_ames ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 73 ## ## Operations: ## ## Centering and scaling for all_numeric_predictors() simple_ames %&gt;% prep() %&gt;% bake(new_data = NULL) %&gt;% select(Sale_Price, Neighborhood, Gr_Liv_Area, Year_Built, Bldg_Type) %&gt;% head(5) ## # A tibble: 5 × 5 ## Sale_Price Neighborhood Gr_Liv_Area Year_Built Bldg_Type ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 215000 North_Ames 0.309 -0.375 OneFam ## 2 105000 North_Ames -1.19 -0.342 OneFam ## 3 172000 North_Ames -0.338 -0.442 OneFam ## 4 244000 North_Ames 1.21 -0.111 OneFam ## 5 189900 Gilbert 0.256 0.848 OneFam 4.7.1.2 Dicotomización de categorías Otra transformación necesaria en la mayoría de los modelos predictivos en la creación de las variables dummy. Se mencionó anteriormente que los modelos requieren de una matriz numérica de características explicativas que permita calcular patrones estadísticos para predecir la variable de respuesta. El proceso de dicotomización consiste en crear una variable dicotómica por cada categoría de una columna con valores nominales. ames %&gt;% select(Sale_Price, Bldg_Type) %&gt;% head(5) ## # A tibble: 5 × 2 ## Sale_Price Bldg_Type ## &lt;int&gt; &lt;fct&gt; ## 1 215000 OneFam ## 2 105000 OneFam ## 3 172000 OneFam ## 4 244000 OneFam ## 5 189900 OneFam ames %&gt;% select(Bldg_Type) %&gt;% distinct() %&gt;% pull() ## [1] OneFam TwnhsE Twnhs Duplex TwoFmCon ## Levels: OneFam TwoFmCon Duplex Twnhs TwnhsE simple_ames &lt;- recipe(Sale_Price ~ Bldg_Type, data = ames) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% prep() simple_ames ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 1 ## ## Training data contained 2930 data points and no missing data. ## ## Operations: ## ## Dummy variables from Bldg_Type [trained] simple_ames %&gt;% bake(new_data = NULL) %&gt;% head(5) ## # A tibble: 5 × 5 ## Sale_Price Bldg_Type_TwoFmCon Bldg_Type_Duplex Bldg_Type_Twnhs ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 215000 0 0 0 ## 2 105000 0 0 0 ## 3 172000 0 0 0 ## 4 244000 0 0 0 ## 5 189900 0 0 0 ## # ℹ 1 more variable: Bldg_Type_TwnhsE &lt;dbl&gt; El proceso de dicotomización demanda que únicamente (n-1) categorías sean expresadas, mientras que la restante será considerada la categoría default o basal. Esta última categoría es la usada en el modelo cuando todas las demás se encuentran ausentes. 4.7.1.3 Codificación de datos cualitativos nuevos o faltantes Una de las tareas de ingeniería de datos más comunes es el tratamiento de datos faltantes, datos no antes vistos y datos con poca frecuencia. El problema principal con estos casos es que los modelos no saben cómo relacionar estos eventos con futuras predicciones. Es conveniente realizar las transformaciones necesarias de tratamiento de estos datos antes de pasar a la etapa de modelado. Por ejemplo: step_unknown() cambia los valores perdidos en un nivel de factor “desconocido”. step_other() analiza las frecuencias de los niveles de los factores en el conjunto de datos y convierte los valores que ocurren con poca frecuencia a un nivel general de “otro”, con un umbral que se puede especificar. step_novel() puede asignar un nuevo nivel si anticipamos que se puede encontrar un nuevo factor en datos futuros. Un buen ejemplo es el predictor de vecindad en nuestros datos. Aquí hay dos vecindarios que tienen menos de cinco propiedades. ggplot(data = ames, aes(y = Neighborhood)) + geom_bar() + labs(y = NULL) Para algunos modelos, puede resultar problemático tener variables dummy con una sola entrada distinta de cero en la columna. Como mínimo, es muy improbable que estas características sean importantes para un modelo. Si agregamos step_other (Neighborhood, threshold = 0.01) a nuestra receta, el último \\(1\\%\\) de los vecindarios se agrupará en un nuevo nivel llamado “otro”, esto atrapará a 8 vecindarios. simple_ames &lt;- recipe( Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type, data = ames) %&gt;% step_other(Neighborhood, threshold = 0.01) %&gt;% prep() ejemplo &lt;- juice(simple_ames) ggplot(ejemplo, aes(y = Neighborhood)) + geom_bar() + labs(y = NULL) 4.7.2 Imputaciones La función step_unknown crea una categoría nombrada unknown, la cual sirve como reemplazo de datos categóricos faltantes, sin embargo, para imputar datos numéricos se requiere de otra estrategia. Las imputaciones o sustituciones más comunes son realizadas a través de medidas de tendencia central tales como la media y mediana. A continuación se muestra un ejemplo: ames_na &lt;- ames ames_na[sample(nrow(ames), 5), c(&quot;Gr_Liv_Area&quot;, &quot;Lot_Area&quot;)] &lt;- NA ames_na %&gt;% filter(is.na(Gr_Liv_Area) | is.na(Lot_Area)) %&gt;% select(Sale_Price, Gr_Liv_Area, Lot_Area) ## # A tibble: 5 × 3 ## Sale_Price Gr_Liv_Area Lot_Area ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 130000 NA NA ## 2 143000 NA NA ## 3 189000 NA NA ## 4 302000 NA NA ## 5 111500 NA NA simple_ames &lt;- recipe(Sale_Price ~ Gr_Liv_Area + Lot_Area, data = ames_na) %&gt;% step_impute_mean(Gr_Liv_Area) %&gt;% step_impute_median(Lot_Area) %&gt;% prep() bake(simple_ames, new_data = ames_na) %&gt;% filter(is.na(Gr_Liv_Area) | is.na(Lot_Area)) ## # A tibble: 0 × 3 ## # ℹ 3 variables: Gr_Liv_Area &lt;int&gt;, Lot_Area &lt;int&gt;, Sale_Price &lt;int&gt; Forzamos algunos renglones a que sean omitidos aleatoriamente. Posteriormente, estos valores son imputados mediante su media y mediana. 4.7.3 Agregar o modificar columnas Quizá la transformación más usada sea la agregación o mutación de columnas existentes. Similar a la función mutate() de dplyr, la función step_mutate() se encarga de realizar esta tarea dentro de un pipeline o receta. ejemplo &lt;- recipe( Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Year_Remod_Add, data = ames) %&gt;% step_mutate( Sale_Price_Peso = Sale_Price * 19.87, Last_Inversion = Year_Remod_Add - Year_Built ) %&gt;% step_arrange(desc(Last_Inversion)) %&gt;% prep() ejemplo ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 5 ## ## Training data contained 2930 data points and no missing data. ## ## Operations: ## ## Variable mutation for ~Sale_Price * 19.87, ~Year_Remod_Add - Yea... [trained] ## Row arrangement using ~desc(Last_Inversion) [trained] ejemplo %&gt;% bake(new_data = NULL) %&gt;% select(Sale_Price, Sale_Price_Peso, Year_Remod_Add, Year_Built, Last_Inversion) ## # A tibble: 2,930 × 5 ## Sale_Price Sale_Price_Peso Year_Remod_Add Year_Built Last_Inversion ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 131000 2602970 2007 1880 127 ## 2 265979 5285003. 2003 1880 123 ## 3 295000 5861650 2002 1880 122 ## 4 94000 1867780 1996 1875 121 ## 5 138000 2742060 2006 1890 116 ## 6 122000 2424140 1987 1872 115 ## 7 240000 4768800 2002 1890 112 ## 8 119600 2376452 2006 1895 111 ## 9 124000 2463880 1991 1880 111 ## 10 100000 1987000 1995 1885 110 ## # ℹ 2,920 more rows En este ejemplo se realiza la creación de una nueva variable y la modificación de una ya existente. 4.7.4 Interacciones Los efectos de interacción involucran dos o más predictores. Tal efecto ocurre cuando un predictor tiene un efecto sobre el resultado que depende de uno o más predictores. Numéricamente, un término de interacción entre predictores se codifica como su producto. Las interacciones solo se definen en términos de su efecto sobre el resultado y pueden ser combinaciones de diferentes tipos de datos (por ejemplo, numéricos, categóricos, etc.). Después de explorar el conjunto de datos de Ames, podríamos encontrar que las pendientes de regresión para el área habitable bruta difieren para los diferentes tipos de edificios: ggplot(ames, aes(x = Gr_Liv_Area, y = Sale_Price)) + geom_point(alpha = .2) + facet_wrap(~ Bldg_Type) + geom_smooth(method = lm, formula = y ~ x, se = FALSE, col = &quot;red&quot;) + scale_x_log10() + scale_y_log10() + labs(x = &quot;Gross Living Area&quot;, y = &quot;Sale Price (USD)&quot;) Con la receta actual, step_dummy() ya ha creado variables ficticias. ¿Cómo combinaríamos estos para una interacción? El paso adicional se vería como step_interact(~ términos de interacción) donde los términos en el lado derecho de la tilde son las interacciones. Estos pueden incluir selectores, por lo que sería apropiado usar: simple_ames &lt;- recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type, data = ames) %&gt;% step_other(Neighborhood, threshold = 0.05) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_interact( ~ Gr_Liv_Area:starts_with(&quot;Bldg_Type_&quot;) ) %&gt;% prep() simple_ames %&gt;% bake(new_data = NULL) %&gt;% glimpse() ## Rows: 2,930 ## Columns: 19 ## $ Gr_Liv_Area &lt;int&gt; 1656, 896, 1329, 2110, 1629, 1604, 13… ## $ Year_Built &lt;int&gt; 1960, 1961, 1958, 1968, 1997, 1998, 2… ## $ Sale_Price &lt;int&gt; 215000, 105000, 172000, 244000, 18990… ## $ Neighborhood_College_Creek &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Old_Town &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Edwards &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Somerset &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Northridge_Heights &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Gilbert &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1… ## $ Neighborhood_Sawyer &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Neighborhood_other &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0… ## $ Bldg_Type_TwoFmCon &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Bldg_Type_Duplex &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Bldg_Type_Twnhs &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Bldg_Type_TwnhsE &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0… ## $ Gr_Liv_Area_x_Bldg_Type_TwoFmCon &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Gr_Liv_Area_x_Bldg_Type_Duplex &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Gr_Liv_Area_x_Bldg_Type_Twnhs &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Gr_Liv_Area_x_Bldg_Type_TwnhsE &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1338, 1280, 1616, 0… Se pueden especificar interacciones adicionales en esta fórmula separándolas con el signo \\(*\\). 4.7.5 Transformaciones generales Reflejando las operaciones originales de dplyr, los siguientes pasos se pueden usar para realizar una variedad de operaciones básicas a los datos. step_select(): Selecciona un subconjunto de variables específicas en el conjunto de datos. step_mutate(): Crea una nueva variable o modifica una existente usando dplyr::mutate(). step_mutate_at(): Lee una especificación de un paso de receta que modificará las variables seleccionadas usando una función común a través de dplyr::mutate_at(). step_filter(): Crea una especificación de un paso de receta que eliminará filas usando dplyr::filter(). step_arrange(): Ordena el conjunto de datos de acuerdo con una o más variables. step_rm(): Crea una especificación de un paso de receta que eliminará las variables según su nombre, tipo o función. step_nzv(): Realiza una selección de variables eliminando todas aquellas cuya varianza se encuentre cercana a cero. step_naomit(): Elimina todos los renglones que tengan alguna variable con valores perdidos. step_normalize(): Centra y escala las variables numéricas especificadas, generando una transformación a una distribución normal estándar. step_range(): Transforma el rango de un conjunto de variables numéricas al especificado. step_interact(): Crea un nuevo conjunto de variables basadas en la interacción entre dos variables. step_ratio(): Crea una nueva variable a partir del cociente entre dos variables. all_predictors(): Selecciona a todos los predictores del conjunto de entrenamiento para aplicarles alguna de las funciones mencionadas. all_numeric_predictors(): Selecciona a todos los predictores numéricos del conjunto de entrenamiento para aplicarles alguna de las funciones mencionadas. all_nominal_predictors(): Selecciona a todos los predictores nominales del conjunto de entrenamiento para aplicarles alguna de las funciones mencionadas. La guía completa de las familia de funciones step puede consultarse en la documentación oficial 4.8 Datos y tipos de modelos En este curso se realizarán ejemplos tanto de regresión como de clasificación. Cada uno de los modelos a estudiar se implementarán tanto para respuestas continuas como variables categóricas Regresión: Preparación de datos En esta sección, prepararemos datos para ajustar modelos de regresión y de clasificación, usando la paquetería recipes. Primero ajustaremos la receta, después obtendremos la receta actualizada con las estimaciones y al final el conjunto de datos listo para el modelo. Datos de regresión: Ames Housing Data Los datos que usaremos son los de Ames Housing Data, el conjunto de datos contiene información de la Ames Assessor’s Office utilizada para calcular valuaciones para propiedades residenciales individuales vendidas en Ames, IA, de 2006 a 2010. Podemos encontrar más información en el siguiente link Ames Housing Data. library(tidymodels) library(stringr) library(tidyverse) data(ames) glimpse(ames) ## Rows: 2,930 ## Columns: 74 ## $ MS_SubClass &lt;fct&gt; One_Story_1946_and_Newer_All_Styles, One_Story_1946… ## $ MS_Zoning &lt;fct&gt; Residential_Low_Density, Residential_High_Density, … ## $ Lot_Frontage &lt;dbl&gt; 141, 80, 81, 93, 74, 78, 41, 43, 39, 60, 75, 0, 63,… ## $ Lot_Area &lt;int&gt; 31770, 11622, 14267, 11160, 13830, 9978, 4920, 5005… ## $ Street &lt;fct&gt; Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pav… ## $ Alley &lt;fct&gt; No_Alley_Access, No_Alley_Access, No_Alley_Access, … ## $ Lot_Shape &lt;fct&gt; Slightly_Irregular, Regular, Slightly_Irregular, Re… ## $ Land_Contour &lt;fct&gt; Lvl, Lvl, Lvl, Lvl, Lvl, Lvl, Lvl, HLS, Lvl, Lvl, L… ## $ Utilities &lt;fct&gt; AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, All… ## $ Lot_Config &lt;fct&gt; Corner, Inside, Corner, Corner, Inside, Inside, Ins… ## $ Land_Slope &lt;fct&gt; Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, G… ## $ Neighborhood &lt;fct&gt; North_Ames, North_Ames, North_Ames, North_Ames, Gil… ## $ Condition_1 &lt;fct&gt; Norm, Feedr, Norm, Norm, Norm, Norm, Norm, Norm, No… ## $ Condition_2 &lt;fct&gt; Norm, Norm, Norm, Norm, Norm, Norm, Norm, Norm, Nor… ## $ Bldg_Type &lt;fct&gt; OneFam, OneFam, OneFam, OneFam, OneFam, OneFam, Twn… ## $ House_Style &lt;fct&gt; One_Story, One_Story, One_Story, One_Story, Two_Sto… ## $ Overall_Cond &lt;fct&gt; Average, Above_Average, Above_Average, Average, Ave… ## $ Year_Built &lt;int&gt; 1960, 1961, 1958, 1968, 1997, 1998, 2001, 1992, 199… ## $ Year_Remod_Add &lt;int&gt; 1960, 1961, 1958, 1968, 1998, 1998, 2001, 1992, 199… ## $ Roof_Style &lt;fct&gt; Hip, Gable, Hip, Hip, Gable, Gable, Gable, Gable, G… ## $ Roof_Matl &lt;fct&gt; CompShg, CompShg, CompShg, CompShg, CompShg, CompSh… ## $ Exterior_1st &lt;fct&gt; BrkFace, VinylSd, Wd Sdng, BrkFace, VinylSd, VinylS… ## $ Exterior_2nd &lt;fct&gt; Plywood, VinylSd, Wd Sdng, BrkFace, VinylSd, VinylS… ## $ Mas_Vnr_Type &lt;fct&gt; Stone, None, BrkFace, None, None, BrkFace, None, No… ## $ Mas_Vnr_Area &lt;dbl&gt; 112, 0, 108, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6… ## $ Exter_Cond &lt;fct&gt; Typical, Typical, Typical, Typical, Typical, Typica… ## $ Foundation &lt;fct&gt; CBlock, CBlock, CBlock, CBlock, PConc, PConc, PConc… ## $ Bsmt_Cond &lt;fct&gt; Good, Typical, Typical, Typical, Typical, Typical, … ## $ Bsmt_Exposure &lt;fct&gt; Gd, No, No, No, No, No, Mn, No, No, No, No, No, No,… ## $ BsmtFin_Type_1 &lt;fct&gt; BLQ, Rec, ALQ, ALQ, GLQ, GLQ, GLQ, ALQ, GLQ, Unf, U… ## $ BsmtFin_SF_1 &lt;dbl&gt; 2, 6, 1, 1, 3, 3, 3, 1, 3, 7, 7, 1, 7, 3, 3, 1, 3, … ## $ BsmtFin_Type_2 &lt;fct&gt; Unf, LwQ, Unf, Unf, Unf, Unf, Unf, Unf, Unf, Unf, U… ## $ BsmtFin_SF_2 &lt;dbl&gt; 0, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1120, 0… ## $ Bsmt_Unf_SF &lt;dbl&gt; 441, 270, 406, 1045, 137, 324, 722, 1017, 415, 994,… ## $ Total_Bsmt_SF &lt;dbl&gt; 1080, 882, 1329, 2110, 928, 926, 1338, 1280, 1595, … ## $ Heating &lt;fct&gt; GasA, GasA, GasA, GasA, GasA, GasA, GasA, GasA, Gas… ## $ Heating_QC &lt;fct&gt; Fair, Typical, Typical, Excellent, Good, Excellent,… ## $ Central_Air &lt;fct&gt; Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, … ## $ Electrical &lt;fct&gt; SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SB… ## $ First_Flr_SF &lt;int&gt; 1656, 896, 1329, 2110, 928, 926, 1338, 1280, 1616, … ## $ Second_Flr_SF &lt;int&gt; 0, 0, 0, 0, 701, 678, 0, 0, 0, 776, 892, 0, 676, 0,… ## $ Gr_Liv_Area &lt;int&gt; 1656, 896, 1329, 2110, 1629, 1604, 1338, 1280, 1616… ## $ Bsmt_Full_Bath &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, … ## $ Bsmt_Half_Bath &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Full_Bath &lt;int&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 2, … ## $ Half_Bath &lt;int&gt; 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, … ## $ Bedroom_AbvGr &lt;int&gt; 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 1, 4, 4, … ## $ Kitchen_AbvGr &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ TotRms_AbvGrd &lt;int&gt; 7, 5, 6, 8, 6, 7, 6, 5, 5, 7, 7, 6, 7, 5, 4, 12, 8,… ## $ Functional &lt;fct&gt; Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, T… ## $ Fireplaces &lt;int&gt; 2, 0, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, … ## $ Garage_Type &lt;fct&gt; Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, Att… ## $ Garage_Finish &lt;fct&gt; Fin, Unf, Unf, Fin, Fin, Fin, Fin, RFn, RFn, Fin, F… ## $ Garage_Cars &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, … ## $ Garage_Area &lt;dbl&gt; 528, 730, 312, 522, 482, 470, 582, 506, 608, 442, 4… ## $ Garage_Cond &lt;fct&gt; Typical, Typical, Typical, Typical, Typical, Typica… ## $ Paved_Drive &lt;fct&gt; Partial_Pavement, Paved, Paved, Paved, Paved, Paved… ## $ Wood_Deck_SF &lt;int&gt; 210, 140, 393, 0, 212, 360, 0, 0, 237, 140, 157, 48… ## $ Open_Porch_SF &lt;int&gt; 62, 0, 36, 0, 34, 36, 0, 82, 152, 60, 84, 21, 75, 0… ## $ Enclosed_Porch &lt;int&gt; 0, 0, 0, 0, 0, 0, 170, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Three_season_porch &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Screen_Porch &lt;int&gt; 0, 120, 0, 0, 0, 0, 0, 144, 0, 0, 0, 0, 0, 0, 140, … ## $ Pool_Area &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Pool_QC &lt;fct&gt; No_Pool, No_Pool, No_Pool, No_Pool, No_Pool, No_Poo… ## $ Fence &lt;fct&gt; No_Fence, Minimum_Privacy, No_Fence, No_Fence, Mini… ## $ Misc_Feature &lt;fct&gt; None, None, Gar2, None, None, None, None, None, Non… ## $ Misc_Val &lt;int&gt; 0, 0, 12500, 0, 0, 0, 0, 0, 0, 0, 0, 500, 0, 0, 0, … ## $ Mo_Sold &lt;int&gt; 5, 6, 6, 4, 3, 6, 4, 1, 3, 6, 4, 3, 5, 2, 6, 6, 6, … ## $ Year_Sold &lt;int&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 201… ## $ Sale_Type &lt;fct&gt; WD , WD , WD , WD , WD , WD , WD , WD , WD , WD , W… ## $ Sale_Condition &lt;fct&gt; Normal, Normal, Normal, Normal, Normal, Normal, Nor… ## $ Sale_Price &lt;int&gt; 215000, 105000, 172000, 244000, 189900, 195500, 213… ## $ Longitude &lt;dbl&gt; -93.61975, -93.61976, -93.61939, -93.61732, -93.638… ## $ Latitude &lt;dbl&gt; 42.05403, 42.05301, 42.05266, 42.05125, 42.06090, 4… 4.8.1 Separación de los datos El primer paso para crear un modelo de regresión es dividir nuestros datos originales en un conjunto de entrenamiento y prueba. No hay que olvidar usar siempre una semilla con la función set.seed() para que sus resultados sean reproducibles. Primero usaremos la función initial_split() de rsample para dividir los datos ames en conjuntos de entrenamiento y prueba. Usamos el parámetro prop para indicar la proporción de los conjuntos train y test. set.seed(4595) ames_split &lt;- initial_split(ames, prop = 0.75) El objeto ames_split es un objeto rsplit y solo contiene la información de partición, para obtener los conjuntos de datos resultantes, aplicamos dos funciones adicionales, training y testing. ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) Estos objetos son data frames con las mismas columnas que los datos originales, pero solo las filas apropiadas para cada conjunto. También existe la función vfold_cv que se usa para crear v particiones del conjunto de entrenamiento. set.seed(2453) ames_folds&lt;- vfold_cv(ames_train, v = 10) Ya con los conjuntos de entrenamiento y prueba definidos, iniciaremos con feature engineering sobre el conjunto de entrenamiento. 4.8.2 Definición de la receta Ahora usaremos la función vista en la sección anterior, recipe(), para definir los pasos de preprocesamiento antes de usar los datos para modelado. receta_casas &lt;- recipe(Sale_Price ~ . , data = ames_train) %&gt;% step_unknown(Alley) %&gt;% step_rename(Year_Remod = Year_Remod_Add) %&gt;% step_rename(ThirdSsn_Porch = Three_season_porch) %&gt;% step_ratio(Bedroom_AbvGr, denom = denom_vars(Gr_Liv_Area)) %&gt;% step_mutate( Age_House = Year_Sold - Year_Remod, TotalSF = Gr_Liv_Area + Total_Bsmt_SF, AvgRoomSF = Gr_Liv_Area / TotRms_AbvGrd, Pool = if_else(Pool_Area &gt; 0, 1, 0), Exter_Cond = forcats::fct_collapse(Exter_Cond, Good = c(&quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;))) %&gt;% step_relevel(Exter_Cond, ref_level = &quot;Good&quot;) %&gt;% step_normalize(all_predictors(), -all_nominal()) %&gt;% step_dummy(all_nominal()) %&gt;% step_interact(~ Second_Flr_SF:First_Flr_SF) %&gt;% step_interact(~ matches(&quot;Bsmt_Cond&quot;):TotRms_AbvGrd) %&gt;% step_rm( First_Flr_SF, Second_Flr_SF, Year_Remod, Bsmt_Full_Bath, Bsmt_Half_Bath, Kitchen_AbvGr, BsmtFin_Type_1_Unf, Total_Bsmt_SF, Kitchen_AbvGr, Pool_Area, Gr_Liv_Area, Sale_Type_Oth, Sale_Type_VWD ) %&gt;% prep() Usamos la función step_mutate() para generar nuevas variables dentro de la receta. La función step_interact() nos ayuda a crear nuevas variables que son interacciones entre las variables especificadas. Con la función step_ratio() creamos proporciones con las variables especificadas. forcats::fct_collapse() se usa para recategorizar variables, colapsando categorías de la variable. step_relevel nos ayuda a asignar la categoria deseada de una variable como referencia. step_normalize() es de gran utilidad ya que sirve para normalizar las variables que se le indiquen. step_dummy() Nos ayuda a crear variables One Hot Encoding. Por último usamos la función step_rm() para eliminar variables que no son de utilidad para el modelo. Ahora crearemos algunas variables auxiliares que podrían ser de utilidad para el ajuste de un modelo de regresión. Recordemos que la función recipe() solo son los pasos a seguir, necesitamos usar la función prep() que nos devuelve una receta actualizada con las estimaciones y la función juice() que nos devuelve la matriz de diseño. casa_juiced &lt;- juice(receta_casas) casa_juiced ## # A tibble: 2,197 × 275 ## Lot_Frontage Lot_Area Year_Built Mas_Vnr_Area BsmtFin_SF_1 BsmtFin_SF_2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.214 0.604 0.848 -0.572 -0.512 -0.293 ## 2 0.363 -0.216 -0.114 -0.572 -0.512 -0.293 ## 3 1.05 -0.0159 1.08 -0.572 1.26 -0.293 ## 4 -0.173 4.87 1.15 3.49 -0.512 -0.293 ## 5 0.512 0.256 1.11 0.526 1.26 -0.293 ## 6 -0.233 -0.696 -0.844 -0.572 1.26 -0.293 ## 7 0.125 -0.261 -0.446 -0.572 -0.512 -0.293 ## 8 0.571 -0.193 -0.479 -0.572 0.819 -0.293 ## 9 0.274 0.704 0.981 -0.572 1.26 -0.293 ## 10 0.0651 -0.356 -0.712 -0.572 0.375 -0.293 ## # ℹ 2,187 more rows ## # ℹ 269 more variables: Bsmt_Unf_SF &lt;dbl&gt;, Full_Bath &lt;dbl&gt;, Half_Bath &lt;dbl&gt;, ## # Bedroom_AbvGr &lt;dbl&gt;, TotRms_AbvGrd &lt;dbl&gt;, Fireplaces &lt;dbl&gt;, ## # Garage_Cars &lt;dbl&gt;, Garage_Area &lt;dbl&gt;, Wood_Deck_SF &lt;dbl&gt;, ## # Open_Porch_SF &lt;dbl&gt;, Enclosed_Porch &lt;dbl&gt;, ThirdSsn_Porch &lt;dbl&gt;, ## # Screen_Porch &lt;dbl&gt;, Misc_Val &lt;dbl&gt;, Mo_Sold &lt;dbl&gt;, Year_Sold &lt;dbl&gt;, ## # Longitude &lt;dbl&gt;, Latitude &lt;dbl&gt;, Sale_Price &lt;int&gt;, … casa_test_bake &lt;- bake(receta_casas, new_data = ames_test) glimpse(casa_test_bake) ## Rows: 733 ## Columns: 275 ## $ Lot_Frontage &lt;dbl&gt; 0.6607556, -1.72… ## $ Lot_Area &lt;dbl&gt; 0.15995167, -0.2… ## $ Year_Built &lt;dbl&gt; -0.34660802, 0.6… ## $ Mas_Vnr_Area &lt;dbl&gt; -0.5721904, -0.5… ## $ BsmtFin_SF_1 &lt;dbl&gt; 0.81885215, -1.3… ## $ BsmtFin_SF_2 &lt;dbl&gt; 0.5851851, -0.29… ## $ Bsmt_Unf_SF &lt;dbl&gt; -0.65580176, -0.… ## $ Full_Bath &lt;dbl&gt; -1.0284858, 0.79… ## $ Half_Bath &lt;dbl&gt; -0.7465678, -0.7… ## $ Bedroom_AbvGr &lt;dbl&gt; -1.0749072, 0.15… ## $ TotRms_AbvGrd &lt;dbl&gt; -0.9221672, -0.2… ## $ Fireplaces &lt;dbl&gt; -0.9297733, -0.9… ## $ Garage_Cars &lt;dbl&gt; -1.0124349, 0.29… ## $ Garage_Area &lt;dbl&gt; 1.19047388, -0.2… ## $ Wood_Deck_SF &lt;dbl&gt; 0.3353735, 3.013… ## $ Open_Porch_SF &lt;dbl&gt; -0.70298891, -0.… ## $ Enclosed_Porch &lt;dbl&gt; -0.3536614, -0.3… ## $ ThirdSsn_Porch &lt;dbl&gt; -0.1029207, -0.1… ## $ Screen_Porch &lt;dbl&gt; 1.9128593, -0.28… ## $ Misc_Val &lt;dbl&gt; -0.09569659, 0.6… ## $ Mo_Sold &lt;dbl&gt; -0.09320608, -1.… ## $ Year_Sold &lt;dbl&gt; 1.672416, 1.6724… ## $ Longitude &lt;dbl&gt; 0.90531385, 0.27… ## $ Latitude &lt;dbl&gt; 1.00647452, 1.24… ## $ Sale_Price &lt;int&gt; 105000, 185000, … ## $ Bedroom_AbvGr_o_Gr_Liv_Area &lt;dbl&gt; 0.34141287, 0.83… ## $ Age_House &lt;dbl&gt; 1.21786294, -0.9… ## $ TotalSF &lt;dbl&gt; -0.94110413, -0.… ## $ AvgRoomSF &lt;dbl&gt; -1.11699593, -0.… ## $ Pool &lt;dbl&gt; -0.0709206, -0.0… ## $ MS_SubClass_One_Story_1945_and_Older &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_One_Story_with_Finished_Attic_All_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_One_and_Half_Story_Unfinished_All_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_One_and_Half_Story_Finished_All_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_Two_Story_1946_and_Newer &lt;dbl&gt; 0, 0, 1, 0, 0, 1… ## $ MS_SubClass_Two_Story_1945_and_Older &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_Two_and_Half_Story_All_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_Split_or_Multilevel &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_Split_Foyer &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_Duplex_All_Styles_and_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_One_Story_PUD_1946_and_Newer &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_One_and_Half_Story_PUD_All_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_Two_Story_PUD_1946_and_Newer &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_PUD_Multilevel_Split_Level_Foyer &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_Two_Family_conversion_All_Styles_and_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_Zoning_Residential_High_Density &lt;dbl&gt; 1, 0, 0, 0, 0, 0… ## $ MS_Zoning_Residential_Low_Density &lt;dbl&gt; 0, 1, 1, 1, 1, 0… ## $ MS_Zoning_Residential_Medium_Density &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_Zoning_A_agr &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_Zoning_C_all &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_Zoning_I_all &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Street_Pave &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Alley_No_Alley_Access &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Alley_Paved &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Alley_unknown &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Lot_Shape_Slightly_Irregular &lt;dbl&gt; 0, 1, 1, 0, 0, 0… ## $ Lot_Shape_Moderately_Irregular &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Lot_Shape_Irregular &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Land_Contour_HLS &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Land_Contour_Low &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Land_Contour_Lvl &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Utilities_NoSeWa &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Utilities_NoSewr &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Lot_Config_CulDSac &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Lot_Config_FR2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Lot_Config_FR3 &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Lot_Config_Inside &lt;dbl&gt; 1, 1, 1, 0, 1, 1… ## $ Land_Slope_Mod &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Land_Slope_Sev &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_College_Creek &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Old_Town &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Edwards &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Somerset &lt;dbl&gt; 0, 0, 0, 0, 0, 1… ## $ Neighborhood_Northridge_Heights &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Gilbert &lt;dbl&gt; 0, 1, 1, 1, 0, 0… ## $ Neighborhood_Sawyer &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Northwest_Ames &lt;dbl&gt; 0, 0, 0, 0, 1, 0… ## $ Neighborhood_Sawyer_West &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Mitchell &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Brookside &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Crawford &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Iowa_DOT_and_Rail_Road &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Timberland &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Northridge &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Stone_Brook &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_South_and_West_of_Iowa_State_University &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Clear_Creek &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Meadow_Village &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Briardale &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Bloomington_Heights &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Veenker &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Northpark_Villa &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Blueste &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Greens &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Green_Hills &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Landmark &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Hayden_Lake &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_1_Feedr &lt;dbl&gt; 1, 0, 0, 0, 0, 0… ## $ Condition_1_Norm &lt;dbl&gt; 0, 1, 1, 1, 1, 1… ## $ Condition_1_PosA &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_1_PosN &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_1_RRAe &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_1_RRAn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_1_RRNe &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_1_RRNn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_2_Feedr &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_2_Norm &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Condition_2_PosA &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_2_PosN &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_2_RRAe &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_2_RRAn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_2_RRNn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bldg_Type_TwoFmCon &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bldg_Type_Duplex &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bldg_Type_Twnhs &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bldg_Type_TwnhsE &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ House_Style_One_and_Half_Unf &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ House_Style_One_Story &lt;dbl&gt; 1, 1, 0, 1, 1, 0… ## $ House_Style_SFoyer &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ House_Style_SLvl &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ House_Style_Two_and_Half_Fin &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ House_Style_Two_and_Half_Unf &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ House_Style_Two_Story &lt;dbl&gt; 0, 0, 1, 0, 0, 1… ## $ Overall_Cond_Poor &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Overall_Cond_Fair &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Overall_Cond_Below_Average &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Overall_Cond_Average &lt;dbl&gt; 0, 0, 1, 1, 0, 1… ## $ Overall_Cond_Above_Average &lt;dbl&gt; 1, 0, 0, 0, 1, 0… ## $ Overall_Cond_Good &lt;dbl&gt; 0, 1, 0, 0, 0, 0… ## $ Overall_Cond_Very_Good &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Overall_Cond_Excellent &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Overall_Cond_Very_Excellent &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Style_Gable &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Roof_Style_Gambrel &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Style_Hip &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Style_Mansard &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Style_Shed &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Matl_CompShg &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Roof_Matl_Membran &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Matl_Metal &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Matl_Roll &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Matl_Tar.Grv &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Matl_WdShake &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Matl_WdShngl &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_AsphShn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_BrkComm &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_BrkFace &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_CBlock &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_CemntBd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_HdBoard &lt;dbl&gt; 0, 1, 0, 0, 0, 0… ## $ Exterior_1st_ImStucc &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_MetalSd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_Plywood &lt;dbl&gt; 0, 0, 0, 0, 1, 0… ## $ Exterior_1st_PreCast &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_Stone &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_Stucco &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_VinylSd &lt;dbl&gt; 1, 0, 1, 1, 0, 1… ## $ Exterior_1st_Wd.Sdng &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_WdShing &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_AsphShn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_Brk.Cmn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_BrkFace &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_CBlock &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_CmentBd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_HdBoard &lt;dbl&gt; 0, 1, 0, 0, 0, 0… ## $ Exterior_2nd_ImStucc &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_MetalSd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_Other &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_Plywood &lt;dbl&gt; 0, 0, 0, 0, 1, 0… ## $ Exterior_2nd_PreCast &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_Stone &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_Stucco &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_VinylSd &lt;dbl&gt; 1, 0, 1, 1, 0, 1… ## $ Exterior_2nd_Wd.Sdng &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_Wd.Shng &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Mas_Vnr_Type_BrkFace &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Mas_Vnr_Type_CBlock &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Mas_Vnr_Type_None &lt;dbl&gt; 1, 1, 1, 1, 0, 1… ## $ Mas_Vnr_Type_Stone &lt;dbl&gt; 0, 0, 0, 0, 1, 0… ## $ Exter_Cond_Fair &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exter_Cond_Poor &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Foundation_CBlock &lt;dbl&gt; 1, 0, 0, 1, 1, 0… ## $ Foundation_PConc &lt;dbl&gt; 0, 1, 1, 0, 0, 1… ## $ Foundation_Slab &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Foundation_Stone &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Foundation_Wood &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_Fair &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_Good &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_No_Basement &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_Poor &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_Typical &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Bsmt_Exposure_Gd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Exposure_Mn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Exposure_No &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Bsmt_Exposure_No_Basement &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ BsmtFin_Type_1_BLQ &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ BsmtFin_Type_1_GLQ &lt;dbl&gt; 0, 0, 0, 0, 0, 1… ## $ BsmtFin_Type_1_LwQ &lt;dbl&gt; 0, 0, 0, 1, 0, 0… ## $ BsmtFin_Type_1_No_Basement &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ BsmtFin_Type_1_Rec &lt;dbl&gt; 1, 0, 0, 0, 0, 0… ## $ BsmtFin_Type_2_BLQ &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ BsmtFin_Type_2_GLQ &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ BsmtFin_Type_2_LwQ &lt;dbl&gt; 1, 0, 0, 0, 0, 0… ## $ BsmtFin_Type_2_No_Basement &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ BsmtFin_Type_2_Rec &lt;dbl&gt; 0, 0, 0, 0, 1, 0… ## $ BsmtFin_Type_2_Unf &lt;dbl&gt; 0, 1, 1, 1, 0, 1… ## $ Heating_GasA &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Heating_GasW &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Heating_Grav &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Heating_OthW &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Heating_Wall &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Heating_QC_Fair &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Heating_QC_Good &lt;dbl&gt; 0, 0, 1, 0, 0, 0… ## $ Heating_QC_Poor &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Heating_QC_Typical &lt;dbl&gt; 1, 0, 0, 0, 1, 0… ## $ Central_Air_Y &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Electrical_FuseF &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Electrical_FuseP &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Electrical_Mix &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Electrical_SBrkr &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Electrical_Unknown &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Functional_Maj2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Functional_Min1 &lt;dbl&gt; 0, 0, 0, 0, 1, 0… ## $ Functional_Min2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Functional_Mod &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Functional_Sal &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Functional_Sev &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Functional_Typ &lt;dbl&gt; 1, 1, 1, 1, 0, 1… ## $ Garage_Type_Basment &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Type_BuiltIn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Type_CarPort &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Type_Detchd &lt;dbl&gt; 0, 0, 0, 1, 0, 0… ## $ Garage_Type_More_Than_Two_Types &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Type_No_Garage &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Finish_No_Garage &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Finish_RFn &lt;dbl&gt; 0, 0, 0, 0, 0, 1… ## $ Garage_Finish_Unf &lt;dbl&gt; 1, 0, 0, 1, 1, 0… ## $ Garage_Cond_Fair &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Cond_Good &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Cond_No_Garage &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Cond_Poor &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Cond_Typical &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Paved_Drive_Partial_Pavement &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Paved_Drive_Paved &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Pool_QC_Fair &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Pool_QC_Good &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Pool_QC_No_Pool &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Pool_QC_Typical &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Fence_Good_Wood &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Fence_Minimum_Privacy &lt;dbl&gt; 1, 0, 0, 0, 1, 0… ## $ Fence_Minimum_Wood_Wire &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Fence_No_Fence &lt;dbl&gt; 0, 0, 1, 1, 0, 1… ## $ Misc_Feature_Gar2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Misc_Feature_None &lt;dbl&gt; 1, 0, 1, 1, 1, 1… ## $ Misc_Feature_Othr &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Misc_Feature_Shed &lt;dbl&gt; 0, 1, 0, 0, 0, 0… ## $ Misc_Feature_TenC &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Type_Con &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Type_ConLD &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Type_ConLI &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Type_ConLw &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Type_CWD &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Type_New &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Type_WD. &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Sale_Condition_AdjLand &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Condition_Alloca &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Condition_Family &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Condition_Normal &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Sale_Condition_Partial &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Second_Flr_SF_x_First_Flr_SF &lt;dbl&gt; 0.52453728, -0.0… ## $ Bsmt_Cond_Fair_x_TotRms_AbvGrd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_Good_x_TotRms_AbvGrd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_No_Basement_x_TotRms_AbvGrd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_Poor_x_TotRms_AbvGrd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_Typical_x_TotRms_AbvGrd &lt;dbl&gt; -0.9221672, -0.2… Clasificación: Preparación de Datos Ahora prepararemos los datos para un ejemplo de churn, es decir, la tasa de cancelación de clientes. Usaremos datos de Telco. En este conjunto de datos, la variable a predecir será la cancelación por parte del cliente de los servicios de telecomunicaciones contratados. telco &lt;- read_csv(&quot;data/Churn.csv&quot;) glimpse(telco) ## Rows: 7,043 ## Columns: 21 ## $ customerID &lt;chr&gt; &quot;7590-VHVEG&quot;, &quot;5575-GNVDE&quot;, &quot;3668-QPYBK&quot;, &quot;7795-CFOCW… ## $ gender &lt;chr&gt; &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;,… ## $ SeniorCitizen &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ Partner &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ Dependents &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;… ## $ tenure &lt;dbl&gt; 1, 34, 2, 45, 2, 8, 22, 10, 28, 62, 13, 16, 58, 49, 2… ## $ PhoneService &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, … ## $ MultipleLines &lt;chr&gt; &quot;No phone service&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No phone service&quot;, &quot;… ## $ InternetService &lt;chr&gt; &quot;DSL&quot;, &quot;DSL&quot;, &quot;DSL&quot;, &quot;DSL&quot;, &quot;Fiber optic&quot;, &quot;Fiber opt… ## $ OnlineSecurity &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;… ## $ OnlineBackup &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;N… ## $ DeviceProtection &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Y… ## $ TechSupport &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ StreamingTV &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Ye… ## $ StreamingMovies &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ Contract &lt;chr&gt; &quot;Month-to-month&quot;, &quot;One year&quot;, &quot;Month-to-month&quot;, &quot;One … ## $ PaperlessBilling &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, … ## $ PaymentMethod &lt;chr&gt; &quot;Electronic check&quot;, &quot;Mailed check&quot;, &quot;Mailed check&quot;, &quot;… ## $ MonthlyCharges &lt;dbl&gt; 29.85, 56.95, 53.85, 42.30, 70.70, 99.65, 89.10, 29.7… ## $ TotalCharges &lt;dbl&gt; 29.85, 1889.50, 108.15, 1840.75, 151.65, 820.50, 1949… ## $ Churn &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Y… 4.8.3 Separación de los datos Como en el ejemplo de regresión, primero crearemos los conjuntos de entrenamiento y de prueba. set.seed(1234) telco_split &lt;- initial_split(telco, prop = .7) telco_train &lt;- training(telco_split) telco_test &lt;- testing(telco_split) 4.8.4 Definición de la receta A continuación, se presenta el desarrollo de una receta para el conjunto de datos de Telco: telco_rec &lt;- recipe(Churn ~ ., data = telco_train) %&gt;% update_role(customerID, new_role = &quot;id variable&quot;) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_impute_median(all_numeric_predictors()) %&gt;% step_rm(customerID, skip=T) %&gt;% prep() Ahora recuperamos la matriz de diseño con las funciones prep() y juice(). telco_juiced &lt;- juice(telco_rec) glimpse(telco_juiced) ## Rows: 4,930 ## Columns: 31 ## $ SeniorCitizen &lt;dbl&gt; -0.4417148, -0.4417148, -0.44171… ## $ tenure &lt;dbl&gt; 0.1915835, 0.3140505, -0.4207516… ## $ MonthlyCharges &lt;dbl&gt; -1.50429475, 0.66367718, -0.5084… ## $ TotalCharges &lt;dbl&gt; -0.66321610, 0.47214217, -0.5462… ## $ Churn &lt;fct&gt; No, No, No, Yes, Yes, No, No, No… ## $ gender_Male &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,… ## $ Partner_Yes &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,… ## $ Dependents_Yes &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,… ## $ PhoneService_Yes &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,… ## $ MultipleLines_No.phone.service &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,… ## $ MultipleLines_Yes &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,… ## $ InternetService_Fiber.optic &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,… ## $ InternetService_No &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ OnlineSecurity_No.internet.service &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ OnlineSecurity_Yes &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,… ## $ OnlineBackup_No.internet.service &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ OnlineBackup_Yes &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,… ## $ DeviceProtection_No.internet.service &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ DeviceProtection_Yes &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,… ## $ TechSupport_No.internet.service &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ TechSupport_Yes &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,… ## $ StreamingTV_No.internet.service &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ StreamingTV_Yes &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,… ## $ StreamingMovies_No.internet.service &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ StreamingMovies_Yes &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,… ## $ Contract_One.year &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,… ## $ Contract_Two.year &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,… ## $ PaperlessBilling_Yes &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,… ## $ PaymentMethod_Credit.card..automatic. &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,… ## $ PaymentMethod_Electronic.check &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,… ## $ PaymentMethod_Mailed.check &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… telco_test_bake &lt;- bake(telco_rec, new_data = telco_test) glimpse(telco_test_bake) ## Rows: 2,113 ## Columns: 32 ## $ customerID &lt;chr&gt; &quot;5575-GNVDE&quot;, &quot;9305-CDSKC&quot;, &quot;671… ## $ SeniorCitizen &lt;dbl&gt; -0.4417148, -0.4417148, -0.44171… ## $ tenure &lt;dbl&gt; 0.06911644, -0.99226439, -0.9106… ## $ MonthlyCharges &lt;dbl&gt; -0.27067882, 1.14914329, -1.1751… ## $ TotalCharges &lt;dbl&gt; -0.1752116, -0.6472105, -0.87618… ## $ Churn &lt;fct&gt; No, Yes, No, No, No, No, No, No,… ## $ gender_Male &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,… ## $ Partner_Yes &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,… ## $ Dependents_Yes &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,… ## $ PhoneService_Yes &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,… ## $ MultipleLines_No.phone.service &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,… ## $ MultipleLines_Yes &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,… ## $ InternetService_Fiber.optic &lt;dbl&gt; 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,… ## $ InternetService_No &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ OnlineSecurity_No.internet.service &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ OnlineSecurity_Yes &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,… ## $ OnlineBackup_No.internet.service &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ OnlineBackup_Yes &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,… ## $ DeviceProtection_No.internet.service &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ DeviceProtection_Yes &lt;dbl&gt; 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,… ## $ TechSupport_No.internet.service &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ TechSupport_Yes &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,… ## $ StreamingTV_No.internet.service &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ StreamingTV_Yes &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,… ## $ StreamingMovies_No.internet.service &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ StreamingMovies_Yes &lt;dbl&gt; 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,… ## $ Contract_One.year &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,… ## $ Contract_Two.year &lt;dbl&gt; 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,… ## $ PaperlessBilling_Yes &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,… ## $ PaymentMethod_Credit.card..automatic. &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,… ## $ PaymentMethod_Electronic.check &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,… ## $ PaymentMethod_Mailed.check &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,… Estos fueron dos ejemplos aplicados de la paquetería recipies, existen distintas funciones step que pueden implementarse en recetas para usarse con tidymodels, en las secciones siguientes les daremos su uso para ajustar un modelo completo. "],["regresión-lineal.html", "Capítulo 5 Regresión Lineal 5.1 Ajuste de modelo 5.2 Residuos del modelo 5.3 Métricas de desempeño 5.4 Implementación en R 5.5 Métodos se selección de variables", " Capítulo 5 Regresión Lineal En esta sección aprenderemos sobre regresión lineal simple y múltiple, como se ajusta un modelo de regresión en R, las métricas de desempeño para problemas de regresión y como podemos comparar modelos con estas métricas. Existen dos tipos de modelos de regresión lineal: Regresión lineal simple: En la regresión lineal simple se utiliza una variable independiente o explicativa “X” (numérica o categórica) para estimar una variable dependiente o de respuesta numérica “Y” mediante el ajuste de una recta permita conocer la relación existente entre ambas variables. Dicha relación entre variables se expresa como: \\[Y = \\beta_0 + \\beta_1X_1 + \\epsilon \\approx b + mx\\] Donde: \\(\\epsilon \\sim Norm(0,\\sigma^2)\\) (error aleatorio) \\(\\beta_0\\) = Coeficiente de regresión 0 (Ordenada al origen o intercepto) \\(\\beta_1\\) = Coeficiente de regresión 1 (Pendiente o regresor de variable \\(X_1\\)) \\(X_1\\) = Variable explicativa observada \\(Y\\) = Respuesta numérica Debido a que los valores reales de \\(\\beta_0\\) y \\(\\beta_1\\) son desconocidos, procedemos a estimarlos estadísticamente: \\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X_1\\] Con \\(\\hat{\\beta}_0\\) el estimado de la ordenada al origen y \\(\\hat{\\beta}_1\\) el estimado de la pendiente. Regresión lineal múltiple: Cuando se utiliza más de una variable independiente, el proceso se denomina regresión lineal múltiple. En este escenario no es una recta sino un hiper-plano lo que se ajusta a partir de las covariables explicativas \\(\\{X_1, X_2, X_3, ...,X_n\\}\\) El objetivo de un modelo de regresión múltiple es tratar de explicar la relación que existe entre una variable dependiente (variable respuesta) \\(&quot;Y&quot;\\) un conjunto de variables independientes (variables explicativas) \\(\\{X1,..., Xm\\}\\), el modelo es de la forma: \\[Y = \\beta_0 + \\beta_1X_1 + \\cdot \\cdot \\cdot + \\beta_mX_m + \\epsilon\\] Donde: \\(Y\\) como variable respuesta. \\(X_1,X_2,...,X_m\\) como las variables explicativas, independientes o regresoras. \\(\\beta_1, \\beta_2,...,\\beta_m\\) Se conocen como coeficientes parciales de regresión. Cada una de ellas puede interpretarse como el efecto promedio que tiene el incremento de una unidad de la variable predictora \\(X_i\\) sobre la variable dependiente \\(Y\\), manteniéndose constantes el resto de variables. 5.1 Ajuste de modelo 5.1.1 Estimación de parámetros: Regresión lineal simple En la gran mayoría de casos, los valores \\(\\beta_0\\) y \\(\\beta_1\\) poblacionales son desconocidos, por lo que, a partir de una muestra, se obtienen sus estimaciones \\(\\hat{\\beta_0}\\) y \\(\\hat{\\beta_1}\\). Estas estimaciones se conocen como coeficientes de regresión o least square coefficient estimates, ya que toman aquellos valores que minimizan la suma de cuadrados residuales, dando lugar a la recta que pasa más cerca de todos los puntos. En términos analíticos, la expresión matemática a optimizar y solución están dadas por: \\[min(\\epsilon) \\Rightarrow min(y-\\hat{y}) = min\\{y -(\\hat{\\beta}_0 + \\hat{\\beta}_1x)\\}\\] \\[\\begin{aligned} \\hat{\\beta}_0 &amp;= \\overline{y} - \\hat{\\beta}_1\\overline{x} \\\\ \\hat{\\beta}_1 &amp;= \\frac{\\sum^n_{i=1}(x_i - \\overline{x})(y_i - \\overline{y})}{\\sum^n_{i=1}(x_i - \\overline{x})^2} =\\frac{S_{xy}}{S^2_x} \\end{aligned}\\] Donde: \\(S_{xy}\\) es la covarianza entre \\(x\\) y \\(y\\). \\(S_{x}^{2}\\) es la varianza de \\(x\\). \\(\\hat{\\beta}_0\\) es el valor esperado la variable \\(Y\\) cuando \\(X = 0\\), es decir, la intersección de la recta con el eje y. 5.1.2 Estimación de parámetros: Regresión lineal múltiple En el caso de múltiples parámetros, la notación se vuelve más sencilla al expresar el modelo mediante una combinación lineal dada por la multiplicación de matrices (álgebra lineal). \\[Y = X\\beta + \\epsilon\\] Donde: \\[Y = \\begin{pmatrix}y_1\\\\y_2\\\\.\\\\.\\\\.\\\\y_n\\end{pmatrix} \\quad \\beta = \\begin{pmatrix}\\beta_0\\\\\\beta_1\\\\.\\\\.\\\\.\\\\\\beta_m\\end{pmatrix} \\quad \\epsilon = \\begin{pmatrix}\\epsilon_1\\\\\\epsilon_2\\\\.\\\\.\\\\.\\\\\\epsilon_n\\end{pmatrix} \\quad \\quad X = \\begin{pmatrix}1 &amp; x_{11} &amp; x_{12} &amp; ... &amp; x_{1m}\\\\1 &amp; x_{21} &amp; x_{22} &amp; ... &amp; x_{2m}\\\\\\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; ... &amp; x_{nm}\\end{pmatrix}\\\\\\] El estimador por mínimos cuadrados está dado por: \\[\\hat{\\beta} = (X^TX)^{-1}X^TY\\] IMPORTANTE: Es necesario entender que para cada uno de los coeficientes de regresión se realiza una prueba de hipótesis. Una vez calculado el valor estimado, se procede a determinar si este valor es significativamente distinto de cero, por lo que la hipótesis de cada coeficiente se plantea de la siguiente manera: \\[H_0:\\beta_i=0 \\quad Vs \\quad H_1:\\beta_i\\neq0\\] El software R nos devuelve el p-value asociado a cada coeficiente de regresión. Recordemos que valores pequeños de p sugieren que al rechazar \\(H_0\\), la probabilidad de equivocarnos es baja, por lo que procedemos a rechazar la hipótesis nula. 5.2 Residuos del modelo El residuo de una estimación se define como la diferencia entre el valor observado y el valor esperado acorde al modelo. \\[\\epsilon_i= y_i -\\hat{y}_i\\] A la hora de contemplar el conjunto de residuos hay dos posibilidades: La suma del valor absoluto de cada residuo. \\[RAS=\\sum_{i=1}^{n}{|e_i|}=\\sum_{i=1}^{n}{|y_i-\\hat{y}_i|}\\] La suma del cuadrado de cada residuo (RSS). Esta es la aproximación más empleada (mínimos cuadrados) ya que magnifica las desviaciones más extremas. \\[RSS=\\sum_{i=1}^{n}{e_i^2}=\\sum_{i=1}^{n}{(y_i-\\hat{y}_i)^2}\\] Los residuos son muy importantes puesto que en ellos se basan las diferentes métricas de desempeño del modelo. Condiciones para el ajuste de una regresión lineal: Existen ciertas condiciones o supuestos que deben ser validados para el correcto ajuste de un modelo de regresión lineal, los cuales se enlistan a continuación: Linealidad: La relación entre ambas variables debe ser lineal. Distribución normal de los residuos: Los residuos se tiene que distribuir de forma normal, con media igual a 0. Varianza de residuos constante (homocedasticidad): La varianza de los residuos tiene que ser aproximadamente constante. Independencia: Las observaciones deben ser independientes unas de otras. Dado que las condiciones se verifican a partir de los residuos, primero se suele generar el modelo y después se valida. 5.3 Métricas de desempeño Dado que nuestra variable a predecir es numérica, podemos medir qué tan cerca o lejos estuvimos del número esperado dada una predicción. Las métricas de desempeño asociadas a los problemas de regresión ocupan esa distancia cómo cuantificación del desempeño o de los errores cometidos por el modelo. Las métricas más utilizadas son: MEA: Mean Absolute Error MAPE: Mean Absolute Percentual Error \\(\\quad \\Rightarrow \\quad\\) más usada para reportar resultados RMSE: Root Mean Squared Error \\(\\quad \\quad \\quad \\Rightarrow \\quad\\) más usada para entrenar modelos \\(R^2\\) : R cuadrada \\(R^2\\) : \\(R^2\\) ajustada MAE: Mean Absolute Error \\[MAE = \\frac{1}{N}\\sum_{i=1}^{N}{|y_{i}-\\hat{y}_{i}|}\\] Donde: \\(N:\\) Número de observaciones predichas. \\(y_{i}:\\) Valor real. \\(\\hat{y}_{i}:\\) Valor de la predicción. Esta métrica suma los errores absolutos de cada predicción y los divide entre el número de observaciones, para obtener el promedio absoluto del error del modelo. Ventajas Vs Desventajas: Todos los errores pesan lo mismo sin importar qué tan pequeños o qué tan grandes sean, es muy sensible a valores atípicos, y dado que obtiene el promedio puede ser que un solo error en la predicción que sea muy grande afecte al valor de todo el modelo, aún y cuando el modelo no tuvo errores tan malos para el resto de las observaciones. Se recomienda utilizar esta métrica cuando los errores importan lo mismo, es decir, importa lo mismo si se equivocó muy poco o se equivocó mucho. MAPE: Mean Absolute Percentage Error \\[MAPE = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{{|y_{i}-\\hat{y}_{i}|}}{|y_{i}|}\\] Donde: \\(N:\\) Número de observaciones predichas. \\(y_{i}:\\) Valor real. \\(\\hat{y}_{i}:\\) Valor de la predicción. Esta métrica es la métrica MAE expresada en porcentaje, por lo que mide el error del modelo en términos de porcentaje, al igual que con MAE, no hay errores negativos por el valor absoluto, y mientras más pequeño el error es mejor. Ventajas Vs Desventajas: Cuando existe un valor real de 0 esta métrica no se puede calcular, por otro lado, una de las ventajas sobre MAE es que no es sensible a valores atípicos. Se recomienda utilizar esta métrica cuando en tu problema no haya valores a predecir que puedan ser 0, por ejemplo, en ventas puedes llegar a tener 0 ventas, en este caso no podemos ocupar esta métrica. En general a las personas de negocio les gusta esta métrica pues es fácil de comprender. RMSE: Root Mean Squared Error \\[RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}{(y_{i}-\\hat{y}_{i})^2}}\\] Donde: \\(N:\\) Número de observaciones predichas. \\(y_{i}:\\) Valor real. \\(\\hat{y}_{i}:\\) Valor de la predicción. Esta métrica es muy parecida a MAE, solo que en lugar de sacar el valor absoluto de la diferencia entre el valor real y el valor predicho, para evitar valores negativos eleva esta diferencia al cuadrado, y saca el promedio de esa diferencia, al final, para dejar el valor en la escala inicial saca la raíz cuadrada. Esta es la métrica más utilizada en problemas de regresión, debido a que es más fácil de optimizar que el MAE. Ventajas Vs Desventaja: Todos los errores pesan lo mismo sin importar qué tan pequeños o qué tan grandes sean, es más sensible a valores atípicos que MAE pues eleva al cuadrado diferencias, y dado que obtiene el promedio puede ser que un solo error en la predicción que sea muy grande afecte al valor de todo el modelo, aún y cuando el modelo no tuvo errores tan malos para el resto de las observaciones. Se recomienda utilizar esta métrica cuando en el problema que queremos resolver es muy costoso tener equivocaciones grandes, podemos tener varios errores pequeños, pero no grandes. \\(R^2\\): R cuadrada \\[R^{2} = \\frac{\\sum_{i=1}^{N}{(\\hat{y}_{i}-\\bar{y}_{i})^2}}{\\sum_{i=1}^{N}{(y_{i}-\\bar{y}_{i})^2}}\\] Donde: \\(N:\\) Número de observaciones predichas. \\(y_{i}:\\) Valor real. \\(\\hat{y}_{i}:\\) Valor de la predicción. \\(\\bar{y}_{i}:\\) Valor promedio de la variable y. El coeficiente de determinación es la proporción de la varianza total de la variable explicada por la regresión. El coeficiente de determinación, también llamado R cuadrado, refleja la bondad del ajuste de un modelo a la variable que pretender explicar. Es importante saber que el resultado del coeficiente de determinación oscila entre 0 y 1. Cuanto más cerca de 1 se sitúe su valor, mayor será el ajuste del modelo a la variable que estamos intentando explicar. De forma inversa, cuanto más cerca de cero, menos ajustado estará el modelo y, por tanto, menos fiable será. Ventajas Vs Desventaja: El problema del coeficiente de determinación, y razón por el cual surge el coeficiente de determinación ajustado, radica en que no penaliza la inclusión de variables explicativas no significativas, es decir, el valor de \\(R^2\\) siempre será más grande cuantas más variables sean incluidas en el modelo, aún cuando estas no sean significativas en la predicción. \\(\\bar{R}^2\\): \\(R^2\\) ajustada \\[\\bar{R}^2=1-\\frac{N-1}{N-k-1}[1-R^2]\\] Donde: \\(\\bar{R}²:\\) Es el valor de R² ajustado \\(R²:\\) Es el valor de R² original \\(N:\\) Es el total de observaciones en el ajuste \\(k:\\) Es el número de variables usadas en el modelo El coeficiente de determinación ajustado (R cuadrado ajustado) es la medida que define el porcentaje explicado por la varianza de la regresión en relación con la varianza de la variable explicada. Es decir, lo mismo que el R cuadrado, pero con una diferencia: El coeficiente de determinación ajustado penaliza la inclusión de variables. En la fórmula, N es el tamaño de la muestra y k el número de variables explicativas. 5.4 Implementación en R Usaremos las recetas antes implementadas para ajustar tanto el modelo de regresión como el de clasificación. Ajustaremos un modelo de regresión usando la receta antes vista. library(tidymodels) data(ames) set.seed(4595) ames_split &lt;- initial_split(ames, prop = 0.75) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) receta_casas &lt;- recipe( Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + Exter_Cond + Bsmt_Cond + Year_Sold + Year_Remod_Add, data = ames_train) %&gt;% step_mutate( Age_House = Year_Sold - Year_Remod_Add, Exter_Cond = forcats::fct_collapse(Exter_Cond, Good = c(&quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;))) %&gt;% step_relevel(Exter_Cond, ref_level = &quot;Good&quot;) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_interact(~ matches(&quot;Bsmt_Cond&quot;):TotRms_AbvGrd) %&gt;% prep() receta_casas ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 6 ## ## Training data contained 2197 data points and no missing data. ## ## Operations: ## ## Variable mutation for ~Year_Sold - Year_Remod_Add, ~forcats::fct... [trained] ## Re-order factor level to ref_level for Exter_Cond [trained] ## Centering and scaling for Gr_Liv_Area, TotRms_AbvGrd, Year_Sold, Year_Rem... [trained] ## Dummy variables from Exter_Cond, Bsmt_Cond [trained] ## Interactions with (Bsmt_Cond_Fair + Bsmt_Cond_Good + Bsmt_Cond_No_Ba... [trained] casa_juiced &lt;- juice(receta_casas) casa_test_bake &lt;- bake(receta_casas, new_data = ames_test) modelo1 &lt;- linear_reg() %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;lm&quot;) lm_fit1 &lt;- fit(modelo1, Sale_Price ~ ., casa_juiced) p_test &lt;- predict(lm_fit1, casa_test_bake) %&gt;% bind_cols(ames_test) %&gt;% select(.pred, Sale_Price) %&gt;% mutate(error = Sale_Price - .pred) %&gt;% filter(.pred &gt; 0) p_test ## # A tibble: 733 × 3 ## .pred Sale_Price error ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 94800. 105000 10200. ## 2 174700. 185000 10300. ## 3 188958. 180400 -8558. ## 4 85587. 141000 55413. ## 5 244570. 210000 -34570. ## 6 214422. 216000 1578. ## 7 163805. 149900 -13905. ## 8 122163. 105500 -16663. ## 9 122163. 88000 -34163. ## 10 164615. 146000 -18615. ## # ℹ 723 more rows 5.4.1 Coeficientes del modelo Podemos recuperar los coeficientes de nuestro modelo con la función tidy() y observar cuales variables explicativas son las más significativas de acuerdo con el p-value. lm_fit1 %&gt;% tidy() %&gt;% arrange(p.value) ## # A tibble: 18 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Gr_Liv_Area 56853. 1822. 31.2 5.64e-177 ## 2 Year_Remod_Add 24376. 1115. 21.9 6.21e- 96 ## 3 (Intercept) 212797. 33979. 6.26 4.55e- 10 ## 4 TotRms_AbvGrd -9494. 1823. -5.21 2.09e- 7 ## 5 Bsmt_Cond_Fair_x_TotRms_AbvGrd -24460. 5216. -4.69 2.91e- 6 ## 6 Exter_Cond_Fair -30809. 7226. -4.26 2.10e- 5 ## 7 Bsmt_Cond_No_Basement_x_TotRms_AbvGrd -19869. 5169. -3.84 1.25e- 4 ## 8 Exter_Cond_Poor -76007. 27988. -2.72 6.67e- 3 ## 9 Bsmt_Cond_No_Basement -62919. 34565. -1.82 6.88e- 2 ## 10 Bsmt_Cond_Fair -60285. 34479. -1.75 8.05e- 2 ## 11 Bsmt_Cond_Poor_x_TotRms_AbvGrd -71907. 46682. -1.54 1.24e- 1 ## 12 Year_Sold -1484. 1033. -1.44 1.51e- 1 ## 13 Bsmt_Cond_Good_x_TotRms_AbvGrd 6447. 5025. 1.28 2.00e- 1 ## 14 Bsmt_Cond_Typical -30166. 33996. -0.887 3.75e- 1 ## 15 Bsmt_Cond_Good -16809. 34336. -0.490 6.25e- 1 ## 16 Bsmt_Cond_Poor -19736. 44381. -0.445 6.57e- 1 ## 17 Age_House NA NA NA NA ## 18 Bsmt_Cond_Typical_x_TotRms_AbvGrd NA NA NA NA lm_fit1 %&gt;% tidy() %&gt;% arrange(desc(p.value)) ## # A tibble: 18 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Bsmt_Cond_Poor -19736. 44381. -0.445 6.57e- 1 ## 2 Bsmt_Cond_Good -16809. 34336. -0.490 6.25e- 1 ## 3 Bsmt_Cond_Typical -30166. 33996. -0.887 3.75e- 1 ## 4 Bsmt_Cond_Good_x_TotRms_AbvGrd 6447. 5025. 1.28 2.00e- 1 ## 5 Year_Sold -1484. 1033. -1.44 1.51e- 1 ## 6 Bsmt_Cond_Poor_x_TotRms_AbvGrd -71907. 46682. -1.54 1.24e- 1 ## 7 Bsmt_Cond_Fair -60285. 34479. -1.75 8.05e- 2 ## 8 Bsmt_Cond_No_Basement -62919. 34565. -1.82 6.88e- 2 ## 9 Exter_Cond_Poor -76007. 27988. -2.72 6.67e- 3 ## 10 Bsmt_Cond_No_Basement_x_TotRms_AbvGrd -19869. 5169. -3.84 1.25e- 4 ## 11 Exter_Cond_Fair -30809. 7226. -4.26 2.10e- 5 ## 12 Bsmt_Cond_Fair_x_TotRms_AbvGrd -24460. 5216. -4.69 2.91e- 6 ## 13 TotRms_AbvGrd -9494. 1823. -5.21 2.09e- 7 ## 14 (Intercept) 212797. 33979. 6.26 4.55e- 10 ## 15 Year_Remod_Add 24376. 1115. 21.9 6.21e- 96 ## 16 Gr_Liv_Area 56853. 1822. 31.2 5.64e-177 ## 17 Age_House NA NA NA NA ## 18 Bsmt_Cond_Typical_x_TotRms_AbvGrd NA NA NA NA 5.4.2 Métricas de desempeño Ahora para calcular las métricas de desempeño usaremos la paquetería MLmetrics y las funciones de dplyr para resumir y estructurar los resultados. library(MLmetrics) p_test %&gt;% summarise( MAE = MLmetrics::MAE(.pred, Sale_Price), MAPE = MLmetrics::MAPE(.pred, Sale_Price), RMSE = MLmetrics::RMSE(.pred, Sale_Price), R2 = MLmetrics::R2_Score(.pred, Sale_Price) ) ## # A tibble: 1 × 4 ## MAE MAPE RMSE R2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 34069. 0.202 49398. 0.624 5.4.3 Gráfica de ajuste library(patchwork) pred_obs_plot &lt;- p_test %&gt;% ggplot(aes(x = .pred, y = Sale_Price)) + geom_point(alpha = 0.2) + geom_abline(color = &quot;red&quot;) + xlab(&quot;Predicciones&quot;) + ylab(&quot;Observaciones&quot;) + ggtitle(&quot;Predicción vs Observación&quot;) error_line &lt;- p_test %&gt;% ggplot(aes(x = Sale_Price, y = error)) + geom_line() + geom_hline(yintercept = 0, color = &quot;red&quot;) + xlab(&quot;Observaciones&quot;) + ylab(&quot;Errores&quot;) + ggtitle(&quot;Varianza de errores&quot;) pred_obs_plot + error_line error_dist &lt;- p_test %&gt;% ggplot(aes(x = error)) + geom_histogram(color = &quot;white&quot;, fill = &quot;black&quot;) + geom_vline(xintercept = 0, color = &quot;red&quot;) + ylab(&quot;Conteos de clase&quot;) + xlab(&quot;Errores&quot;) + ggtitle(&quot;Distribución de error&quot;) error_qqplot &lt;- p_test %&gt;% ggplot(aes(sample = error)) + geom_qq(alpha = 0.3) + stat_qq_line(color = &quot;red&quot;) + xlab(&quot;Distribución normal&quot;) + ylab(&quot;Distribución de errores&quot;) + ggtitle(&quot;QQ-Plot&quot;) error_dist + error_qqplot 5.5 Métodos se selección de variables Una de las preguntas clave a responder es: ¿Cómo selecciono las variables a usar en un modelo?. Existen muchas técnicas para ello. Incluso, existen modelos que se encargan de realizar esta tarea de modo automático. Analizaremos diferentes técnicas a lo largo del curso. 5.5.1 Forward selection (selección hacia adelante) Comienza sin predictores en el modelo, agrega iterativamente los predictores más contribuyentes y se detiene cuando la mejora del modelo ya no es estadísticamente significativa. 5.5.2 Backward selection (selección hacia atrás) Comienza con todos los predictores en el modelo (modelo completo), y elimina iterativamente los predictores menos contribuyentes y se detiene cuando tiene un modelo en el que todos los predictores son estadísticamente significativos. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
